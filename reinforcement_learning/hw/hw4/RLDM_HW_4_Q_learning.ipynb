{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning and Decision Making &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #4\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "In this homework, you will have the complete reinforcement-learning experience:  training an agent from scratch to solve a simple domain using Q-learning.\n",
    "\n",
    "The environment you will be applying Q-learning to is called [Taxi](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py) (Taxi-v3).  The Taxi problem was introduced by [Dietterich 1998](https://www.jair.org/index.php/jair/article/download/10266/24463) and has been used for reinforcement-learning research in the past.  It is a grid-based environment where the goal of the agent is to pick up a passenger at one location and drop them off at another.\n",
    "\n",
    "The map is fixed and the environment has deterministic transitions.  However, the distinct pickup and drop-off points are chosen randomly from 4 fixed locations in the grid, each assigned a different letter.  The starting location of the taxicab is also chosen randomly.\n",
    "\n",
    "The agent has 6 actions: 4 for movement, 1 for pickup, and 1 for drop-off.  Attempting a pickup when there is no passenger at the location incurs a reward of -10.  Dropping off a passenger outside one of the four designated zones is prohibited, and attempting it also incurs a reward of −10.  Dropping the passenger off at the correct destination provides the agent with a reward of 20.  Otherwise, the agent incurs a reward of −1 per time step.\n",
    "\n",
    "Your job is to train your agent until it converges to the optimal state-action value function.  You will have to think carefully about algorithm implementation, especially exploration parameters.\n",
    "\n",
    "## Q-learning\n",
    "\n",
    "Q-learning is a fundamental reinforcement-learning algorithm that has been successfully used to solve a variety of  decision-making  problems.   Like  Sarsa,  it  is  a  model-free  method  based  on  temporal-difference  learning. However, unlike Sarsa, Q-learning is *off-policy*, which means the policy it learns about can be different than the policy it uses to generate its behavior.  In Q-learning, this *target* policy is the greedy policy with respect to the current value-function estimate.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "- You should return the optimal *Q-value* for a specific state-action pair of the Taxi environment.\n",
    "\n",
    "- To solve this problem you should implement the Q-learning algorithm and use it to solve the Taxi environment. The agent  should  explore  the MDP, collect data  to  learn an optimal  policy and also the optimal Q-value function.  Be mindful of how you handle terminal states: if $S_t$ is a terminal state, then $V(St)$ should always be 0.  Use $\\gamma= 0.90$ - this is important, as the optimal value function depends on the discount rate.  Also, note that an $\\epsilon$-greedy strategy can find an optimal policy despite finding sub-optimal Q-values.   As we are looking for optimal  Q-values, you will have to carefully consider your exploration strategy.\n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Lesson 7: Exploring Exploration\n",
    "\n",
    "-   Chapter 6 (6.5 Q-learning: Off-policy TD Control) of http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "-   Chapter 2 (2.6.1 Q-learning) of 'Algorithms for Sequential Decision Making', M.\n",
    "    Littman, 1996\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Canvas page for this assignment.\n",
    "    Make sure you have set your timezone in Canvas to ensure the deadline is accurate.\n",
    "\n",
    "-   Submit your finished notebook on Gradescope. Your grade is based on\n",
    "    a set of hidden test cases. You will have unlimited submissions -\n",
    "    only the last score is kept.\n",
    "\n",
    "-   Use the template below to implement your code. We have also provided\n",
    "    some test cases for you. If your code passes the given test cases,\n",
    "    it will run (though possibly not pass all the tests) on Gradescope. \n",
    "    Be cognisant of performance.  If the autograder fails because of memory \n",
    "    or runtime issues, you need to refactor your solution\n",
    "\n",
    "-   Gradescope is using python 3.6.x. For permitted libraries, please see\n",
    "    the requirements.txt file, You can also use any core library\n",
    "    (i.e., anything in the Python standard library).\n",
    "    No other library can be used.  Also, make sure the name of your\n",
    "    notebook matches the name of the provided notebook.  Gradescope times\n",
    "    out after 10 minutes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "\n",
    "class QLearningAgent(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize your Q table and hyperparameters\n",
    "        \"\"\"\n",
    "        env = gym.make('Taxi-v3')\n",
    "        seed = 42\n",
    "        env.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # TODO\n",
    "        self.Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        self.env = env.env\n",
    "        self.gamma = 0.9\n",
    "        self.epsilon = 0.9\n",
    "        self.epsilon_decay = 0.00001\n",
    "        self.alpha = 1.0\n",
    "        self.num_episodes = 100000\n",
    "\n",
    "    def solve(self):\n",
    "        \"\"\"\n",
    "        Implement the Q learning algorithm\n",
    "        \"\"\"\n",
    "        # loop over episodes\n",
    "        for i in range(self.num_episodes):\n",
    "            # reset the environment\n",
    "            state = self.env.reset()\n",
    "            done = False\n",
    "            # loop over this episode until it is done\n",
    "            while not done:\n",
    "                # select an action using epsilon greedy policy\n",
    "                action = self.get_epsilon_greedy_action(state, self.Q, self.epsilon, self.env)\n",
    "                # take a step in the environment\n",
    "                next_state, reward, done, _ = self.env.step(action)\n",
    "                # select the next action using greedy policy (no exploration, need this for Bellman)\n",
    "                next_state_max_action = self.get_greedy_action(next_state, self.Q)\n",
    "                # update the Q table - Bellman equation\n",
    "                update = self.alpha * (reward + self.gamma * self.Q[next_state, next_state_max_action] - self.Q[state, action])\n",
    "                self.Q[state, action] += update\n",
    "                state = next_state\n",
    "            \n",
    "            # decay epsilon \n",
    "            if self.epsilon > 0.075:\n",
    "                self.epsilon = self.epsilon * (1 - self.epsilon_decay)\n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def Q_table(self, state, action):\n",
    "        \"\"\"\n",
    "        return the optimal value for State-Action pair in the Q Table\n",
    "        \"\"\"\n",
    "        return self.Q[state][action]\n",
    "    \n",
    "\n",
    "    def get_epsilon_greedy_action(self, state, Q, epsilon, env):\n",
    "        \"\"\"\n",
    "        return the action using epsilon greedy policy\n",
    "        \"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.randint(env.action_space.n)\n",
    "        return np.argmax(Q[state, :])\n",
    "\n",
    "\n",
    "    def get_greedy_action(self, state, Q):\n",
    "        \"\"\"\n",
    "        return the greedy action for the given state\n",
    "        \"\"\"\n",
    "        return np.argmax(Q[state, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_case_1 (__main__.TestQNotebook.test_case_1) ... ok\n",
      "test_case_2 (__main__.TestQNotebook.test_case_2) ... ok\n",
      "test_case_3 (__main__.TestQNotebook.test_case_3) ... ok\n",
      "test_case_4 (__main__.TestQNotebook.test_case_4) ... ok\n",
      "test_case_5 (__main__.TestQNotebook.test_case_5) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 49.913s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x1c0d285dad0>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.  This code will ensure that you submission is correct \n",
    "## and will work proberly with the autograder\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        cls.agent = QLearningAgent()\n",
    "        cls.agent.solve()\n",
    "        \n",
    "    def test_case_1(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(462, 4),\n",
    "            -11.374402515,\n",
    "            decimal=3\n",
    "        )\n",
    "        \n",
    "    def test_case_2(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(398, 3),\n",
    "            4.348907,\n",
    "            decimal=3\n",
    "        )\n",
    "    \n",
    "    def test_case_3(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(253, 0),\n",
    "            -0.5856821173,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_4(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(377, 1),\n",
    "            9.683,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "    def test_case_5(self):\n",
    "        np.testing.assert_almost_equal(\n",
    "            self.agent.Q_table(83, 5),\n",
    "            -13.9968,\n",
    "            decimal=3\n",
    "        )\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
