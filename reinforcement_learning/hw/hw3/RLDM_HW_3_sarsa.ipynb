{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Reinforcement Learning and Decision Making &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Homework #3\n",
    "\n",
    "# &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description\n",
    "\n",
    "For this assignment,  you will build a Sarsa agent which will learn policies in the [OpenAI Gym](http://gym.openai.com/docs/) Frozen Lake environment.  [OpenAI Gym](http://gym.openai.com/docs/) is a platform where users can test their RL algorithms on a selection of carefully crafted environments.  As we will continue to use [OpenAI Gym](http://gym.openai.com/docs/) through Project 2, this assignment also provides an opportunity to familiarize yourself with its interface.\n",
    "\n",
    "Frozen Lake is a grid world environment that is highly stochastic,  where the agent must cross a slippery frozen  lake  which  has  deadly  holes  to  fall  through.   The  agent  begins  in  the  starting  state `S` and  is  given  a reward of `1` if it reaches the goal state `G`.  A reward of `0` is given for all other transitions.\n",
    "\n",
    "The agent can take one of four possible moves at each state (left, down, right, or up).  The frozen cells `F` are slippery, so the agent’s actions succeed only `1/3` of the time, while the other `2/3` are split evenly between the two directions orthogonal to the intended direction.  If the agent lands in a hole `H`, then the episode terminates. You will be given a randomized Frozen Lake map with a corresponding set of parameters to train your Sarsa agent with.  If your agent is implemented correctly, then after training it for the specified number of episodes, your agent will produce the same policy (not necessarily an optimal policy) as the automatic grader.\n",
    "\n",
    "\n",
    "## Sarsa $($$S_t$, $A_t$, $R_{t+1}$, $S_{t+1}$, $A_{t+1}$$)$\n",
    "\n",
    "Sarsa uses temporal-difference learning to form a model-free on-policy reinforcement-learning algorithm that solves the *control* problem. It is model free because it does not need and does not use a model of the environment, namely neither a transition nor reward function; instead, Sarsa samples transitions and rewards online.\n",
    "\n",
    "It is on-policy because it learns about the same policy that generates its behaviors (this is in contrast to *Q-learning*, which you’ll examine in your next homework).  That is, Sarsa estimates the action-value function of its behavior policy.  In this homework,  you will not be training a Sarsa agent to approximate the *optimal* action-value function; instead, the hyperparameters of both the exploration strategy and the algorithm will be given to you as input — the goal being to verify that your SARSA agent is correctly implemented.\n",
    "\n",
    "## Procedure\n",
    "\n",
    "Since this homework requires you to match a non-deterministic output between your agent and the autograder’s agent, attention to detail to each of the following points is required:\n",
    "\n",
    "- You must use Python and the library NumPy for this homework *python 3.6.x* and *numpy==1.18.0*\n",
    "\n",
    "- Install OpenAI Gym (e.g.pip install gym) *gym==0.17.2*\n",
    "\n",
    "- The Frozen Lake environment has been instantiated for you.\n",
    "- The pertinent random number generators have been seeded for you. Do *not* use the Python standard library’s *random* library.\n",
    "\n",
    "- Implement your Sarsa agent using an $\\epsilon$-greedy behavioral policy.  Specifically, you must use *numpy.random.random* to  choose  whether  or  not  the  action  is  greedy,  and *numpy.random.randint* to select the random action.\n",
    "\n",
    "- Initialize the agent’s Q-table to zeros.\n",
    "\n",
    "- Train your agent using the given input parameters.  The input *amap* is the Frozen Lake map that you need to resize and provide to the *desc* attribute when you instantiate your environment.  The input *gamma* is the discount rate.  The input *alpha* is the learning rate.  The input *epsilon* is the parameter for the $\\epsilon$-greedy behavior strategy your Sarsa agent will use.  Specifically, an action should be selected uniformly at random if a random number drawn uniformly between 0 and 1 is less than $\\epsilon$.  If the greedy action is selected,  the  action  with  lowest  index  should  be  selected  in  case  of  ties.   The  input `n_episodes` is  the number of episodes to train your agent.  Finally, *seed* is the number used to seed both Gym’s random number generator and NumPy’s random number generator.\n",
    "\n",
    "- To sync with the autograder,  your Sarsa implementation should select the action corresponding to the next state the  agent  will  visit *even when* that  next  state  is  a  terminal  state  (this  action  will  never  be executed by the agent).\n",
    "\n",
    "- You should return the greedy policy with respect to the Q-function obtained by your Sarsa agent after the completion of the final episode.  Specifically, the policy should be expressed as a string ofcharacters: **<, v, >, ^,** representing left, down, right, and up, respectively.  The ordering of the actions in the output should reflect the ordering of states in *amap*. \n",
    "\n",
    "## Resources\n",
    "\n",
    "The concepts explored in this homework are covered by:\n",
    "\n",
    "-   Lesson 4: Convergence\n",
    "\n",
    "-   Chapter 6 (6.4 Sarsa:  On-policy TD Control) of\n",
    "    http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "\n",
    "## Submission\n",
    "\n",
    "-   The due date is indicated on the Canvas page for this assignment.\n",
    "    Make sure you have your timezone in Canvas set to ensure the\n",
    "    deadline is accurate.\n",
    "\n",
    "-   Submit your finished notebook on Gradescope. Your grade is based on\n",
    "    a set of hidden test cases. You will have unlimited submissions -\n",
    "    only the last score is kept.\n",
    "\n",
    "-   Use the template below to implement your code. We have also provided\n",
    "    some test cases for you. If your code passes the given test cases,\n",
    "    it will run (though possibly not pass all the tests) on Gradescope. \n",
    "    Be cognisant of performance.  If the autograder fails because of memory \n",
    "    or runtime issues, you need to refactor your solution\n",
    "\n",
    "-   Gradescope is using python 3.6.x. For permitted libraries, please see\n",
    "    the requirements.txt file, You can also use any core library\n",
    "    (i.e., anything in the Python standard library).\n",
    "    No other library can be used.  Also, make sure the name of your\n",
    "    notebook matches the name of the provided notebook.  Gradescope times\n",
    "    out after 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"C:\\\\users\\\\mccar\\\\miniconda3\\\\lib\\\\site-packages\")\n",
    "from math import sqrt\n",
    "from sys import argv\n",
    "import numpy as np\n",
    "\n",
    "def square(amap):\n",
    "    '''\n",
    "    Take a list and put it into a square array for better readability\n",
    "    '''\n",
    "    return [amap[i * int(sqrt(len(amap))) : (i + 1) * int(sqrt(len(amap)))] for i in range(int(sqrt(len(amap))))]\n",
    "\n",
    "\n",
    "def select_action(_state, _Q, epsilon, env):\n",
    "    \"\"\"\n",
    "    epsilon greedy action selection\n",
    "    Take the best action with probability 1-epsilon, otherwise take a random action\n",
    "    \"\"\"\n",
    "    if np.random.rand() < epsilon:\n",
    "        return np.random.randint(env.action_space.n)\n",
    "    return np.argmax(_Q[_state, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from math import sqrt\n",
    "\n",
    "class FrozenLakeAgent(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def amap_to_gym(self, amap='FFGG'):\n",
    "        \"\"\"\n",
    "        Maps the `amap` string to a gym environment to set the map\n",
    "        \"\"\"\n",
    "        amap = np.asarray(amap, dtype='c')\n",
    "        side = int(sqrt(amap.shape[0]))\n",
    "        # sets the size\n",
    "        amap = amap.reshape((side, side))\n",
    "        return gym.make('FrozenLake-v0', desc=amap).unwrapped\n",
    "\n",
    "\n",
    "    def solve(self, amap, gamma, alpha, epsilon, n_episodes, seed):\n",
    "        \"\"\"\n",
    "        Our solution to the agent\n",
    "        Implement the agent\n",
    "        \"\"\"\n",
    "        # initialize the environment\n",
    "        env = self.amap_to_gym(amap)\n",
    "        np.random.seed(seed)\n",
    "        env.seed(seed)\n",
    "\n",
    "        # TODO: Implement the algorithm\n",
    "        \"\"\"SARSA algorithm\"\"\"\n",
    "        # env = gym.make('FrozenLake-v0', desc=square(amap)).unwrapped\n",
    "        # create the Q table\n",
    "        Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "        # loop over episodes\n",
    "        for _ in range(n_episodes):\n",
    "            # reset the environment\n",
    "            state = env.reset()\n",
    "            # select the first action\n",
    "            action = select_action(state, Q, epsilon, env)\n",
    "\n",
    "            done = False\n",
    "            while not done:\n",
    "                # recieve the next state and reward from the environment\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                # select the next action\n",
    "                next_action = select_action(next_state, Q, epsilon, env)\n",
    "                # update the Q table\n",
    "                # Bellman equation\n",
    "                update = reward + gamma * Q[next_state, next_action] - Q[state, action]\n",
    "                # update the Q table\n",
    "                Q[state, action] = Q[state, action] + alpha * update\n",
    "                state = next_state\n",
    "                action = next_action\n",
    "\n",
    "        env.close()\n",
    "        Q_star = np.argmax(Q, axis=1)\n",
    "        mapping = ['<', 'v', '>', '^']\n",
    "        return ''.join([mapping[action] for action in Q_star])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_case_1 (__main__.TestQNotebook.test_case_1) ... ok\n",
      "test_case_2 (__main__.TestQNotebook.test_case_2) ... ok\n",
      "test_case_3 (__main__.TestQNotebook.test_case_3) ... ok\n",
      "test_case_4 (__main__.TestQNotebook.test_case_4) ... ok\n",
      "test_case_5 (__main__.TestQNotebook.test_case_5) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 39.076s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x20ecb245050>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "## DO NOT MODIFY THIS CODE.  This code will ensure that you submission is correct \n",
    "## and will work proberly with the autograder\n",
    "\n",
    "import unittest\n",
    "\n",
    "\n",
    "class TestQNotebook(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.agent = FrozenLakeAgent()\n",
    "\n",
    "    def test_case_1(self):\n",
    "        example1 = self.agent.solve(\n",
    "            amap='SFFFHFFFFFFFFFFG',\n",
    "            gamma=1.0,\n",
    "            alpha=0.25,\n",
    "            epsilon=0.29,\n",
    "            n_episodes=14697,\n",
    "            seed=741684\n",
    "        )\n",
    "        assert(example1 == '^vv><>>vvv>v>>><')\n",
    "\n",
    "    def test_case_2(self):\n",
    "        example2 = self.agent.solve(\n",
    "            amap='SFFFFHFFFFFFFFFFFFFFFFFFG',\n",
    "            gamma=0.91,\n",
    "            alpha=0.12,\n",
    "            epsilon=0.13,\n",
    "            n_episodes=42271,\n",
    "            seed=983459\n",
    "        )\n",
    "        assert(example2 == '^>>>><>>>vvv>>vv>>>>v>>^<')\n",
    "\n",
    "    def test_case_3(self):\n",
    "        example3 = self.agent.solve(\n",
    "            amap='SFFG',\n",
    "            gamma=1.0,\n",
    "            alpha=0.24,\n",
    "            epsilon=0.09,\n",
    "            n_episodes=49553,\n",
    "            seed=20240\n",
    "        )\n",
    "        assert(example3 == '<<v<')\n",
    "\n",
    "    def test_case_4(self):\n",
    "        example4 = self.agent.solve(\n",
    "            amap='SFFHHFFHHFFHHFFG',\n",
    "            gamma=0.99,\n",
    "            alpha=0.5,\n",
    "            epsilon=0.29,\n",
    "            n_episodes=23111,\n",
    "            seed=44323\n",
    "        )\n",
    "        assert(example4=='^><<<>^<<><<<>^<')\n",
    "\n",
    "    def test_case_5(self):\n",
    "        example5 = self.agent.solve(\n",
    "            amap='SFFFFHFFFHHFFFFFFFFHHFFFG',\n",
    "            gamma=0.88,\n",
    "            alpha=0.15,\n",
    "            epsilon=0.16,\n",
    "            n_episodes=112312,\n",
    "            seed=6854343\n",
    "        )\n",
    "        assert(example5 == '^>><^<>><<<>v<^v>v<<<>vv<')\n",
    "\n",
    "unittest.main(argv=[''], verbosity=2, exit=False) \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
