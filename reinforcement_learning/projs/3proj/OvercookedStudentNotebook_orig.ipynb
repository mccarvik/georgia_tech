{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QBFvvcU05V-X"
   },
   "outputs": [],
   "source": [
    "### Install libraries ###\n",
    "\n",
    "!pip install git+https://github.com/HumanCompatibleAI/overcooked_ai.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ScQhFpxT5ZkL"
   },
   "outputs": [],
   "source": [
    "### Imports ###\n",
    "\n",
    "from overcooked_ai_py.mdp.overcooked_mdp import OvercookedGridworld\n",
    "from overcooked_ai_py.mdp.overcooked_env import OvercookedEnv\n",
    "from overcooked_ai_py.agents.agent import NNPolicy, AgentFromPolicy, AgentPair\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import os\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "## Uncomment if you'd like to use your personal Google Drive to store outputs\n",
    "## from your runs. You can find some hooks to Google Drive commented\n",
    "## throughout the rest of this code.\n",
    "# from google.colab import drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAr6oW-Y5b3C"
   },
   "outputs": [],
   "source": [
    "### Environment setup ###\n",
    "\n",
    "## Swap between the 3 layouts here:\n",
    "layout = \"cramped_room\"\n",
    "# layout = \"asymmetric_advantages\"\n",
    "# layout = \"forced_coordination\"\n",
    "\n",
    "## Reward shaping is disabled by default; i.e., only the sparse rewards are\n",
    "## included in the reward returned by the enviornment).  If you'd like to do\n",
    "## reward shaping (recommended to make the task much easier to solve), this\n",
    "## data structure provides access to a built-in reward-shaping mechanism within\n",
    "## the Overcooked environment.  You can, of course, do your own reward shaping\n",
    "## in lieu of, or in addition to, using this structure. The shaped rewards\n",
    "## provided by this structure will appear in a different place (see below)\n",
    "reward_shaping = {\n",
    "    \"PLACEMENT_IN_POT_REW\": 3,\n",
    "    \"DISH_PICKUP_REWARD\": 3,\n",
    "    \"SOUP_PICKUP_REWARD\": 5\n",
    "}\n",
    "\n",
    "# Length of Episodes.  Do not modify for your submission!\n",
    "# Modification will result in a grading penalty!\n",
    "horizon = 400\n",
    "\n",
    "# Build the environment.  Do not modify!\n",
    "mdp = OvercookedGridworld.from_layout_name(layout, rew_shaping_params=reward_shaping)\n",
    "base_env = OvercookedEnv.from_mdp(mdp, horizon=horizon, info_level=0)\n",
    "env = gym.make(\"Overcooked-v0\", base_env=base_env, featurize_fn=base_env.featurize_state_mdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EsQPGw35fKS"
   },
   "outputs": [],
   "source": [
    "### Train your agent ###\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# !mkdir -p \"/content/drive/My Drive/Colab\"\n",
    "\n",
    "# The code below runs a few episodes with a random agent.  Your learning algorithm\n",
    "# would go here.\n",
    "\n",
    "num_episodes = 5\n",
    "\n",
    "for e in range(num_episodes):\n",
    "    # Episode termination flag\n",
    "    done = False\n",
    "\n",
    "    # The number of soups the agent pair made during the episode\n",
    "    num_soups_made = 0\n",
    "\n",
    "    # Reset the environment at the start of each episode\n",
    "    obs = env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Obtain observations for each agent\n",
    "        obs0 = obs[\"both_agent_obs\"][0]\n",
    "        obs1 = obs[\"both_agent_obs\"][1]\n",
    "\n",
    "        # Select random actions from the set {North, South, East, West, Stay, Interact}\n",
    "        # for each agent.\n",
    "        a0 = env.action_space.sample()\n",
    "        a1 = env.action_space.sample()\n",
    "\n",
    "        # Take the selected actions and receive feedback from the environment\n",
    "        # The returned reward \"R\" only reflects completed soups.\n",
    "        obs, R, done, info = env.step([a0, a1])\n",
    "\n",
    "        # You can find the separate shaping rewards induced by the data\n",
    "        # structure you defined above in the \"info\" dictionary.\n",
    "        ## THE REVERSAL OF THIS ARRAY IS NECESSARY TO ALIGN THE CORRECT REWARD\n",
    "        ## TO THE CORRECT AGENT (see project documentation)!\n",
    "        # Note that this shaping reward does *not* include the +20 reward for\n",
    "        # completed soups (the one returned in \"R\").\n",
    "        r_shaped = info[\"shaped_r_by_agent\"]\n",
    "        if env.agent_idx:\n",
    "            r_shaped_0 = r_shaped[1]\n",
    "            r_shaped_1 = r_shaped[0]\n",
    "        else:\n",
    "            r_shaped_0 = r_shaped[0]\n",
    "            r_shaped_1 = r_shaped[1]\n",
    "\n",
    "        # Accumulate the number of soups made\n",
    "        num_soups_made += int(R / 20) # Each served soup generates 20 reward\n",
    "\n",
    "    # Display status\n",
    "    print(\"Ep {0}\".format(e + 1), end=\" \")\n",
    "    print(\"shaped reward for agent 0: {0}:\".format(r_shaped_0), end=\" \")\n",
    "    print(\"shaped reward for agent 1: {0}\".format(r_shaped_1), end=\" \")\n",
    "    print(\"number of soups made: {0}\".format(num_soups_made))\n",
    "\n",
    "# The info flag returned by the environemnt contains special status info\n",
    "# specifically when done == True.  This information may be useful in\n",
    "# developing, debugging, and analyzing your results.  It may also be a good\n",
    "# way for you to find a metric that you can use in evaluating collaboration\n",
    "# between your agents.\n",
    "print(\"\\nExample end-of-episode info dump:\\n\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GpPoWGp5hzZ"
   },
   "outputs": [],
   "source": [
    "### All of the remaining code in this notebook is solely for using the\n",
    "### built-in Overcooked state visualizer on a trained agent, so that you can see\n",
    "### a graphical rendering of what your agents are doing. It is not\n",
    "### necessary to use this.\n",
    "\n",
    "# The below code is a partcular way to rollout episodes in a format\n",
    "# compatible with the built-in state visualizer.\n",
    "\n",
    "class StudentPolicy(NNPolicy):\n",
    "    \"\"\" Generate policy \"\"\"\n",
    "    def __init__(self):\n",
    "        super(StudentPolicy, self).__init__()\n",
    "\n",
    "    def state_policy(self, state, agent_index):\n",
    "        \"\"\"\n",
    "        This method should be used to generate the poiicy vector corresponding to\n",
    "        the state and agent_index provided as input.  If you're using a neural\n",
    "        network-based solution, the specifics depend on the algorithm you are using.\n",
    "        Below are two commented examples, the first for a policy gradient algorithm\n",
    "        and the second for a value-based algorithm.  In policy gradient algorithms,\n",
    "        the neural networks output a policy directly.  In value-based algorithms,\n",
    "        the policy must be derived from the Q value outputs of the networks.  The\n",
    "        uncommented code below is a placeholder that generates a random policy.\n",
    "        \"\"\"\n",
    "        featurized_state = base_env.featurize_state_mdp(state)\n",
    "        input_state = torch.FloatTensor(featurized_state[agent_index]).unsqueeze(0)\n",
    "\n",
    "        # Example for policy NNs named \"PNN0\" and \"PNN1\"\n",
    "        # with torch.no_grad():\n",
    "        #   if agent_index == 0:\n",
    "        #       action_probs = PNN0(input_state)[0].numpy()\n",
    "        #   else:\n",
    "        #       action_probs = PNN1(input_state)[0].numpy()\n",
    "\n",
    "        # Example for Q value NNs named \"QNN0\" and \"QNN1\"\n",
    "        # action_probs = np.zeros(env.action_space.n)\n",
    "        # with torch.no_grad():\n",
    "        #   if agent_index == 0:\n",
    "        #       action_probs[np.argmax(QNN0(input_state)[0].numpy())] = 1\n",
    "        #   else:\n",
    "        #       action_probs[np.argmax(QNN1(input_state)[0].numpy())] = 1\n",
    "\n",
    "        # Random deterministic policy\n",
    "        action_probs = np.zeros(env.action_space.n)\n",
    "        action_probs[env.action_space.sample()] = 1\n",
    "\n",
    "        return action_probs\n",
    "\n",
    "    def multi_state_policy(self, states, agent_indices):\n",
    "        \"\"\" Generate a policy for a list of states and agent indices \"\"\"\n",
    "        return [self.state_policy(state, agent_index) for state, agent_index in zip(states, agent_indices)]\n",
    "\n",
    "\n",
    "class StudentAgent(AgentFromPolicy):\n",
    "    \"\"\"Create an agent using the policy created by the class above\"\"\"\n",
    "    def __init__(self, policy):\n",
    "        super(StudentAgent, self).__init__(policy)\n",
    "\n",
    "\n",
    "# Instantiate the policies for both agents\n",
    "policy0 = StudentPolicy()\n",
    "policy1 = StudentPolicy()\n",
    "\n",
    "# Instantiate both agents\n",
    "agent0 = StudentAgent(policy0)\n",
    "agent1 = StudentAgent(policy1)\n",
    "agent_pair = AgentPair(agent0, agent1)\n",
    "\n",
    "# Generate an episode\n",
    "ae = AgentEvaluator.from_layout_name({\"layout_name\": layout}, {\"horizon\": horizon})\n",
    "trajs = ae.evaluate_agent_pair(agent_pair, num_games=1)\n",
    "print(\"\\nlen(trajs):\", len(trajs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1ZvcY9d5lhy"
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# The function StateVisualizer() below generates images for the state of the\n",
    "# environment at each time step of the episode.\n",
    "#\n",
    "# You have several options for how to use these images:\n",
    "#\n",
    "# 1) You can set img_dir to a local directory (or a directory within Google Drive\n",
    "# if using Colab), and all the images will be saved to that directory for you to browse.\n",
    "#\n",
    "# 2) If using a notebook, you can set the argument ipthon_display=True to get a\n",
    "# tool with a slider that lets you scan through all the images directly in the\n",
    "# notebook.  This option does not require you to store your images.\n",
    "#\n",
    "# 3) You can generate a GIF of the episode. This requires you to set\n",
    "# img_dir.  The code to generate the GIF is commented out below\n",
    "\n",
    "# Modify as appropriate. Example hooks to Google drive are commented.\n",
    "img_dir =  \"imgs/\" # \"/content/drive/My Drive/Colab/\" + \"imgs_\" + layout + \"/\"\n",
    "ipython_display = True\n",
    "gif_path = \"imgs/imgs.gif\" # \"/content/drive/My Drive/Colab/\" + layout + \".gif\"\n",
    "\n",
    "StateVisualizer().display_rendered_trajectory(trajs, img_directory_path=img_dir, ipython_display=ipython_display)\n",
    "\n",
    "## Uncomment for GIF to be generated and stored in 'gif_path'. Requires 'img_dir'\n",
    "## to point to a directory of saved images.\n",
    "# img_list = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "# img_list = sorted(img_list, key=lambda x: int(x.split('.')[0]))\n",
    "# images = [Image.open(img_dir + img).convert('RGBA') for img in img_list]\n",
    "# images[0].save(gif_path, save_all=True, append_images=images[1:], optimize=False, duration=250, loop=0)\n",
    "# with open(gif_path, 'rb') as f: display(IPImage(data=f.read(), format='png'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
