{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3M1CFw-VT5vI"
   },
   "source": [
    "#Preamble\n",
    "\n",
    "This mini-project involves working through all the steps of a problem, whereas prior assignments asked you to just implement core functions. We will give you a dataset, but you will also have the opportunity to manipulate the data in ways that you find beneficial to the overall project and to explain why and how those manipulations mattered. This will be in addition to building the model from scratch, developing the training loop, and implementing testing. The code will be accompanied by a report written into the notebook.\n",
    "\n",
    "This project will have you working with attention mechanism, in a new type of system for question-answering. This will provide you with experience working with attention mechanisms while not directly working with transformers.\n",
    "\n",
    "This assignment is not autograded. You can modify any code cells as long as you achieve the requirements of each graded component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhZzhjGaTj9P"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "Memory networks learn to access external memory stores (a database or, in the case of this assignment, a dictionary). Key-Value Memory Networks specifically assume that the external memory store is organized as a dictionary with keys and values. In theory memory networks are useful when one wants a neural network to be able to know a lot of information but we don't want to try to encode that information directly into the parameters of the network. This means information can be changed in the external memory database without retraining the neural network.\n",
    "\n",
    "Given a question, e.g., \"Where was Alexander Hamilton born?\", a key-value memory network learns an embedding such that the question has a high cosine similarity to a particular key in the external dictionary. Because there are many keys that need to be matched against, key-value memory networks implement an attention-scoring mechanism to select a key. Because attention is a probabilistic score, the key-value memory network retrieves a sum of embeddings weighted according to the attention score. This weighted embedding is then compared to values using a second attention-scoring mechanism. The value with the highest cosine similarity can then be retrieved and returned as the answer.\n",
    "\n",
    "Memory networks were an important part of the evolution of question-answering systems that have been eclipsed by transformers. However, the attention mechanism in a key-value memory network is very similar to the self-attention inside a transformer, so implementing a key-value memory network is a really great way to experiment and learn about self-attention without the added complexity of transformers.\n",
    "\n",
    "Key-value memory networks are also closely related to retieval-based generation networks, except we will be retrieving facts from a dictionary instead of via the internet. However, the embedding of retrieved data will be similar.\n",
    "\n",
    "Key-value memory networks are described in this [paper](https://arxiv.org/abs/1606.03126). It is recommended that you read the paper, but we will also walk through the steps you will need to complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLQP14bxNXDx"
   },
   "source": [
    "# Some imports\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAe6-NtARNjW"
   },
   "source": [
    "You may add imports as necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "JfnqkHKkITC3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "AviW7dfn6pUi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQM96JBsVHwF"
   },
   "source": [
    "Unidecode is useful for getting rid of issues that arise from unicode. This should not be used if we care about unicode, but for the purposes of an instructional exercise, it eliminates a lot of edge cases that come up with unicode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EYuVdhWfAcGQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: unidecode in c:\\users\\mccar\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (1.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\mccar\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "3E_fVmV2_8ub"
   },
   "outputs": [],
   "source": [
    "import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p_bDmt5iSD_j"
   },
   "source": [
    "If you need to have a reduced vocabulary, you can create an unknown \"unk\" token and add it to the vocabulary. Make sure the token index in the vocabulary and `UNK_ID` match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "DDHhoxzyAByu"
   },
   "outputs": [],
   "source": [
    "UNK = 'unk'\n",
    "UNK_ID = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sl6J8BwVpHCw"
   },
   "source": [
    "# Some utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LVVnmlNRKTp"
   },
   "source": [
    "You may edit these as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4KF3VeFoSenc"
   },
   "source": [
    "Stem words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "CPwG_8VrNUop"
   },
   "outputs": [],
   "source": [
    "# Stemming the text\n",
    "def simple_stemmer(text):\n",
    "    ps=nltk.porter.PorterStemmer()\n",
    "    text= [ps.stem(word) for word in text]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mDg9vUiDShG-"
   },
   "source": [
    "Simple tokenizer that only keeps letters and numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eEofSDrQ9fg_"
   },
   "outputs": [],
   "source": [
    "def tokenize(line):\n",
    "    line = re.sub(r'[^a-zA-Z0-9]', ' ', unidecode.unidecode(line)) # remove punctuation\n",
    "    line = line.lower().split()  # lower case\n",
    "    return line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH2QW9FjSrJh"
   },
   "source": [
    "A standard vocabulary object class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "25siAXsXOJj9"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name = 'vocab'):\n",
    "        self.name = name\n",
    "        self._word2index = {}\n",
    "        self._word2count = {}\n",
    "        self._index2word = {}\n",
    "        self._n_words = 0\n",
    "\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in tokenize(sentence):\n",
    "            self.add_word(word)\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esAT3lG1S5cV"
   },
   "source": [
    "Make a bag of words frmo a sentence, given a vocabulary. Can return a bag of word counts or a a bag of word presences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "oKJP2y8g-TFE"
   },
   "outputs": [],
   "source": [
    "def multihot(s, vocab, preserve_counts = False):\n",
    "  tokens = np.array([vocab.word2index(t) for t in tokenize(s)])\n",
    "  mhot = np.zeros((tokens.size, vocab.num_words()))\n",
    "  mhot[np.arange(tokens.size), tokens] = 1\n",
    "  if preserve_counts:\n",
    "    return mhot.sum(0)\n",
    "  else:\n",
    "    return mhot.sum(0) >= 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4w8BflMUTLW_"
   },
   "source": [
    "If you have a reduced vocabulary, use this to replace out-of-vocab words. If you use this, you may want to merge it with `multihot` above to avoid tokenizing twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "LfvAfjEl7Z6j"
   },
   "outputs": [],
   "source": [
    "def unkit(s, vocab):\n",
    "  return ' '.join(list(map(lambda x: UNK if x not in vocab._word2index else x, tokenize(s))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YHZm-bAanrrf"
   },
   "source": [
    "# Part A: Download and Process Data (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-L61V7tVPok"
   },
   "source": [
    "This dataset contains the information in tables that are commonly used in Wikipedia biography pages. Each person has different rows of information pertaining to their notable accomplishments and details about their life. There are a large number of types of information that can appear as rows in the biography tables, however they are relatively uniform. We call the keys of the rows \"relations\".\n",
    "\n",
    "For example [Alexander Hamilton](https://en.wikipedia.org/wiki/Alexander_Hamilton) has information about the President he worked for as Secretary of State, birth date, date of death, parents' names, etc.\n",
    "\n",
    "The code below will download the dataset and process it to create two things:\n",
    "- `DB`: a hash table that map titles of biography wikipedia articles to table information. The table information is represented as a nested hash table containing relations as keys, and associated values. For example, `DB['alexander hamilton'] = {'party': 'federalist',\n",
    " 'spouse': 'elizabeth schuyler', ...}`\n",
    "- `VOCAB`: A vocabulary object that maps words to tokens and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "PYzcdAR2ntvm"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'wikipedia-biography-dataset'...\n",
      "Updating files:  68% (13/19)\n",
      "Updating files:  73% (14/19)\n",
      "Updating files:  78% (15/19)\n",
      "Updating files:  84% (16/19)\n",
      "Updating files:  89% (17/19)\n",
      "Updating files:  94% (18/19)\n",
      "Updating files: 100% (19/19)\n",
      "Updating files: 100% (19/19), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/rlebret/wikipedia-biography-dataset.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "RZEHbtftn17L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z00\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z01\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z02\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z03\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z04\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z05\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z06\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z07\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z08\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z09\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z10\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z11\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z12\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z13\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z14\n",
      "wikipedia-biography-dataset\\wikipedia-biography-dataset.z15\n",
      "        1 file(s) copied.\n"
     ]
    }
   ],
   "source": [
    "# !cat wikipedia-biography-dataset/wikipedia-biography-dataset.z?? > tmp.zip\n",
    "# !unzip -o tmp.zip\n",
    "# !rm tmp.zip\n",
    "\n",
    "# edting htis shiz so it works on windows\n",
    "!copy /b wikipedia-biography-dataset\\wikipedia-biography-dataset.z?? tmp.zip\n",
    "!tar -xf tmp.zip\n",
    "!del tmp.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rRYqmfSTcw0"
   },
   "source": [
    "Get all the wikipedia titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "mdVrlCf-4TAq"
   },
   "outputs": [],
   "source": [
    "train_titles = []\n",
    "with open(\"wikipedia-biography-dataset/train/train.title\", \"r\", encoding=\"utf-8\") as file:\n",
    "  for line in file:\n",
    "    train_titles.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOkCic0NTeuS"
   },
   "source": [
    "Boxes contain all the information, with each line corresponding to a title in `titles`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "C80pu63x4o-Y"
   },
   "outputs": [],
   "source": [
    "train_boxes = []\n",
    "with open(\"wikipedia-biography-dataset/train/train.box\", \"r\", encoding=\"utf-8\") as file:\n",
    "  for line in file:\n",
    "    train_boxes.append(line.rstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63cxS1DWTk0h"
   },
   "source": [
    "This will make the DB object, a dictionary of dictionaries for each wikipedia title, which is more or less the same as names. This function only keeps politicians (containing the \"office\" key term) and strips out information about images. It can be improved in many ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "Pr97KJLc4v7v"
   },
   "outputs": [],
   "source": [
    "# Make a dictionary of dictionaries\n",
    "def make_db(titles, boxes):\n",
    "  db = {} # The DB\n",
    "  # Iterate through titles\n",
    "  for i in tqdm(range(len(titles))):\n",
    "    box = boxes[i] # Grab the corresponding box information\n",
    "    d  = {} # Inner dictionary\n",
    "    # Build a dict for the ith entry\n",
    "    # grab each key:value pair\n",
    "    for pair in re.findall(r'([a-zA-Z_]+)[0-9]*\\:([\\w\\d]+)', box):\n",
    "      key, value = pair\n",
    "      # Do a bit of cleaning\n",
    "      key = key.strip()\n",
    "      value = value.strip()\n",
    "      # If the key contains the word image, we probably don't want to keep it\n",
    "      if 'image' not in key:\n",
    "        # The regex maintains underscores, strip those off\n",
    "        if key[-1] == '_':\n",
    "          key = key[:-1]\n",
    "        # Make a new entry in inner dictionary if we don't have one\n",
    "        if key not in d:\n",
    "          d[key] = value\n",
    "        # Keys with compound values are split up, which is annoying, so put them back together\n",
    "        else:\n",
    "          d[key] += ' ' + value\n",
    "    # If it has an office key, keep it.\n",
    "    if 'office' in d:\n",
    "      db[titles[i]] = d\n",
    "  return db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9hB1o63U45e"
   },
   "source": [
    "Build the vocab from the DB. Convert the whole thing into a string, tokenize it, and feed the surviving words into the vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "p13O5dcVvdZu"
   },
   "outputs": [],
   "source": [
    "def make_vocab(DB):\n",
    "  # Make the vocab object\n",
    "  vocab = Vocab()\n",
    "  # Tokenize the data by converting the entire DB into a string\n",
    "  tokens = tokenize(str(DB))\n",
    "  # Iterate through all the tokens (tqdm provides a progress bar)\n",
    "  for t in tqdm(tokens):\n",
    "    vocab.add_word(t)\n",
    "  return vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zkjf7mbAVA9H"
   },
   "source": [
    "If you want to discard rare words, this will rebuild the vocab. This is just an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "448Ww-BFtBL3"
   },
   "outputs": [],
   "source": [
    "def reduce_vocab(vocab, min_word_occurrence = 2):\n",
    "  # make a new vocab\n",
    "  vocab2 = Vocab(\"top\")\n",
    "  # Add the UNK token\n",
    "  vocab2.add_word(UNK)\n",
    "  # Iterate through vocabulary\n",
    "  for w in list(vocab._word2count.keys()):\n",
    "    count = vocab._word2count[w]\n",
    "    idx = vocab._word2index[w]\n",
    "    # If the word count passes threshold, add it to the new vocabulary object\n",
    "    if count >= min_word_occurrence:\n",
    "      vocab2.add_word(w)\n",
    "      vocab2._word2count[w] = count\n",
    "  # Return the new vocabulary object\n",
    "  return vocab2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVS602YPVE9N"
   },
   "source": [
    "Make the DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "cio_ZwncrGf0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 582659/582659 [01:11<00:00, 8119.19it/s] \n"
     ]
    }
   ],
   "source": [
    "DB = make_db(train_titles, train_boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ETLqtWtyVKLE"
   },
   "source": [
    "Make the VOCAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "lc7oi3p54SOb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2997937/2997937 [00:02<00:00, 1323067.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "VOCAB = make_vocab(DB)\n",
    "print(VOCAB.num_words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9AP3Dcle5VC"
   },
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiTQxguSXEuq"
   },
   "source": [
    "You may find it useful to save the processed dataset to your Google Drive.\n",
    "\n",
    "It is recommended that you save the file to your Google Drive. To mount your Google Drive, open the file icon on the left side of the screen to get to the option). To save the file in your Google Drive use the path `'drive/MyDrive/filename'`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "6DNxueU-gpz_"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "DwagcDU_Ut2Q"
   },
   "outputs": [],
   "source": [
    "# with open(\"drive/MyDrive/data\", \"wb\") as f:\n",
    "#   pickle.dump(DB, f, protocol=None, fix_imports=True, buffer_callback=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "WS6a8EtX_h0F"
   },
   "outputs": [],
   "source": [
    "# with open('drive/MyDrive/vocab', 'wb') as f:\n",
    "#     pickle.dump(VOCAB, f, protocol=None, fix_imports=True, buffer_callback=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzlH6IKhe_Qa"
   },
   "source": [
    "## Load processed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2AlkMbibXYWg"
   },
   "source": [
    "If you have saved the processed data in your Google Drive, you can re-load it with these commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ZCaiBhdvCa_C"
   },
   "outputs": [],
   "source": [
    "# with open(\"drive/MyDrive/vocab\", \"rb\") as f:\n",
    "#   VOCAB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "IinOyL_4C1Hx"
   },
   "outputs": [],
   "source": [
    "# with open(\"drive/MyDrive/data\", \"rb\") as f:\n",
    "#   DB = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcZcFUxrfCnB"
   },
   "source": [
    "## Data example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKKoOuLCVYxd"
   },
   "source": [
    "Get to know your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "dbLkOyB2pp_M"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'alexander hamilton',\n",
       " 'office': '1st united states secretary of the treasury senior officer of the army delegate to the congress of the confederation from new york',\n",
       " 'president': 'george washington john adams',\n",
       " 'term_start': 'september 11 1789 december 14 1799 november 3 1788 november 4 1782',\n",
       " 'term_end': 'january 31 1795 june 15 1800 march 2 1789 june 21 1783',\n",
       " 'predecessor': 'position established george washington egbert benson seat established',\n",
       " 'successor': 'oliver wolcott jr james wilkinson seat abolished seat abolished',\n",
       " 'birth_date': '11 january 1755',\n",
       " 'birth_place': 'charlestown nevis british west indies',\n",
       " 'death_date': 'july 12 1804 aged 47 or 49',\n",
       " 'death_place': 'new york city new york u',\n",
       " 'party': 'federalist',\n",
       " 'spouse': 'elizabeth schuyler',\n",
       " 'children': 'philip angelica alexander james alexander john church william stephen eliza holly phil',\n",
       " 'alma_mater': 'kings college new york',\n",
       " 'religion': 'presbyterian episcopalian convert',\n",
       " 'signature': 'alexander hamilton signaturert',\n",
       " 'allegiance': 'flag_of_new_york _ 1778 svg 23px new york 1775 1777 united states 1795 23px 1777 1800',\n",
       " 'branch': 'flag_of_new_york _ 1778 svg 23px new york company of artillery united states 1777 23px continental army 25px united states army',\n",
       " 'serviceyears': '1775 1776 militia 1776 1781 1798 1800',\n",
       " 'rank': '23px',\n",
       " 'article_title': 'alexander hamilton'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DB[\"alexander hamilton\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITACHJTWfE5m"
   },
   "source": [
    "# Part B: Implement the Key-Value Memory Network (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQqLE-fcI_43"
   },
   "source": [
    "This [paper](https://arxiv.org/abs/1606.03126) describes the key-value memory networks in detail, which is also sketched out below.\n",
    "\n",
    "A key-value memory network takes a natural language question. This question will be converted into a bag-of-words (i.e., a multihot) Call this $x$ and it is a 1D tensor of vocabulary length.\n",
    "\n",
    "![KVMemNet architecture](https://github.com/markriedl/kvmemnet-assignment/blob/32479dd1e88a9f8dfc72f11ccb8e9e0e1f78905f/kvmemnet-inside.png?raw=true)\n",
    "\n",
    "The KVMemNet will contain a linear layer (or embedding layer) that will produce a 1D embedding of the question $q=A(x)$.\n",
    "\n",
    "The KVMemNet will also take in a stack of keys as a tensor of shape `num_keys x vocab_size`. Each row is embedded using the same embedding, $k=A(keys)$, producing a tensor of shape `num_keys x embed_dim`. How this stack of keys is chosen will be discussed below.\n",
    "\n",
    "The KVMemNet will take in a third input, a stack of values associated with the stack of keys. This will also be of shape `num_values x vocab_size`. Each row is embedded using the same embedding, $v=A(values)$, producing a tensor of shape `num_keys x embed_dim`.\n",
    "\n",
    "The KVMemNet will also contain a second linear embedding layer, $B$. More on this later.\n",
    "\n",
    "Once we have `q`, `k`, and `v` embeddings, the next step is to use `q` and `k` to compute attention scores that can be applied against `v`. Think of $A$ as learning how to make questions and the keys that should match against values that have received the same treatment.\n",
    "\n",
    "The attention scores `p` are computed by taking the inner-product (`torch.inner()`) between `q` and `k`. The result will be a 1D tensor with `num_keys` length. Use softmax so that `p` contains scores between 1.0 and 0.0.\n",
    "\n",
    "You may be wondering why there isn't a non-linearity such as a sigmoid or ReLU after the linear layer. Softmax is a non-linearity.\n",
    "\n",
    "Next apply the `p` attention scores against `v` to apply a weight against each value in the stack of values. One should be highly weighted and the rest less weighted. Sum all the weighted values up to create a 1D tensor `o` of feature weights of length `embed_dim`. `p` can be thought of as how much of each value gets selected. Then they all get combined together and the feature weights are proportional to how much each value was attended to. The `torch.matmul()` can do the multiplication and summing in one step.\n",
    "\n",
    "The KVMemNet forward function should return this tensor of feature weights `o`.\n",
    "\n",
    "A quick note on `k` and `v`. We can't send the entire set of keys and values in our database through the network's forward function. Instead there should be a selection mechanism that selects just a subset of the database. This subset should contain the best key for the question $x$ to match against, and its corresponding value. We assume that a shallow selection process can narrow down the key-value pairs to a relatively small set, one of which will be best. For example, if the question involves \"Alexander Hamilton\", we can reasonably guess that the best key-value pair is in the part of the database associated with the named person.\n",
    "\n",
    "We are not done though. What about our linear layer $B$? Suppose variable `Y` contains our entire set of values in our databse as bags of words. $B$ is going to be used to embed our entire set of database values $y=B(Y)$. $B$ can be thought of as learning how to make all the values look like the feature weights output by the model such that the highest cosine similarity corresponds to the correct value taken from *all* values in the database.\n",
    "\n",
    "$B$ should live inside the KVMemNet object so that its parameters become trainable, but notice that we do not use $B$ in the KVMemNet's forward function. $B$ will get used to prepare the stack of all values in the database for training. It will bet used in the training loop but outside of the forward function. This is a bit unusual, but necessary to figure out the correct target (the true index of the best value to match against) for training.\n",
    "\n",
    "The above explantion only implements *single-hop* retrieval. *multi-hop* retrieval allows the results of one retrieval to inform a second (and third and so on) to get the right retrieval. This would be used in the case where the answer cannot be inferred directly from the question in a single retrieval, such as \"What was the founding date of the country that Alexander Hamilton was born in?\". To implement multi-hop retrieval, the KVMemNet will have additional linear layers $R_1...R_n$. Each $R_{i}$ will do a linear transform on `q` then attention will score and retrieve values as feature weights `o`. This will be sent to the next $R_{i+1}$ and so on until the hops are complete. This final `o` will be returned.\n",
    "\n",
    "For this assignment is is sufficient to only do *single-hop* retrieval.\n",
    "\n",
    "The above explanation does not include consideration of batching. You may want to add a batch dimension as the first dimension and input a batch as a set of questions, a set of stacks of keys, and a set of stacks of values. To do this, functions like `.inner()`, `.mm()`, and`.matmul()` will not work. Instead use `.bmm()` which handles batching correctly. You will probably need to do some `.squeeze()` and `.unsqueeze()` operations to make sure your tensors are the correct shapes.\n",
    "\n",
    "Instead of bag-of-words, one may also consider first converting each question, key, and value into a general set of embeddings such as [GLoVe](https://nlp.stanford.edu/projects/glove/). To do this one will need to consider how to combine words--convert each word into an embedding vector and then add the vectors together (or maybe average them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSf1SSX7bgdE"
   },
   "source": [
    "**Complete the key-value memory net code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "9zex3XZzw0-l"
   },
   "outputs": [],
   "source": [
    "class KVMemNet(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_dim):\n",
    "    super(KVMemNet, self).__init__()\n",
    "    self.vocab_size = vocab_size\n",
    "    self.embed_dim = embed_dim\n",
    "    ### YOUR CODE HERE\n",
    "    # ok so here we go\n",
    "    # so the first layer, layer a embeds the questions and kyes and values\n",
    "    # the second layer, layer b, embeds the db vals for comp\n",
    "    # standard stuff here, shouldnt be anything crazy\n",
    "    self.a_layer1 = nn.Linear(vocab_size, embed_dim)\n",
    "    self.b_layer1 = nn.LInear(vocab_size, embed_dim)\n",
    "\n",
    "  def forward(self, x, keys, values):\n",
    "    output = None\n",
    "    ### YOUR CODE BELOW\n",
    "    # So here is where we get cooking\n",
    "    # embedding the question, the keys, and the values\n",
    "    # this looks weird pushing them all t the same layer but \n",
    "    ques_embs = self.a_layer1(x)\n",
    "    key_embs = self.a_layer1(keys)\n",
    "    val_embs = self.a_layer1(values)\n",
    "\n",
    "    # notice we DO NOT use layer b here, it will be in the training loop\n",
    "    # later as mentioned in the instructions\n",
    "    \n",
    "    # Compute attn scores here\n",
    "    # inner product between q and each key\n",
    "    # just an nner prod here of the embeddings\n",
    "    # softmax the output\n",
    "    # and then Apply the attn weights, agan a matmul\n",
    "    matprod = torch.inner(ques_embs, key_embs)\n",
    "    matprod = F.softmax(matprod, dim=0)\n",
    "    o = torch.matmul(matprod, val_embs)\n",
    "    # forgot the key word again\n",
    "    output = o\n",
    "\n",
    "    ### YOUR CODE ABOVE\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8pshjz9hRps"
   },
   "source": [
    "\n",
    "# Synthetic Data Set\n",
    "\n",
    "This is a synthetic dataset. One way to test a model during development is to take a small piece of data and show that you can overfit a model. If you can't overfit an easily learned chunk of data, then you probably have something wrong in your code. In this case I have provided a small chunk of synthetic data that should be easy to learn.\n",
    "\n",
    "- The vocabulary is 20 word: 5 names, 5 relations, 5 question-words, 5 values\n",
    " - First 5 elements of the vocab are names (for example index 0 might be \"Hamilton\").\n",
    " - Second 5 elements of the vocab are relations (for example, \"born\", \"died\", \"occupation\").\n",
    " - Third 5 elements are random words that might be part of a query (for example, \"When was\").\n",
    " - Final 5 elements of the vocab are possible values (for example, \"1757\")\n",
    "- A \"question\" is a name (5, 1), relation (5, 1), some words (5, 1), and no values\n",
    "- The keys will all have the same name (5, 5) where each row is idential, relations (5, 5), no words, no values\n",
    "- Values will have no names, no relations, no words, and value vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "Qvuyp-g7grYw"
   },
   "outputs": [],
   "source": [
    "# Turn on a different relation on each row\n",
    "relations = torch.zeros(5, 5)\n",
    "relations.fill_diagonal_(1)\n",
    "\n",
    "# training data\n",
    "train_data = {}\n",
    "for i in range(5):\n",
    "  # Name associated with questions, keys, values\n",
    "  train_data[i] = (torch.cat([F.one_hot(torch.arange(0, 5))[i].repeat(5, 1),\n",
    "                         relations,\n",
    "                         torch.randint(0, 2, (5, 5)).float(),\n",
    "                         torch.zeros(5, 5)], dim=1),\n",
    "              torch.cat([F.one_hot(torch.arange(0, 5))[i].repeat(5, 1),\n",
    "                         relations,\n",
    "                         torch.zeros(5, 5),\n",
    "                         torch.zeros(5, 5)], dim=1),\n",
    "              torch.cat([torch.zeros(5, 5),\n",
    "                         torch.zeros(5, 5),\n",
    "                         torch.zeros(5, 5),\n",
    "                         torch.randint(0, 2, (5, 5)).float()], dim=1))\n",
    "  \n",
    "  # need this to put it on the gpu\n",
    "  Y = torch.cat([v[2] for v in list(train_data.values())], dim=0).to('cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6R_xIi_TZVp"
   },
   "source": [
    "# Part C: Train on Synthetic Data (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4slHUG5cPDtT"
   },
   "source": [
    "The following describes the steps to set up a training loop, including the training of the $B$ layer.\n",
    "\n",
    "![The KVMemNet being used in the training loop](https://github.com/markriedl/kvmemnet-assignment/blob/main/kvmemnet-outside.png?raw=true)\n",
    "\n",
    "- Create a model with the given vocabulary size and an embedding size that is equal to or smaller.\n",
    "- Loop through `N` epochs:\n",
    " - There are five names, loop through each name.\n",
    "   - Get a stack of questions, stack of keys, and stack of values from `DB_synth`.\n",
    "   - Loop through the relations. There is relation on each row of the keys and values.\n",
    "     - Get a single question, the `i`-th row in the questions pulled from `DB_synth` above.\n",
    "     - Compute the target: this is the `name*5 + i` element in `Y`.\n",
    "     - Run the singular question, stack of keys, and stack of values through the model and produce an output, which is a tensor of feature weights.\n",
    "     - Run all of `Y` through `model.B()` to get an embedded stack of values.\n",
    "     - Take the softmax of the inner product between the embedded stack of values from `Y` and the feature weight generated by the model.\n",
    "     - Compute the loss with `nn.CrossEntropyLoss`.\n",
    "     - Call `.backward()` on the loss.\n",
    "\n",
    "In addition to printing the loss (after every question or after every name in `DB_synth`) you can also print the target and the argmax of the softmax result to see if they match. Over time you should see the target and the argmax in agreement. For the purposes of this part of the project it is sufficient to test on the training set.\n",
    "\n",
    "Don't forget to move the model and the tensor to the GPU.\n",
    "\n",
    "You may want to speed up training by implementing batching. To do this, the model `forward()` needs to take tensors with an extra batching dimension as the first dimension. However, `.inner()`, `.mm()`, and `.matmul()` will not work properly. You will need to use `.bmm()` instead, which understands the first dimension is for batching. You will likely find that you need to perform some `.squeeze()` and `.unsqueeze()` operations. You can try batch-size of one, or take entire chunks (or even all synthetic data as a single, large batch). Try it different ways.\n",
    "\n",
    "Try training on the synthetic data first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zu1CMkabZkyU"
   },
   "source": [
    "You may make as many cells as necessary. Save your notebook outputs that plot loss and show it reducing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MqIdFVAbqr1"
   },
   "source": [
    "**Write code blocks below that create the `KVMemNet`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxso4hXaTKSK"
   },
   "outputs": [],
   "source": [
    "# Set up your KVMemNet, move it to the GPU, setup up optimizer (e.g., Adam), and criterion.\n",
    "# 5 fpr tje names, 5 for the relations, 5 for hte questions, and then 5 for the vlaues so should be 20 I think\n",
    "vocab_size_synt = 20\n",
    "# this just needs to be less than 20, well go 8 for quick training \n",
    "# proof of concept\n",
    "emebd_dimen_size = 8\n",
    "# set up the model\n",
    "model_synth = KVMemNet(vocab_size_synt, emebd_dimen_size).to(device)\n",
    "# setup adam optimizer and move to the gpu\n",
    "# learning rate 0.0015 seemed to get the job done\n",
    "optimizer_synth = optim.Adam(model_synth.parameters(), lr=0.0015)\n",
    "# standard criterion here\n",
    "crit = nn.CrossEntropyLoss()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "001D35pTb82K"
   },
   "source": [
    "**Write and run a training testing loop. Show that your training loop loss converges with a plot**\n",
    "\n",
    "To plot a loss curve, compute the mean loss per epoch and save it in a list:\n",
    "```\n",
    "x_axis.append(epoch_number)\n",
    "y_axis.append(mean_epoch_loss_for_this_epoch)\n",
    "plt.plot(x_axis, y_axis)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InBysHJfTTYX"
   },
   "outputs": [],
   "source": [
    "# Write your training loop here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nupd-Gr9P6vc"
   },
   "source": [
    "# Part D: Training on the Full Data (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r02Y9yvhPXy8"
   },
   "source": [
    "To train on the full data, you are going to need to do some pre-processing of the data.\n",
    "\n",
    "First, there are no \"questions\". You need to generate questions for each type of relation. There a number of ways to do this. The simplest is to just assume that a question is the name of a person and a relation, e.g., \"Alexander Hamilton birth date\". Another way would be to create templates for each type of relation. For example the \"birth date\" relation would have the following template: \"When was [name] born?\", filling in the [name]. Because there are a lot of different types of relations, you may want to remove the more obscure relations so you need fewer templates and also have a smaller vocabulary. Templates work well if the questions are expected to be almost identical to the templates. You may want to generate multiple templates per relation. Continuing the previous example, a second template would be: \"What is the birthdate of [name]?\".\n",
    "\n",
    "If you are feeling more ambitious, you could use GPT-J, GPT-NeoX, GPT-3 or ChatGPT to generate templates. It works decently well and you can get some variety of templates.\n",
    "\n",
    "The question should contain information about the person and some words that are representative of the relation even if the exact relation words aren't used (the KVMemNet should figure out that \"birthdate\" and \"born\" are correlated).\n",
    "\n",
    "You only put a subset of all key-value pairs into the KVMemNet. You need a technique for sub-selecting from all the key-value pairs in `DB`. You might just need the ones that are directly associated with the person (Alexander Hamilton has 23). You may need to mix in a few key-value pairs from another person's entries in the database to help ensure against accidental overfitting.\n",
    "\n",
    "The final challenge you will have in the training loop is that there may still be too many unique values in `Y` to encode and create one big tensor. In that case, you can at least use the values that you sent to the KVMemNet, along with as many other randomly selected values as you can fit into the GPU's memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3I3CTqeEfa1m"
   },
   "source": [
    "Create as many cells below as you need. Save the output of your training and testing functions, reporting loss during training and accuracy during testing. 5 points for a training loop that reduces loss. 5 points for a training function with a correct accuracy computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-970kRwZ26n"
   },
   "source": [
    "**Create a training dataset and a non-overlapping testing dataset**\n",
    "\n",
    "If CPU memory becomes a problem you might want to consider a `DataLoader` so that data can be stored on file and pulled up when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6VEnB_VS7Px"
   },
   "outputs": [],
   "source": [
    "# Create your training and test sets here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MI3b5x2zfl8g"
   },
   "source": [
    "**Create your `KVMemNet`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEKJ1KJxTBUJ"
   },
   "outputs": [],
   "source": [
    "# Set up your KVMemNet here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Ug_8X-2fwYD"
   },
   "source": [
    "**Write and run a training loop, showing a loss plot**\n",
    "\n",
    "You may find it handy to also test your network on the test data periodically as it trains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-K6rxBHFTEPO"
   },
   "outputs": [],
   "source": [
    "# Training loop goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7g5uXKRAgDjD"
   },
   "source": [
    "**Write the code for testing your model on the test data**\n",
    "\n",
    "Your training loop can call the testing loop. But make sure that you do one last test on the model after training completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecGNHCNhTGxK"
   },
   "outputs": [],
   "source": [
    "# Testing loop goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSfTnmWbf0Pr"
   },
   "source": [
    "**Suggestion:** Once you have a model that has decent accuracy, you may want to save it to your Google Drive using ``torch.save()`` and load it when working on the next part of the assignment using ``torch.load()``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRCn-LZmMNCp"
   },
   "source": [
    "# Part E: Use the Model (5 points)\n",
    "\n",
    "Given a question in natural language, turn it into a bag of words and feed it into the model with a set of plausible keys and values. Apply the output feature embedding to the full set of values and pick the value with argmax. Return the actual text inside that value (not the bag of words or embedding).\n",
    "\n",
    "That is, given a natural language question, you are asked to create the $q$ and pick a relevant subset of $k$ and $v$. Run the $q$, $k$, and $v$ through the model and get an answer to the original question.\n",
    "\n",
    "For example a question might be \"When was Alexander Hamilton born?\" Depending on how you pre-proessed your data, you may need to extract the entity and the relation.\n",
    "\n",
    "Write a function that takes in the `question` below, the data, and the model, and outputs the text answer, e.g., \"11 january 1755\". You must use your ``KVMemNet``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiRYRlRhfFUG"
   },
   "source": [
    "**Suggestion:** To process a question you will probably want to find the entity and the relation. You may use packages such as [NLTK](https://www.nltk.org/) (already imported), [SpaCY](https://spacy.io/), [Stanza](https://stanfordnlp.github.io/stanza/), or other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ofPVuNzWeIto"
   },
   "source": [
    "Change the question to test your implementation, but don't delete this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JQLNvGsfm3b3"
   },
   "outputs": [],
   "source": [
    "question = \"When was alexander hamilton born?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KGSewc7jeaj1"
   },
   "source": [
    "**Create your function for using the `KVMemNet` to answer a given question.**\n",
    "\n",
    "The function should take in the question, data, model, and any other parameters you need. The function should return a text string.\n",
    "\n",
    "You can create as many cells as necessary. Save the notebook cells showing one example of your input question and output answer. For grading we will look to see that your question is in natural language, the model is used, and the answer is in text. The example doesn't have to be correct. You will analyze your technique later in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sN0ZNtADTzDd"
   },
   "outputs": [],
   "source": [
    "# Create your functions here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N27-OtrzpT-u"
   },
   "source": [
    "# Part F: Reporting (15 points)\n",
    "\n",
    "Your report should answer the following three questions:\n",
    "\n",
    " **Q1:** What pre-processing of the data did you do? What motivated the design decisions and how did it impact training and any processing of natural language questions (Parts A and D)?\n",
    "\n",
    " Hint: This should help one understand any code modifications you made in Parts A and the first part of Part D. But you shouldn't use this to document your code (hopefully you commented your code with code comments and text cells above), but to justify your choices as well as to explain what worked and what didn't work.\n",
    "\n",
    " **Q2:** Report on your training on the real data (Part D). Show your loss curve and report on the testing accuracy. There are many ways to implement the training loop, particularly with the choice of keys and values. What decisions did you make when developing your training loop? Justify your decisions. How did they impact the training?\n",
    "\n",
    " Hint: This assignment doesn't grade you on how well your model learns---your solution will not be perfect. We focus more on how you worked through the process. This part of the report should show how well your solution worked, but also the intuition for why it works, and to document the things you tried that didn't work.\n",
    "\n",
    " **Q3:** Describe your technique on how you process natural language questions (Part E). Provide some examples of your technique answering questions correctly and some examples of your technique answering questions incorrectly. Discuss what causes the failure cases.\n",
    "\n",
    " Hint: You are not penalized for incorrectly answered questions---your model will not be perfect---we are looking for honest reflection. Preferably, show the example as code blocks running your model with notebook outputs saved.\n",
    "\n",
    " We have provided three prompts below. You can create as many text and code cells as necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8dovCuyuEvL"
   },
   "source": [
    "**Q1: Report on Data Pre-processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qhr3QwvuLBQ"
   },
   "source": [
    "Your text here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VBW1vSduN4U"
   },
   "source": [
    "**Q2: Report on Training and Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PT_9MtT2uQ7I"
   },
   "source": [
    "Your text here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d6YimJ0KuScb"
   },
   "source": [
    "**Q3: Report on Model Use**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j1majVIPuVEP"
   },
   "source": [
    "Your text here."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
