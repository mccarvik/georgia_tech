{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "RPRgDUI215D0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Distributional Semantics\n",
    "\n",
    "Distributional semantics models the \"meaning\" of words relative to other words that typically share the same context.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "* Read all the code. We don't ask you to write the training loops, evaluation loops, and generation loops, but it is often instructive to see how the models are trained and evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "CsFulvnv2N4V",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "W6WaMQ7AKP9s",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "VKofzznJxeat",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import hw4_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "a_STNMEi6dKN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# GLOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TnwdoLF02UU-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will first work with a pre-specified set of word embeddings, called [GLOVE](https://nlp.stanford.edu/projects/glove/). We will download it and set up a few basic global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "trlh1SHDxwDj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "GLOVE_MODEL = gensim.downloader.load('glove-wiki-gigaword-100')\n",
    "GLOVE_VOCAB_SIZE = len(GLOVE_MODEL.key_to_index)\n",
    "GLOVE_EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uJ-O6njz2s0A",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "pSy1RRPq2wol",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You must complete the code to compute analogies based on GLOVE embeddings.\n",
    "\n",
    "An analogy is of the form ``a:b :: c:d``.\n",
    "\n",
    "For example:\n",
    "\n",
    "``\n",
    "america : hamburger :: canada : ?\n",
    "``\n",
    "\n",
    "In this case we want to know what the `?` will be.\n",
    "\n",
    "To compute an analogy, first convert `a`, `b`, and `c` into vectors using GLOVE: ``glove[word]``.\n",
    "This will give you three vectors $\\overrightarrow{a}$, $\\overrightarrow{b}$, and $\\overrightarrow{c}$. Next compute $\\overrightarrow{d}=(\\overrightarrow{b}-\\overrightarrow{a})+\\overrightarrow{c}$.\n",
    "\n",
    "Unfortunately, $\\overrightarrow{d}$ might not correspond to any one word. Instead, find the `k` vectors that are most similar to $\\overrightarrow{d}$, and return the words that correspond to those vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "EDlJ_Fec6FmN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# analogy is a:b :: c:d\n",
    "# america:canada :: hamburger:?\n",
    "# DO NOT USE most_similar()\n",
    "def glove_analogy(glove, a, b, c, k):\n",
    "  d_list = None\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return d_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "ljcPug22_psM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "d = glove_analogy(GLOVE_MODEL, 'driver', 'car', 'pilot', k=10)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "arXEXrtT1wze",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- **TODO:** grading. we can look to see if specific words are returned within the top k return results. Create a test list and a set of potential answers. If all (or any) are in the returned list then success. Depending on how variable the results can be. -->\n",
    "Test: Check if the glove_analogy function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "mQE4-ri7p2sd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test A (5 points)\n",
    "ag.test_glove_analogy(GLOVE_MODEL, glove_analogy_fn=glove_analogy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "9xlR9WxF9lG9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2J2Ft_wX9pbs",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this part of the assignment, we will use word vectors to perform document retrieval. Given a query term, retrieve the `k` most related documents.\n",
    "\n",
    "To do this, we will need to embed all the documents in a dataset into a document vector that can be compared to the query term vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "AUx54Kd67n9N",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "IqazmKwB-Th8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The wikitext 2 dataset is a collection of high-quality documents from Wikipedia. We will load them into Panda data frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "CnXBk2DmyvvS",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "wiki_data_train = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"train\").shuffle()\n",
    "wiki_data_test = load_dataset(\"wikitext\", 'wikitext-2-v1', split=\"test\").shuffle()\n",
    "WIKI_TRAIN = pd.DataFrame(wiki_data_train)\n",
    "WIKI_TEST = pd.DataFrame(wiki_data_test)\n",
    "WIKI_ALL = pd.concat([WIKI_TRAIN, WIKI_TEST])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "usosrBje-g2Z",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "PPIFKe0w-iSl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This is a default tokenizer that comes with  the `torchtext` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "5In_zJGE1kCa",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "TOKENIZER = get_tokenizer(\"basic_english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "76J8jcul2sn0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "**Optional:** If you wish to change or modify the tokenization of a string, you can add your own code to the following function.\n",
    "\n",
    "We will use `my_tokenizer` for tokenization tasks from this point forward. It will work even if you do not modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "wwlhfq174B4s",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def my_tokenizer(string):\n",
    "  tokens = TOKENIZER(string)\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "1vJZdlc6_BOV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "RETRIEVAL_MAX_LENGTH = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ncE8kKVa_Q2N",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Embed Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2LRTIgeE_VMQ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the code below. The `embed_dataset()` function converts a Panda data frame into a numpy matrix of size `len(dataframe) x embedding_size`.\n",
    "\n",
    "Your code must iterate through all documents in `dataframe[text]`, tokenize each document, convert each token into a GLOVE vector, and take the average of embeddings in the same document as the embedding representation of the document.\n",
    "\n",
    "The numpy matrix is set up for you, so you must splice your vectors into the appropriate places in the matrix.\n",
    "\n",
    "**Hint:** create a numpy array for a document and use multi-dimensional numpy array slicing to insert it into the appropriate position in the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "WOwJf8xV16Ug",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def embed_dataset(dataframe, glove, tokenizer_fn=my_tokenizer, embed_size=GLOVE_EMBEDDING_SIZE, max_length=RETRIEVAL_MAX_LENGTH):\n",
    "  embedded_data = np.zeros((len(dataframe), max_length, embed_size))\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return embedded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "4pRdJ9tON4Mu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Unit test. Hard code some words in a small custom dataframe and hard-code the glove embeddings, just need to do a simple accuracy check. -->\n",
    "Test: Check if the `embed_dataset` function works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "k7nZVisKdGCQ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test B (10 points)\n",
    "ag.unit_test_embed_dataset(GLOVE_MODEL, embed_dataset_fn=embed_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "cDzZLG1z3T-S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "embedded_data = embed_dataset(WIKI_TRAIN, GLOVE_MODEL)\n",
    "print(embedded_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "aOAaFOi8CcW-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the code below. `retrieve_top_k` takes a word and finds the top `k` documents in `embedded_data`, a matrix of size `num_docs x max_doc_length x embed_size`. Return the *indexes* of the top `k` most similar documents to the input word.\n",
    "\n",
    "**Hint:** you should not need to write a loop. You should be able to do everything through numpy matrix manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "KpmFrJA_BlLZ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def retrieve_top_k(word, glove, embedded_data, k=10):\n",
    "  top_k_docs = []\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return top_k_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "VdYLH6cyDExx",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "word = 'mars'\n",
    "# Retrieve indexes of top k most similar documents to the above word\n",
    "top_k = retrieve_top_k(word, GLOVE_MODEL, embedded_data, k=10)\n",
    "print(\"indexes:\", top_k)\n",
    "# Get the dataframe for the top k\n",
    "WIKI_TRAIN.iloc[top_k]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "4yMDyA-7eRvh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test C (5 points)\n",
    "ag.unit_test_retrieve_top_k(GLOVE_MODEL, embed_dataset_fn=embed_dataset, retrieve_top_k_fn=retrieve_top_k, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uEHVza7_6iDC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2lSjyyyDDsH3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In this section, you will re-implement and train Word2Vec from scratch. There are two versions of Word2Vec. The first uses a continuous bag of words (CBOW) representation and the second uses skip grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "wE9wQg7xD9-b",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Create Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "gc1KF8ogEDGr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following is a standard class that stores a vocabulary. The vocabulary object can:\n",
    "* Tell you all the words: `get_words()`\n",
    "* Tell you how many words there are: `num_words()`\n",
    "* Map a word to an index: `word2index()`\n",
    "* Map an index to a word: `index2word()`\n",
    "\n",
    "Additionally, it has two helper functions used during set up:\n",
    "* `add_word()` adds a word to the vocabulary.\n",
    "* `add_sentence()` adds all the previously unknown words in a sentence to the vocabulary (simply splitting the sentence by blank spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "e7CBDQ5F21QV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# RUN THIS CELL BUT DO NOT EDIT IT\n",
    "UNK_token = 0   # Unknown '<unk>'\n",
    "UNK_symbol = '<unk>'\n",
    "\n",
    "class Vocab:\n",
    "  def __init__(self, name=''):\n",
    "    self.name = name\n",
    "    self._word2index = {UNK_symbol: UNK_token}\n",
    "    self._word2count = {UNK_symbol: 0}\n",
    "    self._index2word = {UNK_token: UNK_symbol}\n",
    "    self._n_words = 1\n",
    "\n",
    "  def get_words(self):\n",
    "    return list(self._word2count.keys())\n",
    "\n",
    "  def num_words(self):\n",
    "    return self._n_words\n",
    "\n",
    "  def word2index(self, word):\n",
    "    if word in self._word2index:\n",
    "      return self._word2index[word]\n",
    "    else:\n",
    "      return self._word2index[UNK_symbol]\n",
    "\n",
    "  def index2word(self, word):\n",
    "    return self._index2word[word]\n",
    "\n",
    "  def word2count(self, word):\n",
    "    return self._word2count[word]\n",
    "\n",
    "  def add_sentence(self, sentence):\n",
    "    for word in sentence.split(' '):\n",
    "      self.add_word(word)\n",
    "\n",
    "  def add_word(self, word):\n",
    "    if word not in self._word2index:\n",
    "      self._word2index[word] = self._n_words\n",
    "      self._word2count[word] = 1\n",
    "      self._index2word[self._n_words] = word\n",
    "      self._n_words += 1\n",
    "    else:\n",
    "      self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "JnFgIfwk_bwv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "0fn9kKpcFmnI",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The continuous bag of words model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "kXMUVEn-zbw_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "GNd7tZotL_EE",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change them\n",
    "CBOW_EMBED_DIMENSIONS = 100\n",
    "CBOW_WINDOW = 4\n",
    "CBOW_MAX_LENGTH = 50\n",
    "CBOW_BATCH_SIZE = 1024\n",
    "CBOW_NUM_EPOCHS = 2\n",
    "CBOW_LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "r-o1tDEfFp5d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before training the CBOW model, we must prepare the data for training. The CBOW model learns to predict a word based on the words to the left and the words to the right.\n",
    "\n",
    "This function takes a Pandas data frame and converts it into a regular python array consisting of `(x, y)` pairs where:\n",
    "* `y` is the index of a word in the corpus.\n",
    "* `x` is a list of indexes of words to the left of `y` and to the right of `y`.\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog\". For a window of size two, we would create the following data:\n",
    "1. `x=[the, quick, fox, jumped]`, `y=brown`\n",
    "2. `x=[quick, brown, jumped, over]`, `y=fox`\n",
    "3. `x=[brown, fox, over, the]`, `y=jumped`\n",
    "4. `x=[fox, jumped, the, lazy]`, `y=over`\n",
    "5. `x=[jumped, over, lazy, dog]`, `y=the`\n",
    "\n",
    "(Except instead of words, there would be the indices for each word in the vocabulary)\n",
    "\n",
    "This is done for every document in the data frame.\n",
    "\n",
    "`prep_cbow_data()` (below) will also simultaneously create the Vocab object.\n",
    "\n",
    "Thus `prep_cbow_data()` should return two values:\n",
    "* the `[(x1, y1) ... (xn, yn)]` data\n",
    "* the Vocab object. The vocab object is initialized for you but not populated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TOZwYPnLzf4C",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the `prep_cbow_data()` function. It takes a data frame and a tokenizer (`my_tokenizer()`) a window to either side of each word, and a max document length. The function should return two values as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "zIERa_oHDjiq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_cbow_data(data_frame, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  vocab = Vocab()\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return data_out, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "2W6tb7s7GMjG",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "CBOW_DATA, CBOW_VOCAB = prep_cbow_data(WIKI_TRAIN, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)\n",
    "print(\"len dataframe=\", len(WIKI_TRAIN), \"len data=\", len(CBOW_DATA))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "51d1xzfxQpFb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    " <!-- Unit test: Do something along the lines of figuring out how many words are in lines with greater than window*2+1 words. What I have below isn't quite matching what my solution above is producing. I'm not sure if my solution above has a bug or if my computation below is incorrect, or if it is just an approximation and we should allow some variance. -->\n",
    " Test: checking the size of the dataset and vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "mBHltjJgRgqD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# student check - Test D (10 points)\n",
    "ag.check_data_size_d(WIKI_TRAIN, CBOW_WINDOW, CBOW_DATA, CBOW_VOCAB, max_length=CBOW_MAX_LENGTH, tokenizer_fn=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GcabfK1P0EU3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Get Batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BqnjtfprK2jb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the following function. `get_batch()` will return a batch of data of the given size, starting at the given index.\n",
    "\n",
    "The function should return two values:\n",
    "1. A batch of `x` components of the data as a tensor of size `window*2 x batch_size`.\n",
    "2. A batch of `y` components of the data as a tensor array of length `window*2`.\n",
    "\n",
    "Both tensors should be moved to the GPU, if available, before being returned (Note: Gradescope will not have a GPU available).\n",
    "\n",
    "**Hint:** You should not need to write a loop. You can achieve what you need using numpy slicing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "OhL7ncHyBX9y",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def get_batch(data, index, batch_size=10):\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uwiTTNSlOpGK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "<!-- Unit test: make up some synthetic data, check if you get the right stuff out for a given idx and batch size. -->\n",
    "Test: Check if get back works properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "CrTiQ-WOULcZ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test E (10 points)\n",
    "ag.unit_test_get_batch(CBOW_DATA, CBOW_WINDOW, 10, get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "_jRn11uf0IrR",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "aq09TkU6UtMn",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the CBOW model specification.\n",
    "\n",
    "The CBOW model should contain:\n",
    "* An embedding layer `nn.Embedding`\n",
    "* A linear layer that transforms the embedding to the vocabulary\n",
    "\n",
    "The forward function will take the `x` component of the data--a list of `window*2` indices and produce a log softmax distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "k2eQkT122Am3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class CBOW(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    super(CBOW, self).__init__()\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x):\n",
    "    probs = None\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UZLSvKvWVmwT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "6vMBdIU8fz0h",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import traceback\n",
    "cbow_model = CBOW(CBOW_VOCAB.num_words(), CBOW_EMBED_DIMENSIONS)\n",
    "cbow_model.to(DEVICE)\n",
    "CBOW_CRITERION = nn.NLLLoss()\n",
    "try:\n",
    "  CBOW_OPTIMIZER = torch.optim.AdamW(cbow_model.parameters(), lr=CBOW_LEARNING_RATE)\n",
    "except:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "amzCDmGi6XeD",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test: Check the structure of CBOW model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "_RVJeGf1oJla",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test F (10 points)\n",
    "ag.test_cbow_structure(cbow_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "WcH2bQU50hJM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train the CBOW Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "kvk4DBN8Vpk5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "w5kYEX41LKKj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_cbow(model, data, num_epochs, batch_size, criterion, optimizer):\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "      x, y = get_batch(data, i, batch_size)\n",
    "      y_hat = model(x)\n",
    "      loss = criterion(y_hat, y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item())\n",
    "      optimizer.step()\n",
    "      if i % 100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "ynoHvwSTVsv6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "CTpPWtKtB07T",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  train_cbow(cbow_model, CBOW_DATA, num_epochs=CBOW_NUM_EPOCHS, batch_size=CBOW_BATCH_SIZE, criterion=CBOW_CRITERION, optimizer=CBOW_OPTIMIZER)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "FYtxtDr1Yf7U",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Test: Now that we have trained the CBOW model, we will be testing it on the `WIKI_TEST` dataset. Your CBOW model will need to achieve an accuracy of at least 30% to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "Y0nbJltGqR8B",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  for row in data_frame['text']:\n",
    "    tokens = tokenizer_fn(row)\n",
    "    token_ids = [vocab.word2index(w) for w in tokens]\n",
    "    if len(token_ids) >= (window*2)+1:\n",
    "      token_ids = token_ids[0:min(len(token_ids), max_length)]\n",
    "      for i in range(window, len(token_ids)-window):\n",
    "        x = token_ids[i-window:i] + token_ids[i+1:i+window+1]\n",
    "        y = token_ids[i]\n",
    "        data_out.append((x, y))\n",
    "  return data_out\n",
    "\n",
    "TEST_DATA = prep_test_data(WIKI_TEST, CBOW_VOCAB, tokenizer_fn=my_tokenizer, window=CBOW_WINDOW, max_length=CBOW_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "tnVcs4HQtXV_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - G (20 points)\n",
    "ag.test_cbow_performance(cbow_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "d7vzldCV_Xhm",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Skip Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qIcLx44H00tj",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The Skip Gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "UDGcOm6L_WxO",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Hyperparameters; feel free to change\n",
    "SKIP_EMBED_DIMENSIONS = 100\n",
    "SKIP_WINDOW = 4\n",
    "SKIP_MAX_LENGTH = 50\n",
    "SKIP_BATCH_SIZE = 1024\n",
    "SKIP_NUM_EPOCHS = 2\n",
    "SKIP_LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "tc1pGyIN02q0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Before training the Skip Gram model, we must prepare the data for training. The Skip Gram model learns to predict words to the left and right of a given word.\n",
    "\n",
    "This function takes a Pandas data frame and converts it into a regular python array consisting of `(x, y)` pairs where:\n",
    "* `x` is the index of a word in the corpus.\n",
    "* `y` is a list of indexes of words to the left of `x` or to the right of `x`.\n",
    "(Note the organization of the data is the opposite of the CBOW model)\n",
    "\n",
    "For example, consider the sentence \"The quick brown fox jumped over the lazy dog\". For a window of size two, we would create the following data:\n",
    "1. `x=brown`, `y=[the, quick, fox, jumped]`\n",
    "2. `x=fox`, `y=[quick, brown, jumped, over]`\n",
    "3. `x=jumped`, `y=[brown, fox, over, the]`\n",
    "4. `x=over`, `y=[fox, jumped, the, lazy]`\n",
    "5. `x=the`, `y=[jumped, over, lazy, dog]`\n",
    "\n",
    "(Except instead of words, there would be the indices for each word in the vocabular)\n",
    "\n",
    "This is done for every document in the data frame.\n",
    "\n",
    "`prep_skip_data()` (below) will also simultaneously create the Vocab object.\n",
    "\n",
    "Thus `prep_skip_data()` should return two values:\n",
    "* the `[(x1, y1) ... (xn, yn)]` data, where each `y` is a list of word indices\n",
    "* the Vocab object. The vocab object is initialized for you but not populated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "pKPaXDir-8ef",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_skip_gram_data(data_frame, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  vocab = Vocab()\n",
    "  ### BEGIN SOLUTION\n",
    "  ### END SOLUTION\n",
    "  return data_out, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "HqoX7hkU_gf6",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "SKIP_DATA, SKIP_VOCAB = prep_skip_gram_data(WIKI_TRAIN, my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "tsWgOrt-QVoB",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  SKIP_DATA[0]\n",
    "except:\n",
    "  print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "8a8y6vLKDe1f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Unit test: compute the number of data points that should be in SKIP_DATA and check the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "YpNy1Qg1sK1u",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - H (5 points)\n",
    "ag.check_data_size_h(WIKI_TRAIN, SKIP_WINDOW, SKIP_DATA, SKIP_VOCAB, max_length=SKIP_MAX_LENGTH, tokenizer_fn=my_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "jYzbwH0BDui4",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The Skip Gram Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "u03pc5UiD2db",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Complete the Skip Gram model specification.\n",
    "\n",
    "The Skip Gram model should contain:\n",
    "* An embedding layer `nn.Embedding`\n",
    "* A linear layer that transforms the embedding to the vocabulary\n",
    "\n",
    "The forward function will take the `x` component of the data--a single token index and produces a log softmax distribution over the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "QmBQWNxS5zAd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class SkipGram(nn.Module):\n",
    "  def __init__(self, vocab_size, embed_size):\n",
    "    super(SkipGram, self).__init__()\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x):\n",
    "    probs = None\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BqgDR_KbEGaf",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Unit test: check the layers and layer ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# initialize the model\n",
    "skip_model = SkipGram(SKIP_VOCAB.num_words(), SKIP_EMBED_DIMENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "s_Td11UJQIcv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test I (5 points)\n",
    "ag.test_skipgram_structure(skip_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UoCtW4HhDniu",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Train the Skip Gram Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "dfeBc-qu_MkK",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  SKIP_CRITERION = nn.NLLLoss()\n",
    "  SKIP_OPTIMIZER = torch.optim.AdamW(skip_model.parameters(), lr=SKIP_LEARNING_RATE)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "tpyRM2LjypCq",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_skipgram(model, data, num_epochs, batch_size, criterion, optimizer):\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for i in range(len(data)//batch_size):\n",
    "      x, y = get_batch(data, i, batch_size)\n",
    "      y_hat = model(x)\n",
    "      loss = None\n",
    "      # Calculate loss for every word in the context\n",
    "      for word in y.T:\n",
    "        if loss is None:\n",
    "          loss = criterion(y_hat, word)\n",
    "        else:\n",
    "          loss += criterion(y_hat, word)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item() / y.shape[1])\n",
    "      optimizer.step()\n",
    "      if i % 100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "RZOfUwgsEUQe",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  train_skipgram(skip_model, SKIP_DATA, num_epochs=SKIP_NUM_EPOCHS, batch_size=SKIP_BATCH_SIZE, criterion=SKIP_CRITERION, optimizer=SKIP_OPTIMIZER)\n",
    "except:\n",
    "    print(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VdEsOoV4PlTh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now that we have trained the Skipgram model, we will be using the `WIKI_TEST` dataset again for evaluation. Your Skipgram model will need to achieve at least 30% accuracy to pass the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "tmNDlKPRNR1G",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def prep_skip_gram_test_data(data_frame, vocab, tokenizer_fn, window=2, max_length=50):\n",
    "  data_out = []\n",
    "  for row in data_frame['text']:\n",
    "    tokens = tokenizer_fn(row)\n",
    "    token_ids = [vocab.word2index(w) for w in tokens]\n",
    "    if len(token_ids) >= (window*2)+1:\n",
    "        token_ids = token_ids[0:min(len(token_ids), max_length)]\n",
    "    for i in range(window, len(token_ids)-window):\n",
    "      x = token_ids[i]\n",
    "      y = token_ids[i-window:i]\n",
    "      y.extend(token_ids[i+1:i+1+window])\n",
    "      data_out.append((x, y))\n",
    "  return data_out\n",
    "\n",
    "TEST_DATA = prep_skip_gram_test_data(WIKI_TEST, SKIP_VOCAB, tokenizer_fn=my_tokenizer, window=SKIP_WINDOW, max_length=SKIP_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "fmVj4r1EtEr7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - Test J (20 points)\n",
    "ag.test_skip_performance(skip_model, TEST_DATA, 512, get_batch_fn=get_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Grading\n",
    "Please submit this .ipynb file to Gradescope for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check\n",
    "ag.final_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
