{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UT-whgM5YV8X",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Language Modeling\n",
    "\n",
    "A language model attempts to approximate the underlying statistics of a text corpus $P(tok_n | tok_1, tok_2, ..., tok_{n-1}; \\theta)$ where $\\theta$ is a set of learned parameters/weights. For the purposes of this notebook, tokens will be words. Language models can be used for a variety of applications, one of which being text generation. In this assignement we will be looking at Recurrent Neural Networks.\n",
    "\n",
    "**Tips:**\n",
    "- Read all the code. We don't ask you to write the training loops, evaluation loops, and generation loops, but it is often instructive to see how the models are trained and evaluated.\n",
    "- If you have a model that is learning (loss is decreasing), but you want to increase accuracy, try using ``nn.Dropout`` layers just before the final linear layer to force the model to handle missing or unfamiliar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VeQWnSY8CgT1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vcoLFG32u68g",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "editable": false,
    "id": "us-v9ZxoTHV1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# import the autograder tests\n",
    "import hw3a_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Fp1_pgBuY5zT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will build a *vocabulary*, which will act as a dictionary of all the words our systems will know about. It will also allow us to map words to tokens, which will be unique indexes in the vocabulary. This will further allow us to transform words into one-hot vectors, where a word is represented as a vector of the same length as the vocabulary wherein all values are zeros except for the *i*th element, where *i* is the token number of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "editable": false,
    "id": "1ybnRLpOTYwi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "SOS_token = 0    # A special token representing the start of a sequence\n",
    "EOS_token = 1    # A special token representing the end of a sequence\n",
    "\n",
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name                             # The name of the vocabulary\n",
    "        self._word2index = {}                        # Map words to token index\n",
    "        self._word2count = {}                        # Track how many times a word occurs in a corpus\n",
    "        self._index2word = {0: \"SOS\", 1: \"EOS\"}      # Map token indexs back into words\n",
    "        self._n_words = 2 # Count SOS and EOS        # Number of unique words in the corpus\n",
    "\n",
    "    # Get a list of all words\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    # Get the number of words\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    # Convert a word into a token index\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    # Convert a token into a word\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    # Get the number of times a word occurs\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    # Add all the words in a sentence to the vocabulary\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    # Add a single word to the vocabulary\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "8L7cXWAYacx3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "These are some helper functions to *normalize* texts, ie, make the text regular and remove some of the more problematic exceptions found in texts. This normalizer will make all words lowercase, trim plurals, and remove non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": false,
    "id": "SpAin8FuTsNr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert any unicode to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "0G1p53AuazDJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Download a corpus. This corpus is the ascii text of the book, *The Silmarillion*, by J.R.R. Tolkein. It has a lot of non-common words and names to illustrate how language models deal with such things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": false,
    "id": "i6IA64U3T3K7",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.txt already downloaded\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile('data.txt'):\n",
    "  print(\"data.txt already downloaded\")\n",
    "else:\n",
    "  print(\"downloading data.txt\")\n",
    "  !wget -O data.txt https://www.dropbox.com/s/pgvn1n7t4sjxt8r/silmarillion?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "EEhv3LTxb1If",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's read in the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": false,
    "id": "AfzYspyHUT7k",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Silmarillon Chapter 1\\n\\n\\nOf the Beginning of Days It is told among the wise that the First War began before Arda was full-shaped, and ere yet there was any thing that grew or walked upon earth; and for long Melkor had the upper hand. But in the midst of the war a spirit of great strength and hardihood came to the aid of the Valar, hearing in the far heaven that there was battle in the Little Kingdom; and Arda was filled with the sound of his laughter. So came Tulkas the Strong, whose anger passes like a mighty wind, scattering cloud and darkness before it; and Melkor fled before his wrath and his laughter, and forsook Arda, and there was peace for a long age. And Tulkas remained and became one of the Valar of the Kingdom of Arda; but Melkor brooded in the outer darkness, and his hate was given to Tulkas for ever after.\\n\\nIn that time the Valar brought order to the seas and the lands and the mountains, and Yavanna planted at last the seeds that she had long devised. And since, when th'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data.txt'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "rPga6_LmcDWJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Normalize the text and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": false,
    "id": "FV_ib2sZYMqo",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "normalized_text = normalize_string(text)\n",
    "VOCAB = Vocab(\"text\")\n",
    "VOCAB.add_sentence(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VBei0ecBcHAy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Make training and testing data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": false,
    "id": "HMc-3xncyc_Q",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 tokens\n",
      "[ 2  3  4  5  2  6  5  7  8  9 10 11  2 12 13  2 14 15 16 17 18 19 20 21\n",
      " 22 23 24 25 19 26 27 13 28 29 30 31 32 22 33 34 35 36  2 37 38 39 40 41\n",
      "  2 42  5  2 15 43 44  5 45 46 22 47 48 49  2 50  5  2 51 52 41  2 53 54\n",
      " 13 25 19 55 41  2 56 57 22 18 19 58 59  2 60  5 61 62 39 63 48 64  2 65\n",
      " 66 67 68 69]\n"
     ]
    }
   ],
   "source": [
    "# Convert every word into a token and build a numpy array of tokens\n",
    "encoded_text = np.array([VOCAB.word2index(word) for word in normalized_text.split()])\n",
    "print(\"The first 100 tokens\")\n",
    "print(encoded_text[:100])\n",
    "# get the validation and the training data\n",
    "test_split = 0.1\n",
    "test_idx = int(len(encoded_text) * (1 - test_split))\n",
    "TRAIN = encoded_text[:test_idx]\n",
    "TEST = encoded_text[test_idx:]\n",
    "# Decrease the size of the training set to make the assignment more tractable\n",
    "TRAIN = TRAIN[:len(TRAIN)//10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "niQcV2wIFq29",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# RNN\n",
    "\n",
    "**Complete the code for an RNN.** An RNN takes a one-hot vector for a single word and a hidden state vector (initially all zeroes), compresses it to a hidden state, and then decompresses it. The decompressed word is tested against the **next** word in the training sequence using cross-entropy loss. The RNN also produces the hidden state vector to pass in with the next word in the training sequence. The neural network must guess the next word as well as learn to create a hidden state vector that helps the next iteration make a better word guess.\n",
    "\n",
    "The neural network's forward function should take two inputs:\n",
    "- The input word (`x`) represented as a one-hot vector of size `1 x vocab_size`\n",
    "- The hidden state, a `1 x hidden_size` vector.\n",
    "\n",
    "A brief note on batching: We will not be using batching in this assignment. But there must always be a batching dimension in our input and output tensors. Thus we will have a batching dimension size of 1 and our tensors will often be of a shape `1 x something`.\n",
    "\n",
    "The neural network architecture shoud concatenate the `x` and the `hidden_state` to make one big long vector. The neural network get's to learn through weights whether to draw from the one-hot (or certain parts of the one-hot) or the hidden state when trying to predict the next token.\n",
    "\n",
    "The neural network should have two affine transformations (`nn.Linear` modules). The first should transform the a tensor of size `1 x (vocab_size + hidden_size)` into a tensor of size `1 x hidden_size`, followed by a sigmoid activation. This compresses the information from the input and forces the neural network to make compromises about what is important. Think of the sigmoid a gate that says yes or no to different combinations of inputs. The second affine transform should be from a tensor of `1 x hidden_size` to one of `1 x vocab_size`. The values in resultant tensor are the raw scores for each possible token in the vocabulary. The forward function should run these scores through a log softmax.\n",
    "\n",
    "The output of the forward function should be two values: a tensor of log-scale scores for each token, and a new hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "y0r-3PvoRV3S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Model (30 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "editable": true,
    "id": "vhb3w8CUqydH",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2ef3f98851d99506",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class MyRNN(nn.Module):\n",
    "  def __init__(self, input_size, hidden_size, output_size=None):\n",
    "    super(MyRNN, self).__init__()\n",
    "    # If output_size is not given, use input_size\n",
    "    if output_size is None:\n",
    "      output_size = input_size\n",
    "    self.input_size = input_size      # the size of the input vocabulary\n",
    "    self.hidden_size = hidden_size    # the size of the hidden state\n",
    "    self.output_size = output_size    # the size of the output vocabulary (if different)\n",
    "    ### BEGIN SOLUTION\n",
    "    # alright so first layer\n",
    "    # we gotta send in input_size for the whole cab and then the hidden size, and then connect to hidden size\n",
    "    input_vector = input_size + hidden_size\n",
    "    self.lin_layer1 = nn.Linear(input_vector, hidden_size)\n",
    "    # now we got the second layer\n",
    "    # and this is jsut gonna be hidden size connected to output size from above\n",
    "    self.lin_layer2 = nn.Linear(hidden_size, output_size)\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x, hidden_state):\n",
    "    output = None\n",
    "    hidden = None\n",
    "    ### BEGIN SOLUTION\n",
    "    # now we gotta connect them all\n",
    "    # first connect the input one hot vector for the word x\n",
    "    # just like above\n",
    "    input_vals = (x, hidden_state)\n",
    "    # i think we need dim=1 here\n",
    "    ff_layer = torch.cat(input_vals, dim=1)\n",
    "    # sigmoid activation incoming for our sigmoid gate\n",
    "    # might have misnamed ff_layer now that im looking at it, might have to circle back\n",
    "    hid_sig = torch.sigmoid(self.lin_layer1(ff_layer))\n",
    "    # sending the activatied outputs thru the second layer\n",
    "    output_vals_pre_softmax = self.lin_layer2(hid_sig)\n",
    "    # finally, softmax this guy to get our finally prediction for the next word\n",
    "    # This is the F func we imported above from torch.nn.functional to quickly to the log_softmax\n",
    "    output_word_vector = F.log_softmax(output_vals_pre_softmax, dim=1)\n",
    "\n",
    "    # crap have to change var names for below\n",
    "    output = output_word_vector\n",
    "    hidden = hid_sig\n",
    "\n",
    "    ### END SOLUTION\n",
    "    return output, hidden\n",
    "\n",
    "  # Make an initial hidden state with some randomness to the values\n",
    "  def init_hidden(self):\n",
    "    return nn.init.kaiming_uniform_(torch.empty(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "QNlqo68hdXT8",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Construct the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "editable": true,
    "id": "BimhoCMSSgRd",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# it's ok to modify this cell\n",
    "RNN_HIDDEN_SIZE = 256\n",
    "RNN_LEARNING_RATE = 0.001\n",
    "RNN_NUM_EPOCHS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": false,
    "id": "xVQa8yuJq9Sv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Create the network\n",
    "rnn = MyRNN(VOCAB.num_words(), RNN_HIDDEN_SIZE)\n",
    "\n",
    "# Create the loss function and optimizer\n",
    "criterion_rnn = nn.NLLLoss()\n",
    "optimizer_rnn = torch.optim.Adam(rnn.parameters(), lr=RNN_LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": false,
    "id": "M6o9RfTm4A5q",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "SigmoidBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "SigmoidBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "SigmoidBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "Test A: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 10 to receive full credit (no partial credit)\n",
    "ag.unit_test_RNN_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the following function.\n",
    "\n",
    "`token2onehot()` takes a token—a number—and converts it to a one-hot tensor of the shape `1 x vocab_size`. All values should be zeros except for one element, which should be a `1`.\n",
    "\n",
    "`get_rnn_x_y_()` should return the `x` and `y` for a recurrent neural network.\n",
    "\n",
    "- The `x` return value should be a tensor containing a one-hot vector representing the word at position `index` and have a shape of `1 x vocab_size` (the batch size is 1).\n",
    "- The `y` return value should be the token of the word at position `index+1` and be a vector with a single value in it. That is, it should not be a scalar but a vector of length 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token2onehot(token, vocab_size = VOCAB.num_words()):\n",
    "  one_hot = None\n",
    "  ### BEGIN SOLUTION\n",
    "  # so first lets get zeroes\n",
    "  one_hot_vector = [0] * vocab_size\n",
    "  # set our word guy\n",
    "  one_hot_vector[token] = 1\n",
    "\n",
    "  # one_hot = one_hot_vector\n",
    "  # need to make it a tensor, think thats my problem?\n",
    "  # one_hot = torch.tensor(one_hot_vector)\n",
    "  one_hot = torch.tensor(one_hot_vector).unsqueeze(0)\n",
    "  ### END SOLUTION\n",
    "  return one_hot\n",
    "\n",
    "def get_rnn_x_y(data, index, vocab_size = VOCAB.num_words()):\n",
    "  x = None\n",
    "  y = None\n",
    "  ### BEGIN SOLUTION\n",
    "  #  so first lets use our func\n",
    "  x = token2onehot(data[index], vocab_size)\n",
    "\n",
    "  # now gotta get our y which is our next work\n",
    "  next_ind = index + 1\n",
    "  # needed to add brackets here, causing errors down later on dims\n",
    "  y = torch.tensor([data[next_ind]])\n",
    "  # got an error so converting to long, no idea if this fixes it\n",
    "  y = y.to(torch.long)\n",
    "\n",
    "  ### END SOLUTION\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test B: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 10 to receive full credit (no partial credit)\n",
    "ag.unit_test_token2onehot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test C: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 100 to receive full credit (no partial credit)\n",
    "ag.unit_test_get_xy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "RDFRH_FUj_4F",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The following is the training loop. You can see how your `get_rnn_x_y()` is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "editable": false,
    "id": "97XvkPmsuC2Q",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_rnn(model, optimizer, criterion, data, num_epochs):\n",
    "  model.train()\n",
    "  for epoch in range(num_epochs):\n",
    "    hidden_state = model.init_hidden()\n",
    "    losses = []\n",
    "    for i in range(len(data)-2):\n",
    "      x, y = get_rnn_x_y(data, i)\n",
    "      x = x.float()\n",
    "      output, new_hidden = model(x, hidden_state)\n",
    "      hidden_state = new_hidden.detach()\n",
    "      loss = criterion(output, y)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      losses.append(loss.item())\n",
    "      nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "      optimizer.step()\n",
    "      if i%100 == 0:\n",
    "        print('iter', i, 'loss', np.array(losses).mean())\n",
    "    print('epoch', epoch, 'loss', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "kyvFAzHzvmJz",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 8.551855087280273\n",
      "iter 100 loss 7.678191888450396\n",
      "iter 200 loss 7.035225713430946\n",
      "iter 300 loss 7.14623060832388\n",
      "iter 400 loss 7.171271779739351\n",
      "iter 500 loss 7.143751794111705\n",
      "iter 600 loss 7.046541880748038\n",
      "iter 700 loss 7.029547111369063\n",
      "iter 800 loss 6.94512294643976\n",
      "iter 900 loss 6.9272182005623995\n",
      "iter 1000 loss 6.869948673736561\n",
      "iter 1100 loss 6.800549690069446\n",
      "iter 1200 loss 6.810482564069746\n",
      "iter 1300 loss 6.802004558182422\n",
      "iter 1400 loss 6.853353730318463\n",
      "iter 1500 loss 6.8792406066031075\n",
      "iter 1600 loss 6.88372741927213\n",
      "iter 1700 loss 6.87605426071112\n",
      "iter 1800 loss 6.873933123489806\n",
      "iter 1900 loss 6.894917487627829\n",
      "iter 2000 loss 6.918327286772463\n",
      "iter 2100 loss 6.9120519003772785\n",
      "iter 2200 loss 6.884153267867129\n",
      "iter 2300 loss 6.857835144775943\n",
      "iter 2400 loss 6.858739093734244\n",
      "iter 2500 loss 6.858714525268727\n",
      "iter 2600 loss 6.833544366064552\n",
      "iter 2700 loss 6.82572900403742\n",
      "iter 2800 loss 6.814327534566135\n",
      "iter 2900 loss 6.823407849395164\n",
      "iter 3000 loss 6.825853344878528\n",
      "iter 3100 loss 6.815223125537262\n",
      "iter 3200 loss 6.822124371488405\n",
      "iter 3300 loss 6.820894757533427\n",
      "iter 3400 loss 6.829518505595285\n",
      "iter 3500 loss 6.846670464085566\n",
      "iter 3600 loss 6.872688608942877\n",
      "iter 3700 loss 6.8917071019852685\n",
      "iter 3800 loss 6.908704431331086\n",
      "iter 3900 loss 6.922018623755425\n",
      "iter 4000 loss 6.92016758048156\n",
      "iter 4100 loss 6.900641270132653\n",
      "iter 4200 loss 6.916591966359339\n",
      "iter 4300 loss 6.930499788440402\n",
      "iter 4400 loss 6.947922992099553\n",
      "iter 4500 loss 6.962691027809318\n",
      "iter 4600 loss 6.97554613662476\n",
      "iter 4700 loss 6.987680092311316\n",
      "iter 4800 loss 6.993346030055124\n",
      "iter 4900 loss 6.979925041659904\n",
      "iter 5000 loss 6.9771367934340836\n",
      "iter 5100 loss 6.982120849412695\n",
      "iter 5200 loss 6.985193217288895\n",
      "iter 5300 loss 6.98191561977316\n",
      "iter 5400 loss 7.000032832480827\n",
      "iter 5500 loss 7.009432946588967\n",
      "iter 5600 loss 6.9954432345909465\n",
      "iter 5700 loss 6.9905859262686905\n",
      "iter 5800 loss 6.993817629228068\n",
      "iter 5900 loss 6.990881152141703\n",
      "iter 6000 loss 6.987338592997791\n",
      "iter 6100 loss 6.969579175561593\n",
      "iter 6200 loss 6.955404194799705\n",
      "iter 6300 loss 6.948707744859926\n",
      "iter 6400 loss 6.940579013030355\n",
      "iter 6500 loss 6.957378709450114\n",
      "iter 6600 loss 6.941031416786961\n",
      "iter 6700 loss 6.931893811494873\n",
      "iter 6800 loss 6.928255799140602\n",
      "iter 6900 loss 6.916279027162644\n",
      "iter 7000 loss 6.904950340306516\n",
      "iter 7100 loss 6.89849411316899\n",
      "iter 7200 loss 6.897049224996646\n",
      "iter 7300 loss 6.891250386754547\n",
      "iter 7400 loss 6.891343554364332\n",
      "iter 7500 loss 6.891852942896182\n",
      "iter 7600 loss 6.890223893577716\n"
     ]
    }
   ],
   "source": [
    "train_rnn(rnn, optimizer_rnn, criterion_rnn, TRAIN, RNN_NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "R5JxdK30Cv8u",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Test (20 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "L7yKgJmZ5iZ5",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Even if loss went down, we can't make any guarantees about what the network will do on unseen sequences. To evaluate, we will measure **perplexity**, how much the network is confused by data. As you adjust hyperparameters and retrain the model you may notice that a model with a lower loss on training data doesn't necessarily produce a model with lower perplexity on test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "Ztoctsl_lu1r",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test D: 0/20\n",
      "Error during execution of Test D:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\mccar\\georgia_tech\\nlp\\hw3a-student\\hw3a_tests.py\", line 304, in evaluate_rnn\n",
      "    x = x.float()\n",
      "        ^^^^^^^\n",
      "AttributeError: 'list' object has no attribute 'float'\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value < 2000 to receive full credit (no partial credit)\n",
    "ag.evaluate_rnn(rnn, TEST, criterion_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "fPbSy48UCzws",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Generate (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "g8oloDU5C2-S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's use the RNN to generate some text. This is going to take a bit of set up. We need to take an input prompt---the start of the text---and tokenize it. Then we need a hidden state that represents the prompt, so we have to run the input prompt through the RNN to build up the hidden state. Then we can finally let the RNN loose to generate new text by feeding the outputs of the RNN (and the hidden state) back into the RNN as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "QMlveLOcm8A9",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input prompt: the First War began\n",
      "input tokens: [2, 14, 15, 16] \n",
      "\n",
      "Prepping hidden state:\n",
      "\n",
      "current token: 2 the\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[42], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Get the hidden state that represents the input prompt\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrepping hidden state:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m hidden_state \u001b[38;5;241m=\u001b[39m \u001b[43mprep_hidden_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrnn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Generate a continuation by sampling from the RNN and then feeding the predicted output\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# back into the RNN over and over. The default sampling is argmax.\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_rnn\u001b[39m(rnn, num_new_tokens, token, hidden_state, fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m d:d\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem(), verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     40\u001b[0m   \u001b[38;5;66;03m# Keep generating more by feeding the predicted output back into the RNN as input\u001b[39;00m\n\u001b[0;32m     41\u001b[0m   \u001b[38;5;66;03m# Start with the last token of the input prompt and the newly prepped hidden state\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[42], line 24\u001b[0m, in \u001b[0;36mprep_hidden_state\u001b[1;34m(tokenized_input, rnn, verbose)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Get the one-hot for the current token\u001b[39;00m\n\u001b[0;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m token2onehot(token)\n\u001b[1;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run the current one-hot and hidden state through the RNN\u001b[39;00m\n\u001b[0;32m     26\u001b[0m output, hidden_state \u001b[38;5;241m=\u001b[39m rnn(x, hidden_state)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "\n",
    "# We need to make a hidden_state that is representative of what is in the input prompt\n",
    "def prep_hidden_state(tokenized_input, rnn, verbose=False):\n",
    "  # Get an initial hidden state\n",
    "  hidden_state = rnn.init_hidden()\n",
    "  # Run the input prompt through the RNN to build up the hidden state.\n",
    "  # Discard the outputs (we are not trying to make predictions) until we get to the end\n",
    "  for token in tokenized_input:\n",
    "    if verbose:\n",
    "      print(\"current token:\", token, VOCAB.index2word(token))\n",
    "    # Get the one-hot for the current token\n",
    "    x = token2onehot(token)\n",
    "    x = x.float()\n",
    "    # Run the current one-hot and hidden state through the RNN\n",
    "    output, hidden_state = rnn(x, hidden_state)\n",
    "    # Get the highest predicted token\n",
    "    next_token = output.argmax().item()\n",
    "    if verbose:\n",
    "      print(\"predicted next token:\", next_token, VOCAB.index2word(next_token), '\\n')\n",
    "  return hidden_state\n",
    "\n",
    "# Get the hidden state that represents the input prompt\n",
    "print(\"Prepping hidden state:\\n\")\n",
    "hidden_state = prep_hidden_state(tokenized_input, rnn, verbose=True)\n",
    "\n",
    "# Generate a continuation by sampling from the RNN and then feeding the predicted output\n",
    "# back into the RNN over and over. The default sampling is argmax.\n",
    "def generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=lambda d:d.argmax().item(), verbose=False):\n",
    "  # Keep generating more by feeding the predicted output back into the RNN as input\n",
    "  # Start with the last token of the input prompt and the newly prepped hidden state\n",
    "  if verbose:\n",
    "    print(\"Generating continuation:\\n\")\n",
    "  continuation = []\n",
    "  for n in range(num_new_tokens):\n",
    "    if verbose:\n",
    "      print(\"current token:\", token, VOCAB.index2word(token))\n",
    "    # Get the one-hot for the current token\n",
    "    x = token2onehot(token)\n",
    "    x = x.float()\n",
    "    # Run the current one-hot through the RNN\n",
    "    output, hidden_state = rnn(x, hidden_state)\n",
    "    # Predict the next token\n",
    "    next_token = fn(output)\n",
    "    if verbose:\n",
    "      print(\"predicted next token:\", next_token, VOCAB.index2word(next_token), '\\n')\n",
    "    # Remember the new token\n",
    "    continuation.append(next_token)\n",
    "    # update the current\n",
    "    token = next_token\n",
    "  return continuation\n",
    "\n",
    "# Generate the continuation. Use the argmax function to sample from the RNN's outputs\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(rnn, num_new_tokens, token, hidden_state, verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(continuation_text)\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "D7AaCW1qwgT-",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Odds are good that you got an output that was highly repetitive. This is in part because we always take the `argmax` of the output logits. There are sequences in the corpus that are highly probable, so by sampling the most likely logit, we are going to get trapped in a local max."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "2YCYDJPo4-gi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Instead, we need to treat the output of the RNN as a distribution and *sample* from the distribution to proabilisticially choose the next token, proportional to how highly activated each token is.\n",
    "\n",
    "**Complete the following function.** `my_sample()` should take a tensor of log probabilities for each token in the the vocabulary (the output of the RNN). It should probabilistically sample from this distribution and return a highly probable next token as an integer.\n",
    "\n",
    "**Hints:** Consider using `torch.multinomial`. Remember, your input is in log scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "Nu-_4kGv4Yv8",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-69f4926e06aa0885",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def log_to_percentage_probs(log_probs):\n",
    "  perc_probs = torch.exp(log_probs)\n",
    "  return perc_probs\n",
    "\n",
    "def my_sample(log_probs):\n",
    "  token = None\n",
    "  ### BEGIN SOLUTION\n",
    "  # so we have log probs so neec to convert them\n",
    "  perc_probs = log_to_percentage_probs(log_probs)\n",
    "  # and then to get our token, use the multinomial function from the hint\n",
    "  # just one pull for now\n",
    "  draws = 1\n",
    "  token = torch.multinomial(perc_probs, draws).item()\n",
    "  ### END SOLUTION\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "iop8mlR7gShH",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score:  95\n",
      "Test E: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value > 90 to receive full credit (no partial credit)\n",
    "ag.unit_test_my_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "GA_n9Dr6Bmhp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Run the cell below a few times and see what gets generated with your sampling technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "brCksz3sw4cr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "# Get an initial hidden state\n",
    "hidden_state = prep_hidden_state(tokenized_input, rnn)\n",
    "\n",
    "# Generate the continuation. Use my_sample\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=my_sample, verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(' '.join(continuation_text))\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "BUPq4bIjRV3i",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## RNN---Optimize (10 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "k-yYCls275hX",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Well, that is probably crazy and non-sensical. If the distribution has a lot of nearly-equal probability candidates, the sampling technique you wrote probably makes some choices that look random. But it probably isn't stuck in a local max.\n",
    "\n",
    "How do we fix this? We introduce something called **temperature**. Temperature is a value between 0.0 and 1.0 that makes higher probability tokens more probable and less probable tokens less probable.\n",
    "\n",
    "**Complete the following function.** It should operate exactly like `my_sample()` except that it should divide the probabilities (between 0.0 and 1.0) by temperature.\n",
    "\n",
    "Dividing by `temperature=1.0` leaves the probability distribution unchanged. As temperature gets smaller, approaching 0.0, the high probability tokens approach infinity faster than low probability tokens. The distribution spreads out along an exponential curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "AB-26jCc-hvX",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7a18829f8109ede9",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def my_temperature_sample(log_probs, temperature=1.0):\n",
    "  token = None\n",
    "  ### BEGIN SOLUTION\n",
    "  # use func from above again\n",
    "  perc_probs = log_to_percentage_probs(log_probs)\n",
    "  # divide by temp\n",
    "  numerator = perc_probs / temperature\n",
    "  # get the sum of probs\n",
    "  denominator = numerator.sum(dim=1, keepdim=True)\n",
    "  val = numerator / denominator\n",
    "  # use our multinomial again\n",
    "  draws = 1\n",
    "  token = torch.multinomial(val, draws).item()\n",
    "  ### END SOLUTION\n",
    "  return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "spkbQciwhgM0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return True to receive full credit (no partial credit)\n",
    "ag.unit_test_my_temperature_sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "uhnhTbWIBEl0",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "One more time. This time, play around with the temperature value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "id": "UmMpnUNxCOVM",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# set the temperature - it's ok to modify this cell\n",
    "RNN_TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "id": "fXT4cW4BBJWV",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "# Get an initial hidden state\n",
    "hidden_state = prep_hidden_state(tokenized_input, rnn)\n",
    "\n",
    "# Generate the continuation. Use my_sample\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=lambda d:my_temperature_sample(d, RNN_TEMPERATURE), verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(' '.join(continuation_text))\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "qXU9cSZjRV3k",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Grading\n",
    "\n",
    "Please submit this .ipynb file to Canvas for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check\n",
    "ag.final_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UT-whgM5YV8X",
    "VeQWnSY8CgT1",
    "UY0I2wl9D61t"
   ],
   "provenance": [
    {
     "file_id": "1vvRsIWcSStCzw7Z28SRnMkJfaENRnwb9",
     "timestamp": 1686165584566
    },
    {
     "file_id": "1Io6bUVyvhylwUuNavlc5pkVdCUq1fa13",
     "timestamp": 1682088867444
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
