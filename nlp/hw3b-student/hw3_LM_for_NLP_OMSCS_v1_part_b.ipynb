{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "UT-whgM5YV8X",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Language Modeling\n",
    "\n",
    "A language model attempts to approximate the underlying statistics of a text corpus $P(tok_n | tok_1, tok_2, ..., tok_{n-1}; \\theta)$ where $\\theta$ is a set of learned parameters/weights. For the purposes of this notebook, tokens will be words. Language models can be used for a variety of applications, one of which being text generation. In this assignement we will be looking at language modeling techniques of increasing sophistication.\n",
    "\n",
    "**Tips:**\n",
    "- Read all the code. We don't ask you to write the training loops, evaluation loops, and generation loops, but it is often instructive to see how the models are trained and evaluated.\n",
    "- If you have a model that is learning (loss is decreasing), but you want to increase accuracy, try using ``nn.Dropout`` layers just before the final linear layer to force the model to handle missing or unfamiliar data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# start time - notebook execution\n",
    "import time\n",
    "start_nb = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VeQWnSY8CgT1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "vcoLFG32u68g",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 11505,
     "status": "ok",
     "timestamp": 1701883137113,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "us-v9ZxoTHV1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "import torch.nn.functional as F\n",
    "import unicodedata\n",
    "\n",
    "# ignore all warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Initialize the Autograder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# import the autograder tests\n",
    "import hw3b_tests as ag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Fp1_pgBuY5zT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We will build a *vocabulary*, which will act as a dictionary of all the words our systems will know about. It will also allow us to map words to tokens, which will be unique indexes in the vocabulary. This will further allow us to transform words into one-hot vectors, where a word is represented as a vector of the same length as the vocabulary wherein all values are zeros except for the *i*th element, where *i* is the token number of the word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1701883137113,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "1ybnRLpOTYwi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, name):\n",
    "        self.name = name                             # The name of the vocabulary\n",
    "        self._word2index = {}                        # Map words to token index\n",
    "        self._word2count = {}                        # Track how many times a word occurs in a corpus\n",
    "        self._index2word = {0: \"SOS\", 1: \"EOS\"}      # Map token indexs back into words\n",
    "        self._n_words = 2 # Count SOS and EOS        # Number of unique words in the corpus\n",
    "\n",
    "    # Get a list of all words\n",
    "    def get_words(self):\n",
    "      return list(self._word2count.keys())\n",
    "\n",
    "    # Get the number of words\n",
    "    def num_words(self):\n",
    "      return self._n_words\n",
    "\n",
    "    # Convert a word into a token index\n",
    "    def word2index(self, word):\n",
    "      return self._word2index[word]\n",
    "\n",
    "    # Convert a token into a word\n",
    "    def index2word(self, word):\n",
    "      return self._index2word[word]\n",
    "\n",
    "    # Get the number of times a word occurs\n",
    "    def word2count(self, word):\n",
    "      return self._word2count[word]\n",
    "\n",
    "    # Add all the words in a sentence to the vocabulary\n",
    "    def add_sentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.add_word(word)\n",
    "\n",
    "    # Add a single word to the vocabulary\n",
    "    def add_word(self, word):\n",
    "        if word not in self._word2index:\n",
    "            self._word2index[word] = self._n_words\n",
    "            self._word2count[word] = 1\n",
    "            self._index2word[self._n_words] = word\n",
    "            self._n_words += 1\n",
    "        else:\n",
    "            self._word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "8L7cXWAYacx3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "These are some helper functions to *normalize* texts, ie, make the text regular and remove some of the more problematic exceptions found in texts. This normalizer will make all words lowercase, trim plurals, and remove non-letter characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701883137113,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "SpAin8FuTsNr",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Convert any unicode to ascii\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "0G1p53AuazDJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Download a corpus. This corpus is the ascii text of the book, *The Silmarillion*, by J.R.R. Tolkein. It has a lot of non-common words and names to illustrate how language models deal with such things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 1359,
     "status": "ok",
     "timestamp": 1701883138469,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "i6IA64U3T3K7",
    "outputId": "39d06901-c85e-40a8-9cd6-ae5998194457",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# if data.txt is not in the current directory, download it\n",
    "if not os.path.isfile('data.txt'):\n",
    "  !wget -O data.txt https://www.dropbox.com/s/pgvn1n7t4sjxt8r/silmarillion?dl=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "EEhv3LTxb1If",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's read in the data and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701883138469,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "AfzYspyHUT7k",
    "outputId": "7409d81f-b1cd-46a4-bbc0-c2736b208ad2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Silmarillon Chapter 1\\n\\n\\nOf the Beginning of Days It is told among the wise that the First War began before Arda was full-shaped, and ere yet there was any thing that grew or walked upon earth; and for long Melkor had the upper hand. But in the midst of the war a spirit of great strength and hardihood came to the aid of the Valar, hearing in the far heaven that there was battle in the Little Kingdom; and Arda was filled with the sound of his laughter. So came Tulkas the Strong, whose anger passes like a mighty wind, scattering cloud and darkness before it; and Melkor fled before his wrath and his laughter, and forsook Arda, and there was peace for a long age. And Tulkas remained and became one of the Valar of the Kingdom of Arda; but Melkor brooded in the outer darkness, and his hate was given to Tulkas for ever after.\\n\\nIn that time the Valar brought order to the seas and the lands and the mountains, and Yavanna planted at last the seeds that she had long devised. And since, when th'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = 'data.txt'\n",
    "with open(filename, encoding='utf-8') as f:\n",
    "  text = f.read()\n",
    "text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "rPga6_LmcDWJ",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Normalize the text and build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 160,
     "status": "ok",
     "timestamp": 1701883138625,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "FV_ib2sZYMqo",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "normalized_text = normalize_string(text)\n",
    "VOCAB = Vocab(\"text\")\n",
    "VOCAB.add_sentence(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "VBei0ecBcHAy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Make training and testing data splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 142,
     "status": "ok",
     "timestamp": 1701883138765,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "HMc-3xncyc_Q",
    "outputId": "22c15d6a-aca5-4967-9144-38dde668862a",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first 100 tokens\n",
      "[ 2  3  4  5  2  6  5  7  8  9 10 11  2 12 13  2 14 15 16 17 18 19 20 21\n",
      " 22 23 24 25 19 26 27 13 28 29 30 31 32 22 33 34 35 36  2 37 38 39 40 41\n",
      "  2 42  5  2 15 43 44  5 45 46 22 47 48 49  2 50  5  2 51 52 41  2 53 54\n",
      " 13 25 19 55 41  2 56 57 22 18 19 58 59  2 60  5 61 62 39 63 48 64  2 65\n",
      " 66 67 68 69]\n"
     ]
    }
   ],
   "source": [
    "# Convert every word into a token and build a numpy array of tokens\n",
    "# encoded_text = np.array([VOCAB.word2index(word) for word in normalized_text.split()])\n",
    "# I know im not supposed to edit this but im getting a weird windows issue around dtypes, so will try this for no\n",
    "# and then will undo for submission\n",
    "encoded_text = np.array([VOCAB.word2index(word) for word in normalized_text.split()], dtype=np.int64)\n",
    "\n",
    "print(\"The first 100 tokens\")\n",
    "print(encoded_text[:100])\n",
    "# get the validation and the training data\n",
    "test_split = 0.1\n",
    "test_idx = int(len(encoded_text) * (1 - test_split))\n",
    "TRAIN = encoded_text[:test_idx]\n",
    "TEST = encoded_text[test_idx:]\n",
    "# Decrease the size of the training set to make the assignment more tractable\n",
    "TRAIN = TRAIN[:len(TRAIN)//10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "NbdKluXiFs3G",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LSTM (20 Points)\n",
    "\n",
    "A more sophisticated version of an RNN is a Long Short-Term Memory network (or an LSTM). It learns to decided what should be kept in the hidden state and what should be removed from the hidden state. This allows it to make better hidden states and thus learn a more accurate probability distribution and be a better generator.\n",
    "\n",
    "We will make two LSTMs. First, we will make a neural network that uses Pytorch's built in `nn.LSTMCell`. The second time, we will write an LSTM memory cell from scratch.\n",
    "\n",
    "**Complete the following network with two or more LSTMCell layers.** The network will take two inputs in its forward function:\n",
    "- `x`: a sequence of words, represented as one-hots. The input should be a tensor of shape `1 x vocab_size` That is, each row is a one-hot (batch size is 1).\n",
    "- `hc` which is a tuple containing (hidden_state, cell_state).\n",
    "\n",
    "The output of the forward function will be:\n",
    "- A sequence of output log probabilities. This output should be a tensor of shape `1 x vocab_size` where each row is a log probability distribution.\n",
    "- A tuple containing (hidden_state, cell_state).\n",
    "\n",
    "The network should contain two our more LSTMCell modules. Send the one-hot into the first LSTMCell along with the original `hc`. Then send the resulting hidden state to the next higher LSTMCell *along with the initial `hc`*. Keep doing this until you get to the top of the stack of LSTMCells. Once you get to the top of the stack, use an affine transformation to expand to vocabular size and generate a log probability with a log softmax.\n",
    "\n",
    "Forward should return the output log probabilities and a (hidden state, cell state) tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701883148144,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "75ICSfVyO9Vw",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# build the model using the pytorch nn module\n",
    "class MyLSTM(nn.ModuleList):\n",
    "  def __init__(self, input_size, hidden_size, cell_type = nn.LSTMCell):\n",
    "    super(MyLSTM, self).__init__()\n",
    "\n",
    "    # init the parameters\n",
    "    self.hidden_dim = hidden_size\n",
    "    self.input_size = input_size\n",
    "\n",
    "    ### Use the cell_type passed into the constructor as the type of LSTM cell module\n",
    "    ### that is made. For the first part of the assignment, this will be the\n",
    "    ### default nn.LSTMCell. For the second part, this will be the custom-written\n",
    "    ### LSTM cell type.\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # well go with just two layers and see if this gets there\n",
    "    # input will be the one-hot input size into the given hidden stat sixe\n",
    "    self.lstm_layer1 = cell_type(input_size, hidden_size)\n",
    "    # layer 2, also cell_type is LSMCell from the signature, took me a second there\n",
    "    self.lstm_layer2 = cell_type(hidden_size, hidden_size)\n",
    "\n",
    "    # linear layer at the end here to get i tb ack to vocab size\n",
    "    self.lin_layer_pre_softmax = nn.Linear(hidden_size, input_size)\n",
    "    # and a standard softmax at the finish here\n",
    "    self.softmax_finish = nn.LogSoftmax(dim=1)\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x, hc):\n",
    "    # Return values\n",
    "    output = None\n",
    "    hidden = None\n",
    "    cell = None\n",
    "\n",
    "    # Pass the hidden and the cell state from one lstm cell to the next one\n",
    "    # we also feed the output of the first layer lstm cell at time step t to the second layer cell\n",
    "    # init both layer cells with the zero hidden and zero cell states\n",
    "\n",
    "    ### BEGIN SOLUTION\n",
    "    # so now we all just connect it like previous assignments\n",
    "    # x and hc comeing from the paramters\n",
    "    hid_lstm1, lstm1 = self.lstm_layer1(x, hc)\n",
    "    # same here for layer 2\n",
    "    hid_lstm2, lstm2 = self.lstm_layer2(hid_lstm1, hc)\n",
    "    # get it back to vocab dimensions\n",
    "    lin_input = self.lin_layer_pre_softmax(hid_lstm2)\n",
    "    output_sf_logs = self.softmax_finish(lin_input)\n",
    "    # and now we gotta update the hid and cell from above\n",
    "    hidden = hid_lstm2\n",
    "    cell = lstm2\n",
    "    # forgot about output var name:\n",
    "    output = output_sf_logs\n",
    "    ### END SOLUTION\n",
    "\n",
    "    return output, (hidden.detach(), cell.detach())\n",
    "\n",
    "  def init_hidden(self):\n",
    "    # initialize the hidden state and the cell state to zeros\n",
    "    return (torch.zeros(1, self.hidden_dim), # 1 is the batch size\n",
    "            torch.zeros(1, self.hidden_dim)) # 1 is the batch size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "5sNibJA1K93Q",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's build our LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701883148144,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "_es9Jos-Qz8A",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# It's ok to change this cell, however, you should not need to change it much (if at all) - note: certain changes may break the autograder, e.g., \n",
    "# increasing the size of the hidden layer could cause out of memory errors in the autograder and large numbers of epochs could cause autograder to time\n",
    "# out (pay attention to the runtime of your notebook and the warnings that are printed out at the end of the notebook)\n",
    "LSTM_HIDDEN_SIZE = 64\n",
    "LSTM_NUM_EPOCHS = 5\n",
    "LSTM_LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 9995,
     "status": "ok",
     "timestamp": 1701883158137,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "n3Q2Z4BFQv-V",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "lstm = MyLSTM(VOCAB.num_words(), LSTM_HIDDEN_SIZE)\n",
    "optimizer_lstm = optim.SGD(lstm.parameters(), lr=LSTM_LEARNING_RATE)\n",
    "criterion_lstm = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701883158137,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "PJh3pC-EXCAo",
    "outputId": "d2b4b492-e966-41f7-a877-a3b19fdd848d",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers found, 5, is correct.\n",
      "Test A: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 3 to receive credit (5 pts)\n",
    "ag.LSTM_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 589,
     "status": "ok",
     "timestamp": 1701883158892,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "MQXazww3Tct5",
    "outputId": "bdaabe65-e1e2-4ce4-fd86-664bd3010d6d",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 100]) torch.Size([1, 50]) torch.Size([1, 50])\n",
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "NllLossBackward0\n",
      "LogSoftmaxBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "AddBackward0\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "MulBackward0\n",
      "SigmoidBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TanhBackward0\n",
      "UnsafeSplitBackward0\n",
      "AddBackward0\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "AddmmBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "TBackward0\n",
      "AccumulateGrad\n",
      "Number of layers found, 5, is correct.\n",
      "Test B: 5/5\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value of 5 to receive credit (5 pts)\n",
    "ag.unit_test_LSTM_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "M-jsgts9DoMC",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LSTM---Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "fy9kxSrwAWkL",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Here is the training loop. Notice it uses `get_rnn_x_y()` from HW2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701883158892,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "jwgcBtN7Qdda",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_lstm(net, optimizer, criterion, num_epochs, data):\n",
    "  epoch_losses = []\n",
    "  scheduler = ExponentialLR(optimizer, gamma=0.9)\n",
    "  net.train()\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    hc = net.init_hidden()\n",
    "    for i in range(len(data)-1):\n",
    "      x, y = ag.get_rnn_x_y(data, i, VOCAB.num_words())\n",
    "      x = x.float()\n",
    "      output, hc = net(x, hc)\n",
    "      loss = criterion(output, y)\n",
    "      losses.append(loss)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      if i%1000 == 0:\n",
    "        print('iter', i, 'loss', torch.stack(losses).mean().item())\n",
    "    scheduler.step()\n",
    "    print('epoch', epoch, 'loss', torch.stack(losses).mean().item())\n",
    "    epoch_losses.append(torch.stack(losses).mean().item())\n",
    "  return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 369876,
     "status": "ok",
     "timestamp": 1701883528766,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "SSo3W6M0Hopi",
    "outputId": "1db3621e-110c-4e24-a7d7-325364fa35c9",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0 loss 8.587872505187988\n",
      "iter 1000 loss 8.52823257446289\n",
      "iter 2000 loss 8.282547950744629\n",
      "iter 3000 loss 7.823319911956787\n",
      "iter 4000 loss 7.584197521209717\n",
      "iter 5000 loss 7.405727386474609\n",
      "iter 6000 loss 7.245823383331299\n",
      "iter 7000 loss 7.11781120300293\n",
      "iter 8000 loss 7.007818698883057\n",
      "iter 9000 loss 6.898065567016602\n",
      "iter 10000 loss 6.8214802742004395\n",
      "iter 11000 loss 6.7499566078186035\n",
      "epoch 0 loss 6.712731838226318\n",
      "iter 0 loss 8.469270706176758\n",
      "iter 1000 loss 6.00563383102417\n",
      "iter 2000 loss 6.016983509063721\n",
      "iter 3000 loss 5.96214485168457\n",
      "iter 4000 loss 5.983663082122803\n",
      "iter 5000 loss 6.001788139343262\n",
      "iter 6000 loss 6.005000591278076\n",
      "iter 7000 loss 6.0034894943237305\n",
      "iter 8000 loss 5.995907306671143\n",
      "iter 9000 loss 5.972186088562012\n",
      "iter 10000 loss 5.962159156799316\n",
      "iter 11000 loss 5.946882247924805\n",
      "epoch 1 loss 5.936708450317383\n",
      "iter 0 loss 8.360736846923828\n",
      "iter 1000 loss 5.868769645690918\n",
      "iter 2000 loss 5.884314060211182\n",
      "iter 3000 loss 5.839179039001465\n",
      "iter 4000 loss 5.870306491851807\n",
      "iter 5000 loss 5.896880149841309\n",
      "iter 6000 loss 5.904809951782227\n",
      "iter 7000 loss 5.906312465667725\n",
      "iter 8000 loss 5.902847766876221\n",
      "iter 9000 loss 5.881455898284912\n",
      "iter 10000 loss 5.873552322387695\n",
      "iter 11000 loss 5.860159873962402\n",
      "epoch 2 loss 5.851012706756592\n",
      "iter 0 loss 8.300745010375977\n",
      "iter 1000 loss 5.810396671295166\n",
      "iter 2000 loss 5.828245162963867\n",
      "iter 3000 loss 5.786144256591797\n",
      "iter 4000 loss 5.820938587188721\n",
      "iter 5000 loss 5.850881576538086\n",
      "iter 6000 loss 5.860813617706299\n",
      "iter 7000 loss 5.862888813018799\n",
      "iter 8000 loss 5.860744953155518\n",
      "iter 9000 loss 5.8397297859191895\n",
      "iter 10000 loss 5.832444190979004\n",
      "iter 11000 loss 5.819591522216797\n",
      "epoch 3 loss 5.810834884643555\n",
      "iter 0 loss 8.268293380737305\n",
      "iter 1000 loss 5.775501728057861\n",
      "iter 2000 loss 5.795139789581299\n",
      "iter 3000 loss 5.754415035247803\n",
      "iter 4000 loss 5.790940761566162\n",
      "iter 5000 loss 5.822723865509033\n",
      "iter 6000 loss 5.834127902984619\n",
      "iter 7000 loss 5.836246490478516\n",
      "iter 8000 loss 5.834731578826904\n",
      "iter 9000 loss 5.8138298988342285\n",
      "iter 10000 loss 5.80683708190918\n",
      "iter 11000 loss 5.794272422790527\n",
      "epoch 4 loss 5.785767078399658\n"
     ]
    }
   ],
   "source": [
    "epoch_losses = train_lstm(lstm, optimizer_lstm, criterion_lstm, num_epochs=LSTM_NUM_EPOCHS, data=TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1737d438890>]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(len(epoch_losses)))\n",
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You should see a curve that slopes down steeply at first and then levels out to some asymptotic minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "cu2yQ-jsDtGi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LSTM---Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Ebr9BQa7Jolv",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Evaluation works the same as with the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 464,
     "status": "ok",
     "timestamp": 1701883529870,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "XfSf3mGXKCc2",
    "outputId": "9f7c8e14-0833-4ced-d235-4b1af43fbd06",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499.60394287109375\n",
      "Perplexity is less than 1000\n",
      "Test C: 10/10\n"
     ]
    }
   ],
   "source": [
    "# student check - the following test must return a value less than 1000 to receive credit (10 pts)\n",
    "ag.eval_lstm_1(max_perplexity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Nz7K467kDwHi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LSTM---Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "CZ6FHHYiJuZk",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Generation works the same as the RNN. In fact you will notice that we can use the `prep_hidden_state` and `generate_rnn` functions without modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701883529870,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "DpdE9HWvVMhl",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to change this cell\n",
    "LSTM_TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701883529870,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "NO2U8mCQMUxM",
    "outputId": "f6a631d5-26c5-4488-8bd5-a3b3c5e3611f",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input prompt: the First War began\n",
      "input tokens: [2, 14, 15, 16] \n",
      "\n",
      "Prepping hidden state:\n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 14 first\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 15 war\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 16 began\n",
      "predicted next token: 2 the \n",
      "\n",
      "Generating continuation:\n",
      "\n",
      "current token: 16 began\n",
      "predicted next token: 5 of \n",
      "\n",
      "current token: 5 of\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 19 was \n",
      "\n",
      "current token: 19 was\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "current token: 2 the\n",
      "predicted next token: 2 the \n",
      "\n",
      "Final continuation:\n",
      "[5, 2, 2, 2, 2, 19, 2, 2, 2, 2]\n",
      "['of', 'the', 'the', 'the', 'the', 'was', 'the', 'the', 'the', 'the']\n",
      "Final:\n",
      "the First War began of the the the the was the the the the\n"
     ]
    }
   ],
   "source": [
    "# COPY YOUR HW3-A SOLUTION HERE\n",
    "# copied from hw3a\n",
    "def token2onehot(token, vocab_size = VOCAB.num_words()):\n",
    "  one_hot = None\n",
    "  ### BEGIN SOLUTION\n",
    "  # so first lets get zeroes\n",
    "  one_hot_vector = [0] * vocab_size\n",
    "  # set our word guy\n",
    "  one_hot_vector[token] = 1\n",
    "\n",
    "  # one_hot = one_hot_vector\n",
    "  # need to make it a tensor, think thats my problem?\n",
    "  # one_hot = torch.tensor(one_hot_vector)\n",
    "  one_hot = torch.tensor(one_hot_vector).unsqueeze(0)\n",
    "  ### END SOLUTION\n",
    "  return one_hot\n",
    "    \n",
    "# COPIED FROM HW3-A\n",
    "def prep_hidden_state(tokenized_input, rnn, verbose=False):\n",
    "  # Get an initial hidden state\n",
    "  hidden_state = rnn.init_hidden()\n",
    "  # Run the input prompt through the RNN to build up the hidden state.\n",
    "  # Discard the outputs (we are not trying to make predictions) until we get to the end\n",
    "  for token in tokenized_input:\n",
    "    if verbose:\n",
    "      print(\"current token:\", token, VOCAB.index2word(token))\n",
    "    # Get the one-hot for the current token\n",
    "    x = token2onehot(token)\n",
    "    x = x.float()\n",
    "    # Run the current one-hot and hidden state through the RNN\n",
    "    output, hidden_state = rnn(x, hidden_state)\n",
    "    # Get the highest predicted token\n",
    "    next_token = output.argmax().item()\n",
    "    if verbose:\n",
    "      print(\"predicted next token:\", next_token, VOCAB.index2word(next_token), '\\n')\n",
    "  return hidden_state\n",
    "\n",
    "def log_to_percentage_probs(log_probs):\n",
    "  perc_probs = torch.exp(log_probs)\n",
    "  return perc_probs\n",
    "\n",
    "# COPY YOUR HW3-A SOLUTION HERE\n",
    "# taken from 3a\n",
    "def my_temperature_sample(log_probs, temperature=1.0):\n",
    "  token = None\n",
    "  ### BEGIN SOLUTION\n",
    "  # apply temp scaling FIRST, what we might have screwed up last time\n",
    "  probs_temp = log_probs / temperature\n",
    "  # THEN get the percentage probs, use func from above again\n",
    "  perc_probs = log_to_percentage_probs(probs_temp)\n",
    "\n",
    "  # divide by temp\n",
    "  numerator = perc_probs\n",
    "  # get the sum of probs\n",
    "  denominator = numerator.sum(dim=1, keepdim=True)\n",
    "  val = numerator / denominator\n",
    "  # use our multinomial again\n",
    "  draws = 1\n",
    "  token = torch.multinomial(val, draws).item()\n",
    "  ### END SOLUTION\n",
    "  return token\n",
    "\n",
    "# COPIED FROM HW3-A\n",
    "def generate_rnn(rnn, num_new_tokens, token, hidden_state, fn=lambda d:d.argmax().item(), verbose=False):\n",
    "  # Keep generating more by feeding the predicted output back into the RNN as input\n",
    "  # Start with the last token of the input prompt and the newly prepped hidden state\n",
    "  if verbose:\n",
    "    print(\"Generating continuation:\\n\")\n",
    "  continuation = []\n",
    "  for n in range(num_new_tokens):\n",
    "    if verbose:\n",
    "      print(\"current token:\", token, VOCAB.index2word(token))\n",
    "    # Get the one-hot for the current token\n",
    "    x = token2onehot(token)\n",
    "    x = x.float()\n",
    "    # Run the current one-hot through the RNN\n",
    "    output, hidden_state = rnn(x, hidden_state)\n",
    "    # Predict the next token\n",
    "    next_token = fn(output)\n",
    "    if verbose:\n",
    "      print(\"predicted next token:\", next_token, VOCAB.index2word(next_token), '\\n')\n",
    "    # Remember the new token\n",
    "    continuation.append(next_token)\n",
    "    # update the current\n",
    "    token = next_token\n",
    "  return continuation\n",
    "\n",
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "\n",
    "# Get the hidden state that represents the input prompt\n",
    "print(\"Prepping hidden state:\\n\")\n",
    "hidden_state = prep_hidden_state(tokenized_input, lstm, verbose=True)\n",
    "\n",
    "# Generate the continuation. Use the argmax function to sample from the RNN's outputs\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(lstm, num_new_tokens, token, hidden_state, fn=lambda d:my_temperature_sample(d, LSTM_TEMPERATURE), verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(continuation_text)\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "k11f76FVDzpb",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# LSTM From Scratch (40 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "TfU-uNxxJ3Qi",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now we do LSTM the hard way---creating the LSTM cells by hand.\n",
    "\n",
    "**Complete the following functions inside the `MyLSTMCell` class.**\n",
    "\n",
    "We have broken the forward function into multiple parts:\n",
    "- Forget gate: determine what of the previous cell state should be discarded (by multiplying 0 or 1 produced by a sigmoid against the cell state).  `forget_gate()` implements $f=\\sigma(W_{i,f}x+b_{i,f} + W_{h,f}h+b_{h,f})$.\n",
    "- Input gate: determine what of the input should be introduced to the cell memory. `input_gate()` implements $i=\\sigma(W_{i,i}x+b_{i,i}+W_{h,i}h+b_{h,i})$\n",
    "- Cell memory: update the previous cell memory state to make a new cell memory state. `cell_memory()` implements $c'=f*c + i*tanh(W_{i,g}x+b_{i,g} + W_{h,g}h + b_{h,g})$.\n",
    "- Output gate: determine what from the current cell memory state. `output_gate()` implements $o=\\sigma(W_{i,o}x+b_{i,o}+W_{h,o}h+b_{h,o})$.\n",
    "- A final function `hidden_out()` will produce the new hidden state by implementing $h'=o*tanh(c')$.\n",
    "\n",
    "You will also need to initialize any linear layer modules, activation functions, etc. in the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701883529871,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "KUaVNN3nuUEh",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class MyLSTMCell(torch.nn.Module):\n",
    "\n",
    "  def __init__(self, input_size=10, hidden_size=64):\n",
    "    super(MyLSTMCell, self).__init__()\n",
    "    self.input_size = input_size\n",
    "    self.hidden_size = hidden_size\n",
    "    ### BEGIN SOLUTION\n",
    "    # so first we need forget gate\n",
    "    self.lin_forget_x_layer = nn.Linear(input_size, hidden_size)\n",
    "    self.lin_forget_hid_layer = nn.Linear(hidden_size, hidden_size)\n",
    "    # ok so that should be simple enough\n",
    "    # now input gate\n",
    "    self.lin_input_x_layer = nn.Linear(input_size, hidden_size)\n",
    "    self.lin_input_hid_layer = nn.Linear(hidden_size, hidden_size)\n",
    "    # now the cell memory\n",
    "    self.lin_cellmem_x_layer = nn.Linear(input_size, hidden_size)\n",
    "    self.lin_cellmem_hid_layer = nn.Linear(hidden_size, hidden_size)\n",
    "    # and finally output\n",
    "    self.lin_output_x_layer = nn.Linear(input_size, hidden_size)\n",
    "    self.lin_output_hid_layer = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    # kind of overkill but need these later\n",
    "    self.sig = nn.Sigmoid()\n",
    "    self.tan = nn.Tanh()\n",
    "    ### END SOLUTION\n",
    "\n",
    "  ### The Forget Gate takes in the input (x) and hidden state (h)\n",
    "  ### The input and hidden state pass through their own linear compression layers,\n",
    "  ### then are concatenated and passed through a sigmoid\n",
    "  def forget_gate(self, x, h):\n",
    "    f = None # The gate vector to return\n",
    "    ### BEGIN SOLUTION\n",
    "    # I think we just gotta go sigmoid here\n",
    "    # linear transform first\n",
    "    lin_trans = self.lin_forget_x_layer(x) + self.lin_forget_hid_layer(x)\n",
    "    f = self.sig(lin_trans)\n",
    "    ### END SOLUTION\n",
    "    return f\n",
    "\n",
    "  ### The Input Gate takes the input (x) and hidden state (h)\n",
    "  ### The input and hidden state pass through their own linear compression layers,\n",
    "  ### then are concatenated and passed through a sigmoid\n",
    "  def input_gate(self, x, h):\n",
    "    i = None # The gate vector to return\n",
    "    ### BEGIN SOLUTION\n",
    "    # same as forget here but just with input\n",
    "    lin_trans = self.lin_input_x_layer(x) + self.lin_input_h_layer(x)\n",
    "    f = self.sig(lin_trans)\n",
    "    ### END SOLUTION\n",
    "    return i\n",
    "\n",
    "  ### The Cell memory gate takes the results from the input gate (i), the results from the forget gate (f)\n",
    "  ### the original input (x), the hidden state(h) and the previous cell state (c_prev).\n",
    "  ### 1. The Cell memory gate compresses the input and hidden and concatenates them and passes it through a Tanh.\n",
    "  ### 2. The resultant intermediate tensor is multiplied by the results from the input gate to determine\n",
    "  ###    what new information is allowed to carry on\n",
    "  ### 3. The results from the forget state are multiplied against the previous cell state (c_prev) to determine\n",
    "  ###    what should be removed from the cell state.\n",
    "  ### 4. The new cell state (c_next) is the new information that survived the input gate and the previous\n",
    "  ###    cell state that survived the forget gate.\n",
    "  ### The new cell state c_next is returned\n",
    "  def cell_memory(self, i, f, x, h, c_prev):\n",
    "    c_next = None\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return c_next\n",
    "\n",
    "  ### The Out gate takes the original input (x) and the hidden state (h)\n",
    "  ### The gate passes the input and hidden through their own compression layers and\n",
    "  ### then concatenates to send through a sigmoid\n",
    "  def out_gate(self, x, h):\n",
    "    o = None # The gate vector to return\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return o\n",
    "\n",
    "  ### This function assembles the new hidden state, give the results of the output gate (o)\n",
    "  ### and the new cells sate (c_next).\n",
    "  ### This function runs c_next through a tanh to get a 1 or -1 which will flip some of the\n",
    "  ### elements of the output.\n",
    "  def hidden_out(self, o, c_next):\n",
    "    h_next = None\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return h_next\n",
    "\n",
    "  def forward(self, x, hc):\n",
    "    (h, c_prev) = hc\n",
    "    # Equation 1. input gate\n",
    "    i = self.input_gate(x, h)\n",
    "\n",
    "    # Equation 2. forget gate\n",
    "    f = self.forget_gate(x, h)\n",
    "\n",
    "    # Equation 3. updating the cell memory\n",
    "    c_next = self.cell_memory(i, f, x, h, c_prev)\n",
    "\n",
    "    # Equation 4. calculate the main output gate\n",
    "    o = self.out_gate(x, h)\n",
    "\n",
    "    # Equation 5. produce next hidden output\n",
    "    h_next = self.hidden_out(o, c_next)\n",
    "\n",
    "    return h_next, c_next\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return (torch.zeros(1, self.hidden_size),\n",
    "            torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1701883529994,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "F1hU85_pKq1k",
    "outputId": "8d30ea15-71c9-416f-fcea-1a2233b952ee",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value of 6 to receive credit (5 pts)\n",
    "ag.test_myLSTMCell_structure(MyLSTMCell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1701883529994,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "9qSAP1b3Kt-T",
    "outputId": "78d0f428-2ba7-4d3f-d88e-91af933590a1",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value of 8 to receive credit (5 pts)\n",
    "ag.MyLSTMCell_linear_layer_size_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "AX6gFeSw_nGN",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Let's build a cell. A cell doesn't do much by itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1701883530118,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "KX5cDfwhjNNp",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "cell = MyLSTMCell(input_size=VOCAB.num_words(), hidden_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 170,
     "status": "ok",
     "timestamp": 1701883530285,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "aIs1bUkCK9nB",
    "outputId": "e6629d89-acb1-4813-9b97-46942ab0c75e",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value of 22 to receive credit (10 pts)\n",
    "ag.test_gate_structure(cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "xHD45HsO_qx_",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now let's load your `MyLSTMCell` class into `MyLSTM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1701883530285,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "OLFMIFotAI0Z",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to change this cell, however, you should not need to change it much (if at all) - note: certain changes may break the autograder, e.g., \n",
    "# increasing the size of the hidden layer could cause out of memory errors in the autograder and large numbers of epochs could cause autograder to time\n",
    "# out (pay attention to the runtime of your notebook and the warnings that are printed out at the end of the notebook)\n",
    "MY_CELL_HIDDEN_SIZE = 64\n",
    "MY_CELL_NUM_EPOCHS = 5\n",
    "MY_CELL_LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": false,
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1701883530285,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "R3CE-rewmg4S",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "my_cell_lstm = MyLSTM(input_size=VOCAB.num_words(), hidden_size=MY_CELL_HIDDEN_SIZE, cell_type=MyLSTMCell)\n",
    "optimizer_my_cell = optim.SGD(my_cell_lstm.parameters(), lr=MY_CELL_LEARNING_RATE)\n",
    "criterion_my_cell = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "Au4bDyOjD7Dg",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LSTM From Scratch---Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "PReAw7jPEHS3",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Lets see if your combination of `MyLSTM` using `MyLSTMCell` learns. We don't need to update the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 627235,
     "status": "ok",
     "timestamp": 1701884157518,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "twn105pnrmFx",
    "outputId": "da587035-90c3-48e5-ed23-ff214729e196",
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "epoch_losses = train_lstm(my_cell_lstm, optimizer_my_cell, criterion_my_cell, num_epochs=MY_CELL_NUM_EPOCHS, data=TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(len(epoch_losses)))\n",
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You should see a curve that slopes down steeply at first and then levels out to some asymptotic minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "5c-ofH8SD-4E",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LSTM From Scratch---Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "jEh_eZJbEP3D",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "We don't need to update the evaluation loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 1285,
     "status": "ok",
     "timestamp": 1701884160284,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "LVNyCqQfMVdC",
    "outputId": "872d7872-d2cb-4f2c-8fe2-663602a24801",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value less than 1000 to receive credit (20 pts)\n",
    "ag.eval_lstm_2(max_perplexity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "fcrqcIgJEibA",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## LSTM From Scratch---Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": false,
    "id": "wUo5LWdVFctT",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Generation works the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1701884160284,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "eGShC3Y_E6vy",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to change this cell\n",
    "MY_CELL_TEMPERATURE = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "editable": false,
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1701884160285,
     "user": {
      "displayName": "Frank Ginac",
      "userId": "00024240915320510876"
     },
     "user_tz": 360
    },
    "id": "dD5OwZMXEldP",
    "outputId": "ca390e57-e8b0-4165-c0a7-68c7250b40c2",
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# Example input prompt:\n",
    "input_prompt = \"the First War began\"\n",
    "# How long should the continuation be?\n",
    "num_new_tokens = 10\n",
    "\n",
    "# Normalize the input\n",
    "normalized_input = normalize_string(input_prompt)\n",
    "# Tokenize the input\n",
    "tokenized_input = [VOCAB.word2index(w) for w in normalized_input.split()]\n",
    "print(\"input prompt:\", input_prompt)\n",
    "print(\"input tokens:\", tokenized_input, '\\n')\n",
    "\n",
    "# Get the hidden state that represents the input prompt\n",
    "print(\"Prepping hidden state:\\n\")\n",
    "hidden_state = prep_hidden_state(tokenized_input, my_cell_lstm, verbose=True)\n",
    "\n",
    "# Generate the continuation. Use the argmax function to sample from the RNN's outputs\n",
    "token = tokenized_input[-1]\n",
    "continuation = generate_rnn(my_cell_lstm, num_new_tokens, token, hidden_state, fn=lambda d:my_temperature_sample(d, MY_CELL_TEMPERATURE), verbose=True)\n",
    "\n",
    "# All done\n",
    "print(\"Final continuation:\")\n",
    "print(continuation)\n",
    "continuation_text = [VOCAB.index2word(t) for t in continuation]\n",
    "print(continuation_text)\n",
    "print(\"Final:\")\n",
    "print(input_prompt + ' ' + ' '.join(continuation_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Attention (40 Points)\n",
    "\n",
    "Attention allows the network to look back at previous data when trying to predict the next token.\n",
    "\n",
    "We will split the LSTM into an Encoder and a Decoder. The Encoder's job will be to update the hidden state based on the latest token. The Decoder's job is to predict the next token (log softmax over the vocabulary) based on the current hidden state as well as *n* previous hidden states. You will see that the training loop now collects up a stack of hidden states to pass to the Decoder. The Decoder will figure out how much the network should attend to each of the *n* prior hidden states before making its final prediction.\n",
    "\n",
    "The Encoder will be a simple `nn.LSTMCell`. While the encoder could be more complicated, this allows us to focus on the Decoder. The Decoder is more complicated, involving both an LSTMCell and an attention mechanism.\n",
    "\n",
    "**Complete the class defnition below**\n",
    "\n",
    "`MyAttentionDecoder` will implement another `nn.LSTMCell` plus an attention mechanism.\n",
    "\n",
    "Inputs:\n",
    "- `x`: a one-hot of the current token as a `1 x vocab_size` tensor\n",
    "- `hc`: a tuple containing a tuple with encoder's hidden state and memory cell state. The hidden state and cell state are both `1 x hidden_size` tensors.\n",
    "- `encoder_outputs`: a history of *n* encoded hidden states, as a `n x hidden_size` tensor (this data is not batched).\n",
    "\n",
    "Outputs:\n",
    "- `h_hat`: a log softmax probability distribution over the vocabular, as a `1 x vocab_size` tensor\n",
    "- `hc_out`: a tuple containing the LSTMCell's hidden state and memory cell state. The hidden state and cell state are both `1 x hidden_size` tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "class MyAttentionDecoder(nn.Module):\n",
    "  def __init__(self, hidden_size, input_size, max_length):\n",
    "    super(MyAttentionDecoder, self).__init__()\n",
    "    self.hidden_size = hidden_size\n",
    "    self.input_size = input_size\n",
    "    self.max_length = max_length\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "\n",
    "  def forward(self, x, hc, encoder_outputs):\n",
    "    log_probs = None\n",
    "    hc_out = None\n",
    "    ### BEGIN SOLUTION\n",
    "    ### END SOLUTION\n",
    "    return log_probs, hc_out\n",
    "\n",
    "  def init_hidden(self):\n",
    "    return (torch.zeros(1, self.hidden_size),\n",
    "            torch.zeros(1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# It's ok to change this cell, however, you should not need to change it much (if at all) - note: certain changes may break the autograder, e.g., \n",
    "# increasing the size of the hidden layer could cause out of memory errors in the autograder and large numbers of epochs could cause autograder to time\n",
    "# out (pay attention to the runtime of your notebook and the warnings that are printed out at the end of the notebook)\n",
    "ATTN_MAX_LENGTH = 5  # The number of past hidden states that can be attended to\n",
    "ATTN_HIDDEN_SIZE = 64\n",
    "ATTN_NUM_EPOCHS = 5\n",
    "ATTN_LEARNING_RATE = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "attn_encoder = nn.LSTMCell(VOCAB.num_words(), ATTN_HIDDEN_SIZE)\n",
    "attn_decoder = MyAttentionDecoder(ATTN_HIDDEN_SIZE, VOCAB.num_words(), ATTN_MAX_LENGTH)\n",
    "attn_criterion = nn.NLLLoss()\n",
    "attn_encoder_optimizer = optim.SGD(attn_decoder.parameters(), lr=ATTN_LEARNING_RATE, momentum=0.9)\n",
    "attn_decoder_optimizer = optim.SGD(attn_decoder.parameters(), lr=ATTN_LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value of 4 to receive credit (10 pts)\n",
    "ag.test_attention_structure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value of 8 to receive credit (10 pts)\n",
    "ag.attention_linear_layer_size_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Attention---Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "The training loop is a bit more involved because it must collect up a number of past hidden states. It still uses your `get_rnn_x_y()` function though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "def train_attn(data, num_epochs, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=ATTN_MAX_LENGTH):\n",
    "  epoch_losses = []\n",
    "  encoder.train()\n",
    "  decoder.train()\n",
    "  for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    # Get an empty hc\n",
    "    encoder_hc = decoder.init_hidden()\n",
    "    # Create an empty history of hiddens\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size)\n",
    "\n",
    "    for iter in range(len(data)-1):\n",
    "      x, y = ag.get_rnn_x_y(data, iter, VOCAB.num_words())\n",
    "      x = x.float()\n",
    "      # Call encoder\n",
    "      encoder_hidden, encoder_cell = encoder(x, encoder_hc)\n",
    "      encoder_hc = (encoder_hidden, encoder_cell)\n",
    "      # unbatch the hidden so it can be added to encoder_outputs\n",
    "      encoder_output = encoder_hidden[0]\n",
    "      # Shift all the previous outputs\n",
    "      # Grab elements 1...max (dropping row 0) and flatten\n",
    "      encoder_outputs = encoder_outputs[1:,:].view(-1)\n",
    "      # Add the new output\n",
    "      encoder_outputs = torch.cat((encoder_outputs, encoder_output.detach()))\n",
    "      # re-fold\n",
    "      encoder_outputs = encoder_outputs.view(max_length, -1)\n",
    "      # decoder's input hc is the encoder's output hc\n",
    "      decoder_hc = encoder_hc\n",
    "      # Call the decoder\n",
    "      decoder_output, decoder_hc = decoder(x, decoder_hc, encoder_outputs)\n",
    "\n",
    "      loss = criterion(decoder_output, y)\n",
    "      losses.append(loss)\n",
    "\n",
    "      encoder_optimizer.zero_grad()\n",
    "      decoder_optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      encoder_optimizer.step()\n",
    "      decoder_optimizer.step()\n",
    "\n",
    "      # Prep the decoder hc for the next iteration\n",
    "      encoder_hc = (decoder_hc[0].detach(), decoder_hc[1].detach())\n",
    "\n",
    "      if iter%1000 == 0:\n",
    "        print(\"iter\", iter, \"loss\", torch.stack(losses).mean().item())\n",
    "    print(\"epoch\", epoch, \"loss\", torch.stack(losses).mean().item())\n",
    "    epoch_losses.append(torch.stack(losses).mean().item())\n",
    "  return epoch_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "epoch_losses = train_attn(TRAIN, ATTN_NUM_EPOCHS, attn_encoder, attn_decoder, attn_encoder_optimizer, attn_decoder_optimizer, attn_criterion, ATTN_MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.xticks(range(len(epoch_losses)))\n",
    "plt.plot(epoch_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "You should see a curve that slopes down steeply at first and then levels out to some asymptotic minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Attention---Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check - the following test must return a value less than 1000 to receive credit (20 pts)\n",
    "ag.eval_attn(max_perplexity=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Grading\n",
    "\n",
    "Please submit this .ipynb file to Gradescope for grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Final Grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# student check\n",
    "ag.final_grade()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Notebook Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "editable": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "# end time - notebook execution\n",
    "end_nb = time.time()\n",
    "# print notebook execution time in minutes\n",
    "print(\"Notebook execution time in minutes =\", (end_nb - start_nb)/60)\n",
    "# warn student if notebook execution time is greater than 30 minutes\n",
    "if (end_nb - start_nb)/60 > 30:\n",
    "  print(\"WARNING: Notebook execution time is greater than 30 minutes. Your submission may not complete auto-grading on Gradescope. Please optimize your code to reduce the notebook execution time.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "UT-whgM5YV8X",
    "VeQWnSY8CgT1",
    "UY0I2wl9D61t",
    "NbdKluXiFs3G",
    "k11f76FVDzpb",
    "ioeaPBMUF8zZ"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
