{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pdb\n",
    "# sys.path.append(\"C:\\\\users\\\\mccar\\\\miniconda3\\\\lib\\\\site-packages\")\n",
    "# import gymnasium as gym\n",
    "import gym\n",
    "from bettermdptools.algorithms.planner import Planner\n",
    "from bettermdptools.utils.plots import Plots\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env = gym.make('LunarLander-v2')\n",
    "# env = gym.make(\"SpaceInvaders-v4\")\n",
    "# env = gym.make(\"CartPole-v1\")\n",
    "# env = gym.make(\"CartPole-v0\")\n",
    "# env = gym.make(\"MountainCar-v0\")\n",
    "env = gym.make(\"Blackjack-v1\")\n",
    "# all = gym.envs.registry\n",
    "# for env, val in all.items():\n",
    "#     print(env)\n",
    "# print(len(all))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'BlackjackEnv' object has no attribute 'P'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m action_values \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn):\n\u001b[1;32m---> 31\u001b[0m     action_values[action] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m([prob \u001b[38;5;241m*\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m V[next_state]) \u001b[38;5;28;01mfor\u001b[39;00m prob, next_state, reward, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mP\u001b[49m[state][action]])\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# pick a random move of the max states\u001b[39;00m\n\u001b[0;32m     33\u001b[0m max_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(action_values\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\core.py:241\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'BlackjackEnv' object has no attribute 'P'"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "verbose = True\n",
    "gamma = 0.999\n",
    "# gamma = 1\n",
    "epsilon = 1e-2\n",
    "\n",
    "## Initialize the value function\n",
    "# print(env.observation_space)\n",
    "# V = np.zeros(env.observation_space.n)\n",
    "V = np.zeros((32, 11, 2)) \n",
    "rewards_by_iteration = []\n",
    "steps_by_iteration = []\n",
    "num_iterations = 0\n",
    "rewards_by_episode = []\n",
    "steps_by_episode = []\n",
    "found_it = False\n",
    "found_it_iter = 0\n",
    "\n",
    "while True:\n",
    "    state = env.reset()[0]\n",
    "    delta = 0\n",
    "    num_steps = 0\n",
    "    total_reward = 0\n",
    "    # For each episode, perform value iteration until convergence\n",
    "    while True:\n",
    "        # Increment the number of steps\n",
    "        num_steps += 1\n",
    "        # Choose the best action based on the current value function\n",
    "        action_values = {}\n",
    "        for action in range(env.action_space.n):\n",
    "            action_values[action] = sum([prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]])\n",
    "        # pick a random move of the max states\n",
    "        max_value = max(action_values.values())\n",
    "        max_states = [state for state, value in action_values.items() if value == max_value]\n",
    "        action = np.random.choice(max_states)\n",
    "        # Take the action and observe the next state and reward\n",
    "        next_state, reward, done, trunc, _ = env.step(action)\n",
    "        if reward > 0:\n",
    "            if not found_it:\n",
    "                found_it = True\n",
    "                found_it_iter = num_iterations\n",
    "            print(\"Found IT!!!!\")\n",
    "        # print(num_steps, next_state, reward, done, trunc)\n",
    "        # Update the total reward\n",
    "        total_reward += reward\n",
    "        # Update the value function\n",
    "        V[state] = max(sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n))\n",
    "        # Check for convergence\n",
    "        delta = max(delta, np.abs(total_reward - V[state]))\n",
    "        if done or trunc:\n",
    "            break\n",
    "        # print(state)\n",
    "        state = next_state\n",
    "    # Increment the number of iterations\n",
    "    num_iterations += 1\n",
    "    rewards_by_episode.append(total_reward)\n",
    "    steps_by_episode.append(num_steps)\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Episode {num_iterations} - Reward: {total_reward}, Steps: {num_steps}\")\n",
    "    # Check for convergence\n",
    "    if (delta < epsilon and num_iterations > 1000) or num_iterations > 3000:\n",
    "        break\n",
    "\n",
    "# moving avgs\n",
    "reward_moving_avg_10 = np.convolve(rewards_by_episode, np.ones(10)/10, mode='valid')\n",
    "reward_moving_avg_50 = np.convolve(rewards_by_episode, np.ones(50)/50, mode='valid')\n",
    "reward_moving_avg_100 = np.convolve(rewards_by_episode, np.ones(100)/100, mode='valid')\n",
    "reward_moving_avg_500 = np.convolve(rewards_by_episode, np.ones(500)/500, mode='valid')\n",
    "reward_moving_avg_1000 = np.convolve(rewards_by_episode, np.ones(1000)/1000, mode='valid')\n",
    "steps_moving_avg_10 = np.convolve(steps_by_episode, np.ones(10)/10, mode='valid')\n",
    "steps_moving_avg_50 = np.convolve(steps_by_episode, np.ones(50)/50, mode='valid')\n",
    "steps_moving_avg_100 = np.convolve(steps_by_episode, np.ones(100)/100, mode='valid')\n",
    "steps_moving_avg_500 = np.convolve(steps_by_episode, np.ones(500)/500, mode='valid')\n",
    "steps_moving_avg_1000 = np.convolve(steps_by_episode, np.ones(1000)/1000, mode='valid')\n",
    "\n",
    "\n",
    "# Extract and return the optimal policy\n",
    "policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "for state in range(env.observation_space.n):\n",
    "    action_values = [sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n)]\n",
    "    policy[state] = np.argmax(action_values)\n",
    "\n",
    "# Plot the convergence\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Steps per Episode', color=color)\n",
    "# ax1.plot(steps_moving_avg_1000, label='Steps (MA-1000)', color=color)\n",
    "ax1.plot(steps_moving_avg_500, label='Steps (MA-500)', color=color)\n",
    "ax1.plot(steps_moving_avg_100, label='Steps (MA-100)', color=color, linestyle='dashed')\n",
    "# ax1.plot(steps_moving_avg_50, label='Steps (MA-50)', color=color, linestyle='dashed')\n",
    "# ax1.plot(steps_moving_avg_10, label='Steps (MA-10)', color=color, linestyle='dotted')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Reward per Episode', color=color)\n",
    "# ax2.plot(reward_moving_avg_1000, label='Reward (MA-1000)', color=color)\n",
    "ax2.plot(reward_moving_avg_500, label='Reward (MA-500)', color=color)\n",
    "ax2.plot(reward_moving_avg_100, label='Reward (MA-100)', color=color, linestyle='dashed')\n",
    "# ax2.plot(reward_moving_avg_50, label='Reward (MA-50)', color=color, linestyle='dashed')\n",
    "# ax2.plot(reward_moving_avg_10, label='Reward (MA-10)', color=color, linestyle='dotted')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()\n",
    "plt.title('Convergence of Value Iteration Moving Averages (Steps and Reward per Episode)')\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal policy\n",
    "# print(\"Optimal Policy:\")\n",
    "# print(policy.reshape((8, 8)))\n",
    "print(\"Found it in iteration: \", found_it_iter)\n",
    "\n",
    "# plot state values\n",
    "size=(8,8)\n",
    "Plots.values_heat_map(V, \"Frozen Lake\\nValue Iteration State Values\", size)\n",
    "\n",
    "# plot policy\n",
    "fl_actions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "fl_map_size=(8,8)\n",
    "title=\"FL Mapped Policy\\nArrows represent best action\"\n",
    "val_max, policy_map = Plots.get_policy_map(policy, V, fl_actions, fl_map_size)\n",
    "Plots.plot_policy(val_max, policy_map, fl_map_size, title)\n",
    "\n",
    "print(np.array(print_env))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define State Space\n",
    "\n",
    "# Define the number of bins for each parameter\n",
    "num_bins = 6\n",
    "# Define the ranges for each parameter\n",
    "position_range = [-1.2, 0.6]\n",
    "velocity_range = [-0.07, 0.07]\n",
    "\n",
    "def get_stare_space():\n",
    "    state_space = []\n",
    "\n",
    "    # Create bins for each parameter\n",
    "    for cart_pos in range(num_bins):\n",
    "        for cart_vel in range(num_bins):\n",
    "            # Calculate the value for each parameter\n",
    "            pos_value = position_range[0] + cart_pos * (position_range[1] - position_range[0]) / (num_bins - 1)\n",
    "            velocity_value = velocity_range[0] + cart_vel * (velocity_range[1] - velocity_range[0]) / (num_bins - 1)\n",
    "\n",
    "            # Add the discretized state to the state space list\n",
    "            state_space.append([round(pos_value, 3), round(velocity_value,3)])\n",
    "    return state_space\n",
    "\n",
    "\n",
    "# Function to initialize the value function for each state to zero\n",
    "def initialize_value_function(state_space):\n",
    "    value_function = {}\n",
    "    for state in state_space:\n",
    "        value_function[tuple(state)] = 0\n",
    "    return value_function\n",
    "\n",
    "state_space = get_stare_space()\n",
    "POS_RANGE = [x[0] for x in state_space]\n",
    "VEL_RANGE = [x[1] for x in state_space]\n",
    "\n",
    "def get_state_bin(state):\n",
    "    # get the ranges of each of the parameters divided by bins\n",
    "    closest_numbers = []\n",
    "    for number, range_list in zip(state, [POS_RANGE, VEL_RANGE]):\n",
    "        closest_number = min(range_list, key=lambda x: abs(x - number))\n",
    "        closest_numbers.append(closest_number)\n",
    "    return closest_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, state_space, gamma=0.99, theta=0.0001):\n",
    "    # intialize env\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    # total iterations\n",
    "    total_iterations = 0\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = initialize_value_function(state_space)\n",
    "\n",
    "\n",
    "    # Perform value iteration until convergence\n",
    "    while True:\n",
    "        total_iterations += 1\n",
    "        delta = 0\n",
    "        # Iterate over each state in the state space\n",
    "        for state in state_space:\n",
    "            old_value = value_function[tuple(state)]\n",
    "            # Calculate the value for the current state using the Bellman equation\n",
    "            new_value = 0\n",
    "            for action in range(env.action_space.n):\n",
    "                # Set the environment state to the current state\n",
    "                env.state = np.array(state)\n",
    "                print(env.state())\n",
    "                # Take action in the environment\n",
    "                next_state, reward, done, trunc, _ = env.step(action)\n",
    "                print(reward)\n",
    "                next_state = get_state_bin(next_state)\n",
    "                print(value_function)\n",
    "                # Update the value function based on the Bellman equation\n",
    "                new_value += (reward + gamma * value_function[tuple(next_state)]) / env.action_space.n\n",
    "                env.reset()\n",
    "            # Update the value function for the current state\n",
    "            value_function[tuple(state)] = new_value\n",
    "            # Update the maximum change in value function\n",
    "            delta = max(delta, abs(new_value - old_value))\n",
    "            print(\"Iteration:\", total_iterations, \"State:\", state, \"Value:\", value_function[tuple(state)])\n",
    "        # Check for convergence\n",
    "        if delta < theta and  total_iterations > 50:\n",
    "            break\n",
    "    return value_function\n",
    "\n",
    "state_space = get_stare_space()\n",
    "# Perform value iteration\n",
    "# value_function = value_iteration(env, state_space)\n",
    "\n",
    "# Example value for a state\n",
    "position_index = 0\n",
    "velocity_index = 0\n",
    "# print(\"Value for state (position_index={}, velocity_index={}): {}\".format(position_index, velocity_index, value_function[position_index, velocity_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mccar\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.0\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(0.5, 0.5, 0.0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m state_space \u001b[38;5;241m=\u001b[39m create_state_space()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Perform value iteration\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m V \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_space\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtheta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Print the value function\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, value \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mitems():\n",
      "Cell \u001b[1;32mIn[5], line 86\u001b[0m, in \u001b[0;36mvalue_iteration\u001b[1;34m(env, state_space, gamma, theta)\u001b[0m\n\u001b[0;32m     84\u001b[0m next_state \u001b[38;5;241m=\u001b[39m get_state_bin(next_state)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Update the value function based on the Bellman equation\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m new_value \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m \u001b[43mvalue_function\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m/\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[0;32m     88\u001b[0m     env\u001b[38;5;241m.\u001b[39mreset()\n",
      "\u001b[1;31mKeyError\u001b[0m: (0.5, 0.5, 0.0)"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "verbose = False\n",
    "\n",
    "# Define the number of bins for each parameter\n",
    "num_bins = 5\n",
    "\n",
    "# Define the ranges for each parameter\n",
    "cart_position_range = [-0.5, 0.5]\n",
    "cart_velocity_range = [-0.5, 0.5]  # If value is infinity, set to 10\n",
    "pole_angle_range = [-0.2, 0.2]\n",
    "pole_angular_velocity_range = [-0.5, 0.5]  # If value is infinity, set to 10\n",
    "\n",
    "\n",
    "# Function to create the state space\n",
    "def create_state_space():\n",
    "    # Create lists to store the discretized states\n",
    "    state_space = []\n",
    "\n",
    "    # Create bins for each parameter\n",
    "    for cart_pos in range(num_bins):\n",
    "        for cart_vel in range(num_bins):\n",
    "            for pole_angle in range(num_bins):\n",
    "                for pole_ang_vel in range(num_bins):\n",
    "                    # Calculate the value for each parameter\n",
    "                    cart_pos_value = cart_position_range[0] + cart_pos * (cart_position_range[1] - cart_position_range[0]) / (num_bins - 1)\n",
    "                    cart_vel_value = cart_velocity_range[0] + cart_vel * (cart_velocity_range[1] - cart_velocity_range[0]) / (num_bins - 1)\n",
    "                    pole_angle_value = pole_angle_range[0] + pole_angle * (pole_angle_range[1] - pole_angle_range[0]) / (num_bins - 1)\n",
    "                    pole_ang_vel_value = pole_angular_velocity_range[0] + pole_ang_vel * (pole_angular_velocity_range[1] - pole_angular_velocity_range[0]) / (num_bins - 1)\n",
    "\n",
    "                    # Add the discretized state to the state space list\n",
    "                    state_space.append([cart_pos_value, cart_vel_value, pole_angle_value, pole_ang_vel_value])\n",
    "    return state_space\n",
    "\n",
    "\n",
    "# Function to initialize the value function for each state to zero\n",
    "def initialize_value_function(state_space):\n",
    "    value_function = {}\n",
    "    for state in state_space:\n",
    "        value_function[tuple(state)] = 0\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def get_state_bin(state):\n",
    "    # get the ranges of each of the parameters divided by bins\n",
    "    ranges = []\n",
    "    ranges.append(np.linspace(cart_position_range[0], cart_position_range[1], num_bins))\n",
    "    ranges.append(np.linspace(cart_velocity_range[0], cart_velocity_range[1], num_bins))\n",
    "    ranges.append(np.linspace(pole_angle_range[0], pole_angle_range[1], num_bins))\n",
    "    ranges.append(np.linspace(pole_angular_velocity_range[0], pole_angular_velocity_range[1], num_bins))\n",
    "\n",
    "    closest_numbers = []\n",
    "    for number, range_list in zip(state, ranges):\n",
    "        closest_number = min(range_list, key=lambda x: abs(x - number))\n",
    "        closest_numbers.append(closest_number)\n",
    "    return closest_numbers\n",
    "\n",
    "\n",
    "# Function to perform value iteration\n",
    "def value_iteration(env, state_space, gamma=0.99, theta=0.00001):\n",
    "    # intialize env\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    # total iterations\n",
    "    total_iterations = 0\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = initialize_value_function(state_space)\n",
    "\n",
    "    # Perform value iteration until convergence\n",
    "    while True:\n",
    "        total_iterations += 1\n",
    "        delta = 0\n",
    "        # Iterate over each state in the state space\n",
    "        for state in state_space:\n",
    "            old_value = value_function[tuple(state)]\n",
    "            # Calculate the value for the current state using the Bellman equation\n",
    "            new_value = 0\n",
    "            for action in range(env.action_space.n):\n",
    "                # Set the environment state to the current state\n",
    "                env.state = np.array(state)\n",
    "                # Take action in the environment\n",
    "                next_state, reward, done, trunc, _ = env.step(action)\n",
    "                print(reward)\n",
    "                next_state = get_state_bin(next_state)\n",
    "                # Update the value function based on the Bellman equation\n",
    "                new_value += (reward + gamma * value_function[tuple(next_state)]) / env.action_space.n\n",
    "                if done:\n",
    "                    env.reset()\n",
    "            # Update the value function for the current state\n",
    "            value_function[tuple(state)] = new_value\n",
    "            # Update the maximum change in value function\n",
    "            delta = max(delta, abs(new_value - old_value))\n",
    "            if verbose:\n",
    "                print(\"Iteration:\", total_iterations, \"State:\", state, \"Value:\", value_function[tuple(state)])\n",
    "        # Check for convergence\n",
    "        if delta < theta and  total_iterations > 100:\n",
    "            break\n",
    "    return value_function\n",
    "\n",
    "# gamma\n",
    "gamma = 0.99\n",
    "theta = 0.0001\n",
    "\n",
    "# Discretize the state space\n",
    "state_space = create_state_space()\n",
    "\n",
    "# Perform value iteration\n",
    "V = value_iteration(env, state_space, gamma=gamma, theta=theta)\n",
    "\n",
    "# Print the value function\n",
    "for state, value in V.items():\n",
    "    print(\"State:\", state, \"Value:\", value)\n",
    "\n",
    "# Extract and return the optimal policy\n",
    "# policy = np.zeros(state_space, dtype=int)\n",
    "# for state in range(state_space.shape[0]):\n",
    "#     action_values = [sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n)]\n",
    "#     policy[state] = np.argmax(action_values)\n",
    "\n",
    "\n",
    "# # plot state values\n",
    "# size=(8,8)\n",
    "# Plots.values_heat_map(V, \"Frozen Lake\\nValue Iteration State Values\", size)\n",
    "\n",
    "# # plot policy\n",
    "# fl_actions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "# fl_map_size=(8,8)\n",
    "# title=\"FL Mapped Policy\\nArrows represent best action\"\n",
    "# val_max, policy_map = Plots.get_policy_map(policy, V, fl_actions, fl_map_size)\n",
    "# Plots.plot_policy(val_max, policy_map, fl_map_size, title)\n",
    "\n",
    "# # Clip trailing zeros in case convergence is reached before max iterations\n",
    "# # This is likely when setting the n_iters parameter\n",
    "# max_value_per_iter = np.trim_zeros(np.mean(V_track, axis=1), 'b')\n",
    "# Plots.v_iters_plot(max_value_per_iter, \"Frozen Lake\\nMean Value v Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m state_space \u001b[38;5;241m=\u001b[39m create_state_space()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Perform value iteration\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m V \u001b[38;5;241m=\u001b[39m value_iteration(\u001b[43menv\u001b[49m, state_space, gamma\u001b[38;5;241m=\u001b[39mgamma, theta\u001b[38;5;241m=\u001b[39mtheta)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# Print the value function\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state, value \u001b[38;5;129;01min\u001b[39;00m V\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Value Iteration\n",
    "verbose = False\n",
    "\n",
    "# Define the number of bins for each parameter\n",
    "num_bins = 5\n",
    "\n",
    "# Define the ranges for each parameter\n",
    "cart_position_range = [-0.5, 0.5]\n",
    "cart_velocity_range = [-0.5, 0.5]  # If value is infinity, set to 10\n",
    "pole_angle_range = [-0.2, 0.2]\n",
    "pole_angular_velocity_range = [-0.5, 0.5]  # If value is infinity, set to 10\n",
    "\n",
    "\n",
    "# Function to create the state space\n",
    "def create_state_space():\n",
    "    # Create lists to store the discretized states\n",
    "    state_space = []\n",
    "\n",
    "    # Create bins for each parameter\n",
    "    for cart_pos in range(num_bins):\n",
    "        for cart_vel in range(num_bins):\n",
    "            for pole_angle in range(num_bins):\n",
    "                for pole_ang_vel in range(num_bins):\n",
    "                    # Calculate the value for each parameter\n",
    "                    cart_pos_value = cart_position_range[0] + cart_pos * (cart_position_range[1] - cart_position_range[0]) / (num_bins - 1)\n",
    "                    cart_vel_value = cart_velocity_range[0] + cart_vel * (cart_velocity_range[1] - cart_velocity_range[0]) / (num_bins - 1)\n",
    "                    pole_angle_value = pole_angle_range[0] + pole_angle * (pole_angle_range[1] - pole_angle_range[0]) / (num_bins - 1)\n",
    "                    pole_ang_vel_value = pole_angular_velocity_range[0] + pole_ang_vel * (pole_angular_velocity_range[1] - pole_angular_velocity_range[0]) / (num_bins - 1)\n",
    "\n",
    "                    # Add the discretized state to the state space list\n",
    "                    state_space.append([cart_pos_value, cart_vel_value, pole_angle_value, pole_ang_vel_value])\n",
    "    return state_space\n",
    "\n",
    "\n",
    "# Function to initialize the value function for each state to zero\n",
    "def initialize_value_function(state_space):\n",
    "    value_function = {}\n",
    "    for state in state_space:\n",
    "        value_function[tuple(state)] = 0\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def get_state_bin(state):\n",
    "    # get the ranges of each of the parameters divided by bins\n",
    "    ranges = []\n",
    "    ranges.append(np.linspace(cart_position_range[0], cart_position_range[1], num_bins))\n",
    "    ranges.append(np.linspace(cart_velocity_range[0], cart_velocity_range[1], num_bins))\n",
    "    ranges.append(np.linspace(pole_angle_range[0], pole_angle_range[1], num_bins))\n",
    "    ranges.append(np.linspace(pole_angular_velocity_range[0], pole_angular_velocity_range[1], num_bins))\n",
    "\n",
    "    closest_numbers = []\n",
    "    for number, range_list in zip(state, ranges):\n",
    "        closest_number = min(range_list, key=lambda x: abs(x - number))\n",
    "        closest_numbers.append(closest_number)\n",
    "    return closest_numbers\n",
    "\n",
    "\n",
    "# Function to perform value iteration\n",
    "def value_iteration(env, state_space, gamma=0.99, theta=0.00001):\n",
    "    # intialize env\n",
    "    state = env.reset()[0]\n",
    "\n",
    "    # total iterations\n",
    "    total_iterations = 0\n",
    "\n",
    "    # Initialize value function\n",
    "    value_function = initialize_value_function(state_space)\n",
    "\n",
    "    # Perform value iteration until convergence\n",
    "    while True:\n",
    "        total_iterations += 1\n",
    "        delta = 0\n",
    "        # Iterate over each state in the state space\n",
    "        for state in state_space:\n",
    "            old_value = value_function[tuple(state)]\n",
    "            # Calculate the value for the current state using the Bellman equation\n",
    "            new_value = 0\n",
    "            for action in range(env.action_space.n):\n",
    "                # Set the environment state to the current state\n",
    "                env.state = np.array(state)\n",
    "                # Take action in the environment\n",
    "                next_state, reward, done, trunc, _ = env.step(action)\n",
    "                print(reward)\n",
    "                next_state = get_state_bin(next_state)\n",
    "                # Update the value function based on the Bellman equation\n",
    "                new_value += (reward + gamma * value_function[tuple(next_state)]) / env.action_space.n\n",
    "                if done:\n",
    "                    env.reset()\n",
    "            # Update the value function for the current state\n",
    "            value_function[tuple(state)] = new_value\n",
    "            # Update the maximum change in value function\n",
    "            delta = max(delta, abs(new_value - old_value))\n",
    "            if verbose:\n",
    "                print(\"Iteration:\", total_iterations, \"State:\", state, \"Value:\", value_function[tuple(state)])\n",
    "        # Check for convergence\n",
    "        if delta < theta and  total_iterations > 100:\n",
    "            break\n",
    "    return value_function\n",
    "\n",
    "# gamma\n",
    "gamma = 0.99\n",
    "theta = 0.0001\n",
    "\n",
    "# Discretize the state space\n",
    "state_space = create_state_space()\n",
    "\n",
    "# Perform value iteration\n",
    "V = value_iteration(env, state_space, gamma=gamma, theta=theta)\n",
    "\n",
    "# Print the value function\n",
    "for state, value in V.items():\n",
    "    print(\"State:\", state, \"Value:\", value)\n",
    "\n",
    "# Extract and return the optimal policy\n",
    "# policy = np.zeros(state_space, dtype=int)\n",
    "# for state in range(state_space.shape[0]):\n",
    "#     action_values = [sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n)]\n",
    "#     policy[state] = np.argmax(action_values)\n",
    "\n",
    "\n",
    "# # plot state values\n",
    "# size=(8,8)\n",
    "# Plots.values_heat_map(V, \"Frozen Lake\\nValue Iteration State Values\", size)\n",
    "\n",
    "# # plot policy\n",
    "# fl_actions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "# fl_map_size=(8,8)\n",
    "# title=\"FL Mapped Policy\\nArrows represent best action\"\n",
    "# val_max, policy_map = Plots.get_policy_map(policy, V, fl_actions, fl_map_size)\n",
    "# Plots.plot_policy(val_max, policy_map, fl_map_size, title)\n",
    "\n",
    "# # Clip trailing zeros in case convergence is reached before max iterations\n",
    "# # This is likely when setting the n_iters parameter\n",
    "# max_value_per_iter = np.trim_zeros(np.mean(V_track, axis=1), 'b')\n",
    "# Plots.v_iters_plot(max_value_per_iter, \"Frozen Lake\\nMean Value v Iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Box' object has no attribute 'n'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)\n",
      "Cell \u001b[1;32mIn[16], line 75\u001b[0m\n",
      "\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m policy, V, rewards_by_episode, steps_by_episode, found_it_iter\n",
      "\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Solve the Frozen Lake problem using Policy Iteration\u001b[39;00m\n",
      "\u001b[1;32m---> 75\u001b[0m policy, V, rewards_by_episode, steps_by_episode, found_it_iter \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy_iteration\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# moving avgs\u001b[39;00m\n",
      "\u001b[0;32m     79\u001b[0m reward_moving_avg_10 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconvolve(rewards_by_episode, np\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m10\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalid\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\n",
      "Cell \u001b[1;32mIn[16], line 37\u001b[0m, in \u001b[0;36mpolicy_iteration\u001b[1;34m(env, max_iterations, gamma)\u001b[0m\n",
      "\u001b[0;32m     34\u001b[0m rewards_per_episode \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;32m     35\u001b[0m steps_per_episode \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;32m---> 37\u001b[0m policy \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn, size\u001b[38;5;241m=\u001b[39m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn\u001b[49m)\n",
      "\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iterations):\n",
      "\u001b[0;32m     39\u001b[0m     V \u001b[38;5;241m=\u001b[39m policy_evaluation(env, policy, gamma)\n",
      "\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Box' object has no attribute 'n'"
     ]
    }
   ],
   "source": [
    "# Policy Iteration\n",
    "verbose = True\n",
    "found_it = False\n",
    "found_it_iter = 0\n",
    "\n",
    "# Discretize the state space\n",
    "def discretize_state(observation, bins):\n",
    "    state = []\n",
    "    for i in range(len(observation)):\n",
    "        state.append(np.digitize(observation[i], bins[i]))\n",
    "    return tuple(state)\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=0.99, epsilon=1e-6):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.observation_space.n):\n",
    "            v = V[state]\n",
    "            action = policy[state]\n",
    "            V[state] = sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action])\n",
    "            delta = max(delta, np.abs(v - V[state]))\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(env, V, gamma=0.99):\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "    for state in range(env.observation_space.n):\n",
    "        action_values = [sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n)]\n",
    "        policy[state] = np.argmax(action_values)\n",
    "    return policy\n",
    "\n",
    "def policy_iteration(env, max_iterations=100, gamma=0.99):\n",
    "    rewards_per_episode = []\n",
    "    steps_per_episode = []\n",
    "    \n",
    "    policy = np.random.randint(env.action_space.n, size=env.observation_space.n)\n",
    "    for epi in range(max_iterations):\n",
    "        V = policy_evaluation(env, policy, gamma)\n",
    "        new_policy = policy_improvement(env, V, gamma)\n",
    "        \n",
    "        # Track rewards and steps per episode\n",
    "        episode_reward = 0\n",
    "        episode_steps = 0\n",
    "        \n",
    "        state = env.reset()[0]\n",
    "        while True:\n",
    "            action = new_policy[state]\n",
    "            next_state, reward, done, trunc, _ = env.step(action)\n",
    "            episode_reward += reward\n",
    "            episode_steps += 1\n",
    "\n",
    "            if reward > 0:\n",
    "                if not found_it:\n",
    "                    found_it = True\n",
    "                    found_it_iter = epi\n",
    "                print(\"Found IT!!!!\")\n",
    "\n",
    "            if done or trunc:\n",
    "                rewards_per_episode.append(episode_reward)\n",
    "                steps_per_episode.append(episode_steps)\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"Episode {epi+1} - Reward: {episode_reward}, Steps: {episode_steps}\")\n",
    "        \n",
    "        if np.array_equal(new_policy, policy) and epi > 100:\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy, V, rewards_by_episode, steps_by_episode, found_it_iter\n",
    "\n",
    "\n",
    "# Solve the Frozen Lake problem using Policy Iteration\n",
    "policy, V, rewards_by_episode, steps_by_episode, found_it_iter = policy_iteration(env)\n",
    "\n",
    "\n",
    "# moving avgs\n",
    "reward_moving_avg_10 = np.convolve(rewards_by_episode, np.ones(10)/10, mode='valid')\n",
    "reward_moving_avg_50 = np.convolve(rewards_by_episode, np.ones(50)/50, mode='valid')\n",
    "reward_moving_avg_100 = np.convolve(rewards_by_episode, np.ones(100)/100, mode='valid')\n",
    "reward_moving_avg_500 = np.convolve(rewards_by_episode, np.ones(500)/500, mode='valid')\n",
    "reward_moving_avg_1000 = np.convolve(rewards_by_episode, np.ones(1000)/1000, mode='valid')\n",
    "steps_moving_avg_10 = np.convolve(steps_by_episode, np.ones(10)/10, mode='valid')\n",
    "steps_moving_avg_50 = np.convolve(steps_by_episode, np.ones(50)/50, mode='valid')\n",
    "steps_moving_avg_100 = np.convolve(steps_by_episode, np.ones(100)/100, mode='valid')\n",
    "steps_moving_avg_500 = np.convolve(steps_by_episode, np.ones(500)/500, mode='valid')\n",
    "steps_moving_avg_1000 = np.convolve(steps_by_episode, np.ones(1000)/1000, mode='valid')\n",
    "\n",
    "\n",
    "# # Extract and return the optimal policy\n",
    "# policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "# for state in range(env.observation_space.n):\n",
    "#     action_values = [sum(prob * (reward + gamma * V[next_state]) for prob, next_state, reward, _ in env.P[state][action]) for action in range(env.action_space.n)]\n",
    "#     policy[state] = np.argmax(action_values)\n",
    "\n",
    "# Plot the convergence\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Episode')\n",
    "ax1.set_ylabel('Steps per Episode', color=color)\n",
    "# ax1.plot(steps_moving_avg_1000, label='Steps (MA-1000)', color=color)\n",
    "ax1.plot(steps_moving_avg_500, label='Steps (MA-500)', color=color)\n",
    "ax1.plot(steps_moving_avg_100, label='Steps (MA-100)', color=color, linestyle='dashed')\n",
    "# ax1.plot(steps_moving_avg_50, label='Steps (MA-50)', color=color, linestyle='dashed')\n",
    "# ax1.plot(steps_moving_avg_10, label='Steps (MA-10)', color=color, linestyle='dotted')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "ax2 = ax1.twinx()\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Reward per Episode', color=color)\n",
    "# ax2.plot(reward_moving_avg_1000, label='Reward (MA-1000)', color=color)\n",
    "ax2.plot(reward_moving_avg_500, label='Reward (MA-500)', color=color)\n",
    "ax2.plot(reward_moving_avg_100, label='Reward (MA-100)', color=color, linestyle='dashed')\n",
    "# ax2.plot(reward_moving_avg_50, label='Reward (MA-50)', color=color, linestyle='dashed')\n",
    "# ax2.plot(reward_moving_avg_10, label='Reward (MA-10)', color=color, linestyle='dotted')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "fig.tight_layout()\n",
    "plt.title('Convergence of Value Iteration Moving Averages (Steps and Reward per Episode)')\n",
    "plt.show()\n",
    "\n",
    "# Print the optimal policy\n",
    "# print(\"Optimal Policy:\")\n",
    "# print(policy.reshape((8, 8)))\n",
    "print(\"Found it in iteration: \", found_it_iter)\n",
    "\n",
    "# plot state values\n",
    "size=(8,8)\n",
    "Plots.values_heat_map(V, \"Frozen Lake\\nValue Iteration State Values\", size)\n",
    "\n",
    "# plot policy\n",
    "fl_actions = {0: \"←\", 1: \"↓\", 2: \"→\", 3: \"↑\"}\n",
    "fl_map_size=(8,8)\n",
    "title=\"FL Mapped Policy\\nArrows represent best action\"\n",
    "val_max, policy_map = Plots.get_policy_map(policy, V, fl_actions, fl_map_size)\n",
    "Plots.plot_policy(val_max, policy_map, fl_map_size, title)\n",
    "\n",
    "print(np.array(print_env))\n",
    "\n",
    "\n",
    "# Print the optimal policy\n",
    "print(\"Optimal Policy:\")\n",
    "print(policy.reshape((8, 8)))\n",
    "\n",
    "# Print the value function\n",
    "print(\"\\nValue Function:\")\n",
    "print(V.reshape((8, 8)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
