{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Assignment 1 - main run method for mushroom dataset\n",
    "\"\"\"\n",
    "import sys\n",
    "import pdb\n",
    "sys.path.append(\"C:\\\\users\\\\mccar\\\\miniconda3\\\\lib\\\\site-packages\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import validation_curve, GridSearchCV, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "from utils import learn_curve, val_curve\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.callbacks import EarlyStopping, Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785)\n"
     ]
    }
   ],
   "source": [
    "# get data\n",
    "DATA_PATH = \"data/mnist/mnist_train.csv\"\n",
    "train_df = pd.read_csv(DATA_PATH)\n",
    "DATA_PATH = \"data/mnist/mnist_test.csv\"\n",
    "test_df = pd.read_csv(DATA_PATH)\n",
    "print(train_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "# none for now - dataset came with images already pixelated into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K val: 1   acc: 0.9363\n",
      "K val: 2   acc: 0.9491\n",
      "K val: 3   acc: 0.9552\n",
      "K val: 4   acc: 0.9588\n",
      "K val: 5   acc: 0.9624\n",
      "K val: 6   acc: 0.9657\n",
      "K val: 7   acc: 0.9664\n",
      "K val: 8   acc: 0.9674\n",
      "K val: 9   acc: 0.9693\n",
      "K val: 10   acc: 0.9705\n",
      "Train accs:  {1.0: 0.9370833333333334, 2.0: 0.9541666666666667, 3.0: 0.9636333333333333, 4.0: 0.9692, 5.0: 0.9743833333333334, 6.0: 0.9774666666666667, 7.0: 0.9800166666666666, 8.0: 0.9826166666666667, 9.0: 0.9850666666666666, 10.0: 0.9867166666666667}\n",
      "Test accs:  {1.0: 0.9363, 2.0: 0.9491, 3.0: 0.9552, 4.0: 0.9588, 5.0: 0.9624, 6.0: 0.9657, 7.0: 0.9664, 8.0: 0.9674, 9.0: 0.9693, 10.0: 0.9705}\n"
     ]
    }
   ],
   "source": [
    "# KNN\n",
    "y_train = train_df[\"label\"]\n",
    "x_train = train_df.drop([\"label\"], axis=1)\n",
    "y_test = test_df[\"label\"]\n",
    "x_test = test_df.drop([\"label\"], axis=1)\n",
    "\n",
    "# Curve based on value of K - Validation curve\n",
    "val_knn = False\n",
    "if val_knn:\n",
    "    k_accs = {}\n",
    "    for k_val in range(1,11):\n",
    "        knn = KNeighborsClassifier(n_neighbors=k_val)\n",
    "        knn.fit(x_train, y_train)\n",
    "        y_pred = knn.predict(x_test)\n",
    "        k_accs[k_val] = accuracy_score(y_test, y_pred)\n",
    "        print(\"K val: {}   acc: {}\".format(k_val, k_accs[k_val]))\n",
    "    val_curve(list(k_accs.keys()), list(k_accs.values()), \"knn\")\n",
    "\n",
    "learn_knn = True\n",
    "if learn_knn:\n",
    "    # Shuffle the data without replacement - Learning curve\n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    for x_perc in range(1,11):\n",
    "        x_perc = x_perc / 10\n",
    "        # shuffled_data = shuffle(x_train, random_state=RANDOM_SEED)\n",
    "        x_percent_index = int(x_perc * len(x_train))\n",
    "        # Take the first X% of the shuffled data\n",
    "        first_x_percent = x_train[:x_percent_index]\n",
    "        y_train_iter = y_train[:x_percent_index]\n",
    "        knn = KNeighborsClassifier(n_neighbors=3)\n",
    "        knn.fit(first_x_percent, y_train_iter)\n",
    "        y_pred = knn.predict(x_test)\n",
    "        x_accs[x_perc*10] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = knn.predict(x_train)\n",
    "        train_accs[x_perc*10] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"K val: {}   acc: {}\".format(int(x_perc*10), x_accs[x_perc*10]))\n",
    "    # learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"knn\")\n",
    "    print(\"Train accs: \", train_accs)\n",
    "    print(\"Test accs: \", x_accs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "y_train = train_df[\"label\"]\n",
    "x_train = train_df.drop([\"label\"], axis=1)\n",
    "y_test = test_df[\"label\"]\n",
    "x_test = test_df.drop([\"label\"], axis=1)\n",
    "\n",
    "# DT - hyperparameter tuning - Validation curve\n",
    "dt_max_depth = False\n",
    "\n",
    "if dt_max_depth:\n",
    "    # Max Depth\n",
    "    # Define the range of values for the hyperparameter (e.g., max_depth)\n",
    "    param_range = np.arange(1, 31)\n",
    "\n",
    "    # Create a decision tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "    # Generate training and test scores using validation_curve\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        dt_classifier, x_train, y_train, param_name=\"max_depth\", param_range=param_range,\n",
    "        cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Calculate mean and standard deviation for training and test scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Validation Curve for Decision Tree - Max Depth\")\n",
    "    plt.xlabel(\"Max Depth\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    # Plot the mean training scores\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", marker='o')\n",
    "    # Fill the area around the mean training scores\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\")\n",
    "    # Plot the mean test scores\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", marker='o')\n",
    "    # Fill the area around the mean test scores\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pngs/validation_curve_dt_max_depth.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min samples to split\n",
    "# Define the range of values for the hyperparameter (e.g., max_depth)\n",
    "dt_min_split = False\n",
    "\n",
    "if dt_min_split:\n",
    "    param_range = np.arange(0, 15)\n",
    "\n",
    "    # Create a decision tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=13)\n",
    "\n",
    "    # Generate training and test scores using validation_curve\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        dt_classifier, x_train, y_train, param_name=\"min_samples_split\", param_range=param_range,\n",
    "        cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Calculate mean and standard deviation for training and test scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Validation Curve for Decision Tree - Min Samples Split\")\n",
    "    plt.xlabel(\"Min Samples Split\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    # Plot the mean training scores\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", marker='o')\n",
    "    # Fill the area around the mean training scores\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\")\n",
    "    # Plot the mean test scores\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", marker='o')\n",
    "    # Fill the area around the mean test scores\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pngs/validation_curve_dt_min_samples_split.png\", dpi=300)\n",
    "    # plt.savefig(\"pngs/validation_curve_dt_min_samples_leaf.png\", dpi=300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criterion\n",
    "\n",
    "crit = False\n",
    "\n",
    "if crit:\n",
    "    # Define the range of values for the hyperparameter (e.g., max_depth)\n",
    "    param_range = ['gini', 'entropy']\n",
    "\n",
    "    # Create a decision tree classifier\n",
    "    dt_classifier = DecisionTreeClassifier(random_state=42, max_depth=13)\n",
    "\n",
    "    # Generate training and test scores using validation_curve\n",
    "    train_scores, test_scores = validation_curve(\n",
    "        dt_classifier, x_train, y_train, param_name=\"criterion\", param_range=param_range,\n",
    "        cv=5, scoring=\"accuracy\", n_jobs=-1\n",
    "    )\n",
    "\n",
    "    # Calculate mean and standard deviation for training and test scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    print(test_scores_mean)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Validation Curve for Decision Tree - Criterion\")\n",
    "    plt.xlabel(\"Criterion\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "\n",
    "    # Plot the mean training scores\n",
    "    plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", marker='o')\n",
    "    # Fill the area around the mean training scores\n",
    "    plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "                    train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\")\n",
    "    # Plot the mean test scores\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", marker='o')\n",
    "    # Fill the area around the mean test scores\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\")\n",
    "    plt.xticks(['gini', 'entropy'])\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pngs/validation_curve_dt_criterion.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc val: 1   acc: 0.782\n",
      "Perc val: 2   acc: 0.8169\n",
      "Perc val: 3   acc: 0.8457\n",
      "Perc val: 4   acc: 0.8623\n",
      "Perc val: 5   acc: 0.8662\n",
      "Perc val: 6   acc: 0.8699\n",
      "Perc val: 7   acc: 0.8763\n",
      "Perc val: 8   acc: 0.8828\n",
      "Perc val: 9   acc: 0.8827\n",
      "Perc val: 10   acc: 0.8867\n",
      "Train accs:  {1.0: 0.8031666666666667, 2.0: 0.8492833333333333, 3.0: 0.8849, 4.0: 0.9069166666666667, 5.0: 0.9259, 6.0: 0.9395833333333333, 7.0: 0.9539, 8.0: 0.9648333333333333, 9.0: 0.9746666666666667, 10.0: 0.9818}\n",
      "Test accs:  {1.0: 0.782, 2.0: 0.8169, 3.0: 0.8457, 4.0: 0.8623, 5.0: 0.8662, 6.0: 0.8699, 7.0: 0.8763, 8.0: 0.8828, 9.0: 0.8827, 10.0: 0.8867}\n"
     ]
    }
   ],
   "source": [
    "# DT - Amount of data - Learning curve\n",
    "l_curve = True\n",
    "\n",
    "if l_curve:\n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    for x_perc in range(1,11):\n",
    "        x_perc = x_perc / 10\n",
    "        x_percent_index = int(x_perc * len(x_train))\n",
    "        # Take the first X% of the shuffled data\n",
    "        first_x_percent = x_train[:x_percent_index]\n",
    "        y_train_iter = y_train[:x_percent_index]\n",
    "        dec_tree = DecisionTreeClassifier(criterion='entropy', max_depth=13)\n",
    "        dec_tree.fit(first_x_percent, y_train_iter)\n",
    "        y_pred = dec_tree.predict(x_test)\n",
    "        x_accs[x_perc*10] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = dec_tree.predict(x_train)\n",
    "        train_accs[x_perc*10] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Perc val: {}   acc: {}\".format(int(x_perc*10), x_accs[x_perc*10]))\n",
    "    # learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"dec_tree\")\n",
    "    print(\"Train accs: \", train_accs)\n",
    "    print(\"Test accs: \", x_accs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT - epochs - Learning curve\n",
    "dt_epoch = False\n",
    "\n",
    "if dt_epoch:\n",
    "    # Number of epochs\n",
    "    epochs = 5\n",
    "\n",
    "    # Initialize the decision tree classifier\n",
    "    clf = DecisionTreeClassifier(criterion='entropy', max_depth=13)\n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "\n",
    "        # Train the decision tree on the training set\n",
    "        clf.fit(x_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = clf.predict(x_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        x_accs[epoch+1] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = dec_tree.predict(x_train)\n",
    "        train_accs[epoch+1] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Perc val: {}   acc: {}\".format(epoch+1, x_accs[epoch+1]))\n",
    "    learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"dec_tree_epochs\", x_axis=\"Epochs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting Tree - Hyperparameter tuning - Validation curve\n",
    "boost_gscv = False\n",
    "\n",
    "if boost_gscv:\n",
    "    # Create XGBoost classifier\n",
    "    xgb_classifier = XGBClassifier()\n",
    "\n",
    "    # Define hyperparameters to tune\n",
    "    param_grid = {\n",
    "        'n_estimators': [15, 50, 150, 200],\n",
    "        'learning_rate': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8],\n",
    "        'max_depth': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14],\n",
    "        'min_child_weight': [1, 5, 10, 15],\n",
    "        'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "    }\n",
    "\n",
    "    # Set up cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create GridSearchCV object\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_classifier,\n",
    "        param_grid=param_grid,\n",
    "        scoring='accuracy',\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        n_jobs=-1  # Use all available CPU cores\n",
    "    )\n",
    "\n",
    "    # Perform grid search\n",
    "    grid_search.fit(x_train, y_train)\n",
    "\n",
    "    # Display the best hyperparameters and corresponding accuracy\n",
    "    print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "    print(\"Best Accuracy:\", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting tree - Validation curve - max depth\n",
    "bt_md = False\n",
    "\n",
    "if bt_md:\n",
    "    param_range = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
    "\n",
    "    # Calculate mean and standard deviation for training and test scores\n",
    "    # train_scores_mean = grid_search.cv_results_['mean_train_score']\n",
    "    # train_scores_std = grid_search.cv_results_['std_train_score']\n",
    "    test_scores_mean = grid_search.cv_results_['mean_test_score']\n",
    "    test_scores_std = grid_search.cv_results_['std_test_score']\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(\"Validation Curve for Boosting Tree - Max Depth\")\n",
    "    plt.xlabel(\"Max Depth\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    # Plot the mean training scores\n",
    "    # plt.plot(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", marker='o')\n",
    "    # # Fill the area around the mean training scores\n",
    "    # plt.fill_between(param_range, train_scores_mean - train_scores_std,\n",
    "    #                 train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\")\n",
    "    # Plot the mean test scores\n",
    "    plt.plot(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", marker='o')\n",
    "    # Fill the area around the mean test scores\n",
    "    plt.fill_between(param_range, test_scores_mean - test_scores_std,\n",
    "                    test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.savefig(\"pngs/validation_curve_bt_min_samples_split.png\", dpi=300)\n",
    "    # plt.savefig(\"pngs/validation_curve_dt_min_samples_leaf.png\", dpi=300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc val: 1   acc: 0.9324\n",
      "Perc val: 2   acc: 0.9486\n",
      "Perc val: 3   acc: 0.9593\n",
      "Perc val: 4   acc: 0.9644\n",
      "Perc val: 5   acc: 0.9672\n",
      "Perc val: 6   acc: 0.9712\n",
      "Perc val: 7   acc: 0.9713\n",
      "Perc val: 8   acc: 0.9739\n",
      "Perc val: 9   acc: 0.9759\n",
      "Perc val: 10   acc: 0.9758\n",
      "Train accs:  {1.0: 0.934, 2.0: 0.9579166666666666, 3.0: 0.9701333333333333, 4.0: 0.9769833333333333, 5.0: 0.9829833333333333, 6.0: 0.9872833333333333, 7.0: 0.9908333333333333, 8.0: 0.99485, 9.0: 0.9978833333333333, 10.0: 1.0}\n",
      "Test accs:  {1.0: 0.9324, 2.0: 0.9486, 3.0: 0.9593, 4.0: 0.9644, 5.0: 0.9672, 6.0: 0.9712, 7.0: 0.9713, 8.0: 0.9739, 9.0: 0.9759, 10.0: 0.9758}\n"
     ]
    }
   ],
   "source": [
    "# BT - Learning Curve - amount of data\n",
    "bt_lc = True\n",
    "\n",
    "if bt_lc:\n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    for x_perc in range(1,11):\n",
    "        x_perc = x_perc / 10\n",
    "        x_percent_index = int(x_perc * len(x_train))\n",
    "        # Take the first X% of the shuffled data\n",
    "        first_x_percent = x_train[:x_percent_index]\n",
    "        y_train_iter = y_train[:x_percent_index]\n",
    "        xgb_classifier = XGBClassifier(n_estimators=150, learning_rate=0.6, max_depth=10,\n",
    "                                       min_child_weight=5, subsample=0.9, colsample_bytree=1.0)\n",
    "        xgb_classifier.fit(first_x_percent, y_train_iter)\n",
    "        y_pred = xgb_classifier.predict(x_test)\n",
    "        x_accs[x_perc*10] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = xgb_classifier.predict(x_train)\n",
    "        train_accs[x_perc*10] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Perc val: {}   acc: {}\".format(int(x_perc*10), x_accs[x_perc*10]))\n",
    "    # learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"boost_tree\")\n",
    "    print(\"Train accs: \", train_accs)\n",
    "    print(\"Test accs: \", x_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BT - epochs - Learning curve\n",
    "bt_epoch = False\n",
    "\n",
    "if bt_epoch:\n",
    "    # Number of epochs\n",
    "    epochs = 5\n",
    "\n",
    "    # Initialize the decision tree classifier\n",
    "    xgb_classifier = XGBClassifier(n_estimators=150, learning_rate=0.6, max_depth=10,\n",
    "                                       min_child_weight=5, subsample=0.9, colsample_bytree=1.0)\n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "\n",
    "        # Train the decision tree on the training set\n",
    "        xgb_classifier.fit(x_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = xgb_classifier.predict(x_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        x_accs[epoch+1] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = xgb_classifier.predict(x_train)\n",
    "        train_accs[epoch+1] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Perc val: {}   acc: {}\".format(epoch+1, x_accs[epoch+1]))\n",
    "    learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"boost_tree_epochs\", x_axis=\"Epochs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - Validation curve - C / kernel func\n",
    "hyper = False\n",
    "\n",
    "if hyper:\n",
    "    # Run through the kernel functions and get validation curves for each hyperparameter\n",
    "    for kern_func in [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]:\n",
    "\n",
    "        x_perc = 0.15\n",
    "        x_percent_index = int(x_perc * len(x_train))\n",
    "        # Take the first X% of the shuffled data\n",
    "        first_x_percent = x_train[:x_percent_index]\n",
    "        y_train_iter = y_train[:x_percent_index]\n",
    "\n",
    "        # Define the hyperparameter values to be tested\n",
    "        param_range = np.logspace(-2, 6, 9)\n",
    "        svm_model = SVC(kernel=kern_func)\n",
    "\n",
    "        # Create a validation curve\n",
    "        train_scores, test_scores = validation_curve(\n",
    "            svm_model, first_x_percent, y_train_iter, param_name=\"C\", param_range=param_range,\n",
    "            cv=3, scoring=\"accuracy\", n_jobs=-1\n",
    "        )\n",
    "        # Plot the validation curves\n",
    "        train_scores_mean = np.mean(train_scores, axis=1)\n",
    "        train_scores_std = np.std(train_scores, axis=1)\n",
    "        test_scores_mean = np.mean(test_scores, axis=1)\n",
    "        test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.title(\"Validation Curve for SVM - {}\".format(kern_func))\n",
    "        plt.xlabel(\"C Parameter\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.semilogx(param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=2)\n",
    "        plt.fill_between(param_range, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color=\"darkorange\", lw=2)\n",
    "        plt.semilogx(param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=2)\n",
    "        plt.fill_between(param_range, test_scores_mean - test_scores_std, test_scores_mean + test_scores_std, alpha=0.2, color=\"navy\", lw=2)\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.savefig(\"pngs/validation_curve_svm_{}.png\".format(kern_func), dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perc val: 1   acc: 0.9601\n",
      "Perc val: 2   acc: 0.9692\n",
      "Perc val: 3   acc: 0.9756\n",
      "Perc val: 4   acc: 0.9782\n",
      "Perc val: 5   acc: 0.9798\n",
      "Perc val: 6   acc: 0.981\n",
      "Perc val: 7   acc: 0.9821\n",
      "Perc val: 8   acc: 0.9829\n",
      "Perc val: 9   acc: 0.9837\n",
      "Perc val: 10   acc: 0.9833\n",
      "Train accs:  {1.0: 0.9612166666666667, 2.0: 0.974, 3.0: 0.9809833333333333, 4.0: 0.9859, 5.0: 0.9891333333333333, 6.0: 0.99165, 7.0: 0.9940666666666667, 8.0: 0.9967333333333334, 9.0: 0.9986166666666667, 10.0: 1.0}\n",
      "Test accs:  {1.0: 0.9601, 2.0: 0.9692, 3.0: 0.9756, 4.0: 0.9782, 5.0: 0.9798, 6.0: 0.981, 7.0: 0.9821, 8.0: 0.9829, 9.0: 0.9837, 10.0: 0.9833}\n"
     ]
    }
   ],
   "source": [
    "# SVM - Learning Curve - amount of data\n",
    "svm_lc = True\n",
    "\n",
    "if svm_lc:\n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    for x_perc in range(1,11):\n",
    "        x_perc = x_perc / 10\n",
    "        x_percent_index = int(x_perc * len(x_train))\n",
    "        # Take the first X% of the shuffled data\n",
    "        first_x_percent = x_train[:x_percent_index]\n",
    "        y_train_iter = y_train[:x_percent_index]\n",
    "        svm_model = SVC(kernel=\"rbf\", C=100)\n",
    "        svm_model.fit(first_x_percent, y_train_iter)\n",
    "        y_pred = svm_model.predict(x_test)\n",
    "        x_accs[x_perc*10] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = svm_model.predict(x_train)\n",
    "        train_accs[x_perc*10] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Perc val: {}   acc: {}\".format(int(x_perc*10), x_accs[x_perc*10]))\n",
    "    # learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"svm\")\n",
    "    print(\"Train accs: \", train_accs)\n",
    "    print(\"Test accs: \", x_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM - epochs - Learning curve\n",
    "svm_epoch = False\n",
    "\n",
    "if svm_epoch:\n",
    "    # Number of epochs\n",
    "    epochs = 5\n",
    "\n",
    "    # Initialize the decision tree classifier\n",
    "    svm_model = SVC(kernel=\"rbf\", C=100)\n",
    "    \n",
    "    x_accs = {}\n",
    "    train_accs = {}\n",
    "    # Loop through epochs\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "\n",
    "        # Train the decision tree on the training set\n",
    "        svm_model.fit(x_train, y_train)\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred = svm_model.predict(x_test)\n",
    "\n",
    "        # Evaluate the model\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        x_accs[epoch+1] = accuracy_score(y_test, y_pred)\n",
    "        y_pred_train = svm_model.predict(x_train)\n",
    "        train_accs[epoch+1] = accuracy_score(y_train, y_pred_train)\n",
    "        print(\"Perc val: {}   acc: {}\".format(epoch+1, x_accs[epoch+1]))\n",
    "    learn_curve(list(x_accs.keys()), list(train_accs.values()), list(x_accs.values()), \"svm_epochs\", x_axis=\"Epochs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5864 - loss: 1.1755Test Accuracy after Epoch 1: 81.49%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.5877 - loss: 1.1722 - val_accuracy: 0.8400 - val_loss: 0.5275\n",
      "Epoch 2/250\n",
      "\u001b[1m1667/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8216 - loss: 0.5628Test Accuracy after Epoch 2: 83.10%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8217 - loss: 0.5625 - val_accuracy: 0.8508 - val_loss: 0.4581\n",
      "Epoch 3/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8396 - loss: 0.4983Test Accuracy after Epoch 3: 86.43%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8396 - loss: 0.4982 - val_accuracy: 0.8723 - val_loss: 0.3951\n",
      "Epoch 4/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8607 - loss: 0.4361Test Accuracy after Epoch 4: 88.79%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8608 - loss: 0.4360 - val_accuracy: 0.9075 - val_loss: 0.3092\n",
      "Epoch 5/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8823 - loss: 0.3730Test Accuracy after Epoch 5: 88.85%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8823 - loss: 0.3731 - val_accuracy: 0.9038 - val_loss: 0.3071\n",
      "Epoch 6/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8829 - loss: 0.3658Test Accuracy after Epoch 6: 88.67%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8829 - loss: 0.3657 - val_accuracy: 0.8997 - val_loss: 0.3096\n",
      "Epoch 7/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8884 - loss: 0.3441Test Accuracy after Epoch 7: 90.92%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.8884 - loss: 0.3441 - val_accuracy: 0.9143 - val_loss: 0.2730\n",
      "Epoch 8/250\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9021 - loss: 0.3112Test Accuracy after Epoch 8: 90.50%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9021 - loss: 0.3112 - val_accuracy: 0.9118 - val_loss: 0.2750\n",
      "Epoch 9/250\n",
      "\u001b[1m1678/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9008 - loss: 0.3088Test Accuracy after Epoch 9: 90.52%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9008 - loss: 0.3088 - val_accuracy: 0.9137 - val_loss: 0.2761\n",
      "Epoch 10/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2971Test Accuracy after Epoch 10: 90.50%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9065 - loss: 0.2971 - val_accuracy: 0.9173 - val_loss: 0.2679\n",
      "Epoch 11/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9112 - loss: 0.2783Test Accuracy after Epoch 11: 91.79%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9112 - loss: 0.2783 - val_accuracy: 0.9320 - val_loss: 0.2222\n",
      "Epoch 12/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9144 - loss: 0.2697Test Accuracy after Epoch 12: 91.90%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9144 - loss: 0.2697 - val_accuracy: 0.9292 - val_loss: 0.2247\n",
      "Epoch 13/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9207 - loss: 0.2537Test Accuracy after Epoch 13: 92.54%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9207 - loss: 0.2537 - val_accuracy: 0.9352 - val_loss: 0.2051\n",
      "Epoch 14/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9211 - loss: 0.2495Test Accuracy after Epoch 14: 92.55%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9211 - loss: 0.2495 - val_accuracy: 0.9335 - val_loss: 0.2110\n",
      "Epoch 15/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9247 - loss: 0.2405Test Accuracy after Epoch 15: 91.47%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9247 - loss: 0.2405 - val_accuracy: 0.9370 - val_loss: 0.2070\n",
      "Epoch 16/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9224 - loss: 0.2426Test Accuracy after Epoch 16: 92.28%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9224 - loss: 0.2426 - val_accuracy: 0.9313 - val_loss: 0.2240\n",
      "Epoch 17/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9233 - loss: 0.2463Test Accuracy after Epoch 17: 92.55%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9233 - loss: 0.2462 - val_accuracy: 0.9328 - val_loss: 0.2233\n",
      "Epoch 18/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9290 - loss: 0.2269Test Accuracy after Epoch 18: 92.31%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9290 - loss: 0.2269 - val_accuracy: 0.9320 - val_loss: 0.2221\n",
      "Epoch 19/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9229 - loss: 0.2426Test Accuracy after Epoch 19: 92.65%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9229 - loss: 0.2426 - val_accuracy: 0.9347 - val_loss: 0.2055\n",
      "Epoch 20/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9267 - loss: 0.2298Test Accuracy after Epoch 20: 92.71%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9267 - loss: 0.2299 - val_accuracy: 0.9320 - val_loss: 0.2149\n",
      "Epoch 21/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9288 - loss: 0.2218Test Accuracy after Epoch 21: 93.34%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9288 - loss: 0.2218 - val_accuracy: 0.9427 - val_loss: 0.1851\n",
      "Epoch 22/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9321 - loss: 0.2094Test Accuracy after Epoch 22: 93.17%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9321 - loss: 0.2094 - val_accuracy: 0.9398 - val_loss: 0.1973\n",
      "Epoch 23/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9294 - loss: 0.2214Test Accuracy after Epoch 23: 93.24%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9294 - loss: 0.2214 - val_accuracy: 0.9402 - val_loss: 0.2032\n",
      "Epoch 24/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9330 - loss: 0.2100Test Accuracy after Epoch 24: 93.52%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9330 - loss: 0.2100 - val_accuracy: 0.9410 - val_loss: 0.1959\n",
      "Epoch 25/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9338 - loss: 0.2060Test Accuracy after Epoch 25: 93.53%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9338 - loss: 0.2060 - val_accuracy: 0.9437 - val_loss: 0.1912\n",
      "Epoch 26/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9351 - loss: 0.2044Test Accuracy after Epoch 26: 94.45%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9351 - loss: 0.2044 - val_accuracy: 0.9485 - val_loss: 0.1650\n",
      "Epoch 27/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9385 - loss: 0.1928Test Accuracy after Epoch 27: 93.70%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9385 - loss: 0.1928 - val_accuracy: 0.9450 - val_loss: 0.1803\n",
      "Epoch 28/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.1946Test Accuracy after Epoch 28: 93.83%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.1946 - val_accuracy: 0.9432 - val_loss: 0.1867\n",
      "Epoch 29/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9371 - loss: 0.1963Test Accuracy after Epoch 29: 93.54%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9371 - loss: 0.1963 - val_accuracy: 0.9460 - val_loss: 0.1855\n",
      "Epoch 30/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9372 - loss: 0.1934Test Accuracy after Epoch 30: 93.29%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9372 - loss: 0.1935 - val_accuracy: 0.9393 - val_loss: 0.1951\n",
      "Epoch 31/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9378 - loss: 0.2020Test Accuracy after Epoch 31: 93.52%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9378 - loss: 0.2020 - val_accuracy: 0.9402 - val_loss: 0.1976\n",
      "Epoch 32/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9350 - loss: 0.1989Test Accuracy after Epoch 32: 94.06%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9350 - loss: 0.1989 - val_accuracy: 0.9465 - val_loss: 0.1813\n",
      "Epoch 33/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9427 - loss: 0.1792Test Accuracy after Epoch 33: 93.94%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9427 - loss: 0.1792 - val_accuracy: 0.9485 - val_loss: 0.1781\n",
      "Epoch 34/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9399 - loss: 0.1888Test Accuracy after Epoch 34: 93.91%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9399 - loss: 0.1888 - val_accuracy: 0.9457 - val_loss: 0.1720\n",
      "Epoch 35/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9406 - loss: 0.1848Test Accuracy after Epoch 35: 92.91%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9406 - loss: 0.1849 - val_accuracy: 0.9417 - val_loss: 0.1907\n",
      "Epoch 36/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9398 - loss: 0.1893Test Accuracy after Epoch 36: 93.79%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9398 - loss: 0.1893 - val_accuracy: 0.9500 - val_loss: 0.1604\n",
      "Epoch 37/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9407 - loss: 0.1852Test Accuracy after Epoch 37: 93.99%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9407 - loss: 0.1851 - val_accuracy: 0.9528 - val_loss: 0.1548\n",
      "Epoch 38/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9424 - loss: 0.1806Test Accuracy after Epoch 38: 94.08%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9424 - loss: 0.1807 - val_accuracy: 0.9497 - val_loss: 0.1638\n",
      "Epoch 39/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9435 - loss: 0.1772Test Accuracy after Epoch 39: 94.32%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9435 - loss: 0.1772 - val_accuracy: 0.9475 - val_loss: 0.1616\n",
      "Epoch 40/250\n",
      "\u001b[1m1683/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9415 - loss: 0.1845Test Accuracy after Epoch 40: 94.31%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9415 - loss: 0.1845 - val_accuracy: 0.9475 - val_loss: 0.1608\n",
      "Epoch 41/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9417 - loss: 0.1825Test Accuracy after Epoch 41: 93.90%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9417 - loss: 0.1825 - val_accuracy: 0.9440 - val_loss: 0.1771\n",
      "Epoch 42/250\n",
      "\u001b[1m1678/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9400 - loss: 0.1901Test Accuracy after Epoch 42: 94.61%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9400 - loss: 0.1901 - val_accuracy: 0.9467 - val_loss: 0.1803\n",
      "Epoch 43/250\n",
      "\u001b[1m1683/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9440 - loss: 0.1763Test Accuracy after Epoch 43: 93.77%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9440 - loss: 0.1764 - val_accuracy: 0.9475 - val_loss: 0.1613\n",
      "Epoch 44/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9448 - loss: 0.1738Test Accuracy after Epoch 44: 94.39%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9448 - loss: 0.1738 - val_accuracy: 0.9485 - val_loss: 0.1649\n",
      "Epoch 45/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9454 - loss: 0.1751Test Accuracy after Epoch 45: 94.53%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9454 - loss: 0.1752 - val_accuracy: 0.9540 - val_loss: 0.1544\n",
      "Epoch 46/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9489 - loss: 0.1632Test Accuracy after Epoch 46: 94.40%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9489 - loss: 0.1632 - val_accuracy: 0.9540 - val_loss: 0.1791\n",
      "Epoch 47/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9494 - loss: 0.1607Test Accuracy after Epoch 47: 94.45%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9494 - loss: 0.1607 - val_accuracy: 0.9540 - val_loss: 0.1590\n",
      "Epoch 48/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9453 - loss: 0.1672Test Accuracy after Epoch 48: 94.15%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9453 - loss: 0.1672 - val_accuracy: 0.9523 - val_loss: 0.1653\n",
      "Epoch 49/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9506 - loss: 0.1554Test Accuracy after Epoch 49: 94.84%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9506 - loss: 0.1554 - val_accuracy: 0.9490 - val_loss: 0.1651\n",
      "Epoch 50/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9516 - loss: 0.1557Test Accuracy after Epoch 50: 94.93%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9516 - loss: 0.1557 - val_accuracy: 0.9572 - val_loss: 0.1459\n",
      "Epoch 51/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9511 - loss: 0.1570Test Accuracy after Epoch 51: 94.75%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9511 - loss: 0.1570 - val_accuracy: 0.9550 - val_loss: 0.1501\n",
      "Epoch 52/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9502 - loss: 0.1562Test Accuracy after Epoch 52: 94.76%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9502 - loss: 0.1563 - val_accuracy: 0.9530 - val_loss: 0.1542\n",
      "Epoch 53/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9501 - loss: 0.1586Test Accuracy after Epoch 53: 94.35%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9501 - loss: 0.1586 - val_accuracy: 0.9490 - val_loss: 0.1684\n",
      "Epoch 54/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9498 - loss: 0.1593Test Accuracy after Epoch 54: 94.61%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9498 - loss: 0.1593 - val_accuracy: 0.9555 - val_loss: 0.1553\n",
      "Epoch 55/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9524 - loss: 0.1510Test Accuracy after Epoch 55: 94.87%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9524 - loss: 0.1511 - val_accuracy: 0.9520 - val_loss: 0.1585\n",
      "Epoch 56/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9489 - loss: 0.1606Test Accuracy after Epoch 56: 94.75%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9489 - loss: 0.1606 - val_accuracy: 0.9537 - val_loss: 0.1448\n",
      "Epoch 57/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9544 - loss: 0.1442Test Accuracy after Epoch 57: 95.32%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9543 - loss: 0.1443 - val_accuracy: 0.9608 - val_loss: 0.1364\n",
      "Epoch 58/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9546 - loss: 0.1425Test Accuracy after Epoch 58: 94.74%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9546 - loss: 0.1425 - val_accuracy: 0.9602 - val_loss: 0.1332\n",
      "Epoch 59/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9548 - loss: 0.1417Test Accuracy after Epoch 59: 94.88%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9547 - loss: 0.1417 - val_accuracy: 0.9612 - val_loss: 0.1414\n",
      "Epoch 60/250\n",
      "\u001b[1m1659/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9529 - loss: 0.1451Test Accuracy after Epoch 60: 95.16%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9529 - loss: 0.1451 - val_accuracy: 0.9553 - val_loss: 0.1530\n",
      "Epoch 61/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.1490Test Accuracy after Epoch 61: 94.60%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9532 - loss: 0.1491 - val_accuracy: 0.9578 - val_loss: 0.1469\n",
      "Epoch 62/250\n",
      "\u001b[1m1654/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9553 - loss: 0.1443Test Accuracy after Epoch 62: 94.72%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9552 - loss: 0.1445 - val_accuracy: 0.9567 - val_loss: 0.1385\n",
      "Epoch 63/250\n",
      "\u001b[1m1657/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1515Test Accuracy after Epoch 63: 95.12%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1514 - val_accuracy: 0.9582 - val_loss: 0.1409\n",
      "Epoch 64/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1406Test Accuracy after Epoch 64: 94.84%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1406 - val_accuracy: 0.9558 - val_loss: 0.1542\n",
      "Epoch 65/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1416Test Accuracy after Epoch 65: 94.91%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1417 - val_accuracy: 0.9517 - val_loss: 0.1573\n",
      "Epoch 66/250\n",
      "\u001b[1m1660/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1485Test Accuracy after Epoch 66: 94.82%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.1485 - val_accuracy: 0.9572 - val_loss: 0.1424\n",
      "Epoch 67/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9565 - loss: 0.1392Test Accuracy after Epoch 67: 95.07%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9565 - loss: 0.1392 - val_accuracy: 0.9578 - val_loss: 0.1388\n",
      "Epoch 68/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9561 - loss: 0.1437Test Accuracy after Epoch 68: 94.62%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9561 - loss: 0.1437 - val_accuracy: 0.9558 - val_loss: 0.1457\n",
      "Epoch 69/250\n",
      "\u001b[1m1683/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9560 - loss: 0.1441Test Accuracy after Epoch 69: 95.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9560 - loss: 0.1441 - val_accuracy: 0.9592 - val_loss: 0.1417\n",
      "Epoch 70/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9544 - loss: 0.1438Test Accuracy after Epoch 70: 95.09%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9544 - loss: 0.1438 - val_accuracy: 0.9568 - val_loss: 0.1428\n",
      "Epoch 71/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1433Test Accuracy after Epoch 71: 95.20%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1433 - val_accuracy: 0.9590 - val_loss: 0.1482\n",
      "Epoch 72/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1458Test Accuracy after Epoch 72: 94.95%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9542 - loss: 0.1459 - val_accuracy: 0.9562 - val_loss: 0.1446\n",
      "Epoch 73/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9510 - loss: 0.1495Test Accuracy after Epoch 73: 94.90%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9510 - loss: 0.1495 - val_accuracy: 0.9577 - val_loss: 0.1523\n",
      "Epoch 74/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1454Test Accuracy after Epoch 74: 95.33%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1454 - val_accuracy: 0.9600 - val_loss: 0.1417\n",
      "Epoch 75/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1387Test Accuracy after Epoch 75: 95.19%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9562 - loss: 0.1387 - val_accuracy: 0.9637 - val_loss: 0.1261\n",
      "Epoch 76/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9570 - loss: 0.1338Test Accuracy after Epoch 76: 95.09%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9570 - loss: 0.1338 - val_accuracy: 0.9605 - val_loss: 0.1300\n",
      "Epoch 77/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9559 - loss: 0.1418Test Accuracy after Epoch 77: 95.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9558 - loss: 0.1418 - val_accuracy: 0.9583 - val_loss: 0.1417\n",
      "Epoch 78/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1418Test Accuracy after Epoch 78: 95.27%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9549 - loss: 0.1418 - val_accuracy: 0.9592 - val_loss: 0.1374\n",
      "Epoch 79/250\n",
      "\u001b[1m1657/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1323Test Accuracy after Epoch 79: 95.16%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1324 - val_accuracy: 0.9600 - val_loss: 0.1358\n",
      "Epoch 80/250\n",
      "\u001b[1m1678/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1268Test Accuracy after Epoch 80: 95.47%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1268 - val_accuracy: 0.9610 - val_loss: 0.1353\n",
      "Epoch 81/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1325Test Accuracy after Epoch 81: 95.79%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9583 - loss: 0.1325 - val_accuracy: 0.9615 - val_loss: 0.1267\n",
      "Epoch 82/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9573 - loss: 0.1368Test Accuracy after Epoch 82: 95.34%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9573 - loss: 0.1368 - val_accuracy: 0.9605 - val_loss: 0.1285\n",
      "Epoch 83/250\n",
      "\u001b[1m1669/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.1414Test Accuracy after Epoch 83: 95.21%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9557 - loss: 0.1414 - val_accuracy: 0.9633 - val_loss: 0.1226\n",
      "Epoch 84/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1335Test Accuracy after Epoch 84: 95.26%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9577 - loss: 0.1336 - val_accuracy: 0.9647 - val_loss: 0.1193\n",
      "Epoch 85/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1341Test Accuracy after Epoch 85: 95.38%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1341 - val_accuracy: 0.9637 - val_loss: 0.1249\n",
      "Epoch 86/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9580 - loss: 0.1317Test Accuracy after Epoch 86: 95.22%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9580 - loss: 0.1317 - val_accuracy: 0.9592 - val_loss: 0.1357\n",
      "Epoch 87/250\n",
      "\u001b[1m1670/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.1421Test Accuracy after Epoch 87: 94.83%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9556 - loss: 0.1420 - val_accuracy: 0.9578 - val_loss: 0.1305\n",
      "Epoch 88/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1327Test Accuracy after Epoch 88: 95.22%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1327 - val_accuracy: 0.9588 - val_loss: 0.1260\n",
      "Epoch 89/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1277Test Accuracy after Epoch 89: 95.63%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1277 - val_accuracy: 0.9603 - val_loss: 0.1289\n",
      "Epoch 90/250\n",
      "\u001b[1m1660/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1282Test Accuracy after Epoch 90: 95.72%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9598 - loss: 0.1283 - val_accuracy: 0.9602 - val_loss: 0.1260\n",
      "Epoch 91/250\n",
      "\u001b[1m1660/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1256Test Accuracy after Epoch 91: 95.58%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1257 - val_accuracy: 0.9592 - val_loss: 0.1367\n",
      "Epoch 92/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9596 - loss: 0.1284Test Accuracy after Epoch 92: 95.33%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9596 - loss: 0.1285 - val_accuracy: 0.9567 - val_loss: 0.1323\n",
      "Epoch 93/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9578 - loss: 0.1348Test Accuracy after Epoch 93: 95.10%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9578 - loss: 0.1348 - val_accuracy: 0.9588 - val_loss: 0.1385\n",
      "Epoch 94/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9580 - loss: 0.1335Test Accuracy after Epoch 94: 95.65%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9580 - loss: 0.1335 - val_accuracy: 0.9598 - val_loss: 0.1293\n",
      "Epoch 95/250\n",
      "\u001b[1m1670/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9591 - loss: 0.1285Test Accuracy after Epoch 95: 95.67%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9591 - loss: 0.1285 - val_accuracy: 0.9602 - val_loss: 0.1362\n",
      "Epoch 96/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.1279Test Accuracy after Epoch 96: 95.61%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.1280 - val_accuracy: 0.9608 - val_loss: 0.1251\n",
      "Epoch 97/250\n",
      "\u001b[1m1670/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1337Test Accuracy after Epoch 97: 95.11%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9569 - loss: 0.1338 - val_accuracy: 0.9627 - val_loss: 0.1334\n",
      "Epoch 98/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1297Test Accuracy after Epoch 98: 95.10%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9592 - loss: 0.1297 - val_accuracy: 0.9620 - val_loss: 0.1297\n",
      "Epoch 99/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 0.1292Test Accuracy after Epoch 99: 95.67%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9586 - loss: 0.1292 - val_accuracy: 0.9653 - val_loss: 0.1216\n",
      "Epoch 100/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1219Test Accuracy after Epoch 100: 95.70%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9600 - loss: 0.1220 - val_accuracy: 0.9600 - val_loss: 0.1338\n",
      "Epoch 101/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9590 - loss: 0.1268Test Accuracy after Epoch 101: 95.20%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9590 - loss: 0.1268 - val_accuracy: 0.9583 - val_loss: 0.1317\n",
      "Epoch 102/250\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1312Test Accuracy after Epoch 102: 95.60%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9582 - loss: 0.1312 - val_accuracy: 0.9628 - val_loss: 0.1289\n",
      "Epoch 103/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9624 - loss: 0.1230Test Accuracy after Epoch 103: 95.37%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9624 - loss: 0.1231 - val_accuracy: 0.9595 - val_loss: 0.1393\n",
      "Epoch 104/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9595 - loss: 0.1267Test Accuracy after Epoch 104: 95.54%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9595 - loss: 0.1267 - val_accuracy: 0.9622 - val_loss: 0.1266\n",
      "Epoch 105/250\n",
      "\u001b[1m1663/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9613 - loss: 0.1231Test Accuracy after Epoch 105: 95.58%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9612 - loss: 0.1231 - val_accuracy: 0.9590 - val_loss: 0.1313\n",
      "Epoch 106/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9602 - loss: 0.1248Test Accuracy after Epoch 106: 95.33%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9602 - loss: 0.1248 - val_accuracy: 0.9597 - val_loss: 0.1362\n",
      "Epoch 107/250\n",
      "\u001b[1m1669/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1220Test Accuracy after Epoch 107: 95.69%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9604 - loss: 0.1220 - val_accuracy: 0.9623 - val_loss: 0.1269\n",
      "Epoch 108/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.1232Test Accuracy after Epoch 108: 95.53%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9601 - loss: 0.1232 - val_accuracy: 0.9615 - val_loss: 0.1275\n",
      "Epoch 109/250\n",
      "\u001b[1m1662/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9616 - loss: 0.1199Test Accuracy after Epoch 109: 95.37%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9616 - loss: 0.1200 - val_accuracy: 0.9603 - val_loss: 0.1287\n",
      "Epoch 110/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9616 - loss: 0.1184Test Accuracy after Epoch 110: 95.45%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9616 - loss: 0.1184 - val_accuracy: 0.9642 - val_loss: 0.1230\n",
      "Epoch 111/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.1178Test Accuracy after Epoch 111: 95.66%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9635 - loss: 0.1178 - val_accuracy: 0.9620 - val_loss: 0.1309\n",
      "Epoch 112/250\n",
      "\u001b[1m1661/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9618 - loss: 0.1213Test Accuracy after Epoch 112: 95.54%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9618 - loss: 0.1214 - val_accuracy: 0.9573 - val_loss: 0.1286\n",
      "Epoch 113/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.1256Test Accuracy after Epoch 113: 95.68%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9609 - loss: 0.1256 - val_accuracy: 0.9635 - val_loss: 0.1235\n",
      "Epoch 114/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.1211Test Accuracy after Epoch 114: 95.55%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9622 - loss: 0.1211 - val_accuracy: 0.9615 - val_loss: 0.1300\n",
      "Epoch 115/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1248Test Accuracy after Epoch 115: 95.85%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1248 - val_accuracy: 0.9612 - val_loss: 0.1345\n",
      "Epoch 116/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9627 - loss: 0.1206Test Accuracy after Epoch 116: 95.65%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9627 - loss: 0.1206 - val_accuracy: 0.9620 - val_loss: 0.1377\n",
      "Epoch 117/250\n",
      "\u001b[1m1668/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.1231Test Accuracy after Epoch 117: 96.02%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9607 - loss: 0.1230 - val_accuracy: 0.9657 - val_loss: 0.1230\n",
      "Epoch 118/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9638 - loss: 0.1128Test Accuracy after Epoch 118: 95.27%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9638 - loss: 0.1129 - val_accuracy: 0.9612 - val_loss: 0.1339\n",
      "Epoch 119/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9624 - loss: 0.1193Test Accuracy after Epoch 119: 95.37%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9624 - loss: 0.1193 - val_accuracy: 0.9625 - val_loss: 0.1238\n",
      "Epoch 120/250\n",
      "\u001b[1m1683/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.1139Test Accuracy after Epoch 120: 95.45%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.1140 - val_accuracy: 0.9618 - val_loss: 0.1246\n",
      "Epoch 121/250\n",
      "\u001b[1m1665/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1150Test Accuracy after Epoch 121: 96.01%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1150 - val_accuracy: 0.9637 - val_loss: 0.1221\n",
      "Epoch 122/250\n",
      "\u001b[1m1661/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.1131Test Accuracy after Epoch 122: 95.95%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9644 - loss: 0.1131 - val_accuracy: 0.9623 - val_loss: 0.1272\n",
      "Epoch 123/250\n",
      "\u001b[1m1662/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1161Test Accuracy after Epoch 123: 95.77%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1161 - val_accuracy: 0.9635 - val_loss: 0.1260\n",
      "Epoch 124/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1130Test Accuracy after Epoch 124: 95.66%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9650 - loss: 0.1130 - val_accuracy: 0.9645 - val_loss: 0.1310\n",
      "Epoch 125/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1179Test Accuracy after Epoch 125: 95.32%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1180 - val_accuracy: 0.9595 - val_loss: 0.1407\n",
      "Epoch 126/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1202Test Accuracy after Epoch 126: 95.68%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1202 - val_accuracy: 0.9597 - val_loss: 0.1349\n",
      "Epoch 127/250\n",
      "\u001b[1m1661/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9615 - loss: 0.1173Test Accuracy after Epoch 127: 95.35%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9615 - loss: 0.1174 - val_accuracy: 0.9577 - val_loss: 0.1422\n",
      "Epoch 128/250\n",
      "\u001b[1m1667/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9614 - loss: 0.1182Test Accuracy after Epoch 128: 95.55%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9614 - loss: 0.1182 - val_accuracy: 0.9607 - val_loss: 0.1373\n",
      "Epoch 129/250\n",
      "\u001b[1m1662/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1155Test Accuracy after Epoch 129: 95.96%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1156 - val_accuracy: 0.9647 - val_loss: 0.1297\n",
      "Epoch 130/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1172Test Accuracy after Epoch 130: 95.72%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1172 - val_accuracy: 0.9630 - val_loss: 0.1250\n",
      "Epoch 131/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1145Test Accuracy after Epoch 131: 95.65%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1145 - val_accuracy: 0.9653 - val_loss: 0.1240\n",
      "Epoch 132/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1114Test Accuracy after Epoch 132: 96.03%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1114 - val_accuracy: 0.9645 - val_loss: 0.1257\n",
      "Epoch 133/250\n",
      "\u001b[1m1662/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1165Test Accuracy after Epoch 133: 95.66%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9647 - loss: 0.1166 - val_accuracy: 0.9623 - val_loss: 0.1265\n",
      "Epoch 134/250\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9645 - loss: 0.1114Test Accuracy after Epoch 134: 95.40%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9645 - loss: 0.1114 - val_accuracy: 0.9628 - val_loss: 0.1275\n",
      "Epoch 135/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9603 - loss: 0.1247Test Accuracy after Epoch 135: 95.10%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9603 - loss: 0.1247 - val_accuracy: 0.9612 - val_loss: 0.1329\n",
      "Epoch 136/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.1207Test Accuracy after Epoch 136: 95.55%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9632 - loss: 0.1207 - val_accuracy: 0.9637 - val_loss: 0.1290\n",
      "Epoch 137/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1194Test Accuracy after Epoch 137: 95.68%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1195 - val_accuracy: 0.9617 - val_loss: 0.1299\n",
      "Epoch 138/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1181Test Accuracy after Epoch 138: 95.60%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9623 - loss: 0.1181 - val_accuracy: 0.9613 - val_loss: 0.1251\n",
      "Epoch 139/250\n",
      "\u001b[1m1661/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.1115Test Accuracy after Epoch 139: 96.00%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9655 - loss: 0.1116 - val_accuracy: 0.9683 - val_loss: 0.1136\n",
      "Epoch 140/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1130Test Accuracy after Epoch 140: 96.03%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.1130 - val_accuracy: 0.9677 - val_loss: 0.1128\n",
      "Epoch 141/250\n",
      "\u001b[1m1667/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1157Test Accuracy after Epoch 141: 95.87%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1157 - val_accuracy: 0.9623 - val_loss: 0.1314\n",
      "Epoch 142/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.1116Test Accuracy after Epoch 142: 95.73%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.1116 - val_accuracy: 0.9623 - val_loss: 0.1292\n",
      "Epoch 143/250\n",
      "\u001b[1m1668/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1135Test Accuracy after Epoch 143: 96.12%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1136 - val_accuracy: 0.9615 - val_loss: 0.1242\n",
      "Epoch 144/250\n",
      "\u001b[1m1668/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.1114Test Accuracy after Epoch 144: 95.59%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.1114 - val_accuracy: 0.9588 - val_loss: 0.1338\n",
      "Epoch 145/250\n",
      "\u001b[1m1667/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1095Test Accuracy after Epoch 145: 96.08%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9653 - loss: 0.1095 - val_accuracy: 0.9632 - val_loss: 0.1249\n",
      "Epoch 146/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1033Test Accuracy after Epoch 146: 96.15%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1034 - val_accuracy: 0.9635 - val_loss: 0.1153\n",
      "Epoch 147/250\n",
      "\u001b[1m1671/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9680 - loss: 0.1009Test Accuracy after Epoch 147: 96.19%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.1010 - val_accuracy: 0.9677 - val_loss: 0.1113\n",
      "Epoch 148/250\n",
      "\u001b[1m1668/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1051Test Accuracy after Epoch 148: 95.47%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9660 - loss: 0.1051 - val_accuracy: 0.9643 - val_loss: 0.1250\n",
      "Epoch 149/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1149Test Accuracy after Epoch 149: 95.85%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9640 - loss: 0.1149 - val_accuracy: 0.9652 - val_loss: 0.1218\n",
      "Epoch 150/250\n",
      "\u001b[1m1660/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.1026Test Accuracy after Epoch 150: 95.92%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9677 - loss: 0.1027 - val_accuracy: 0.9635 - val_loss: 0.1244\n",
      "Epoch 151/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.1102Test Accuracy after Epoch 151: 96.01%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.1102 - val_accuracy: 0.9628 - val_loss: 0.1149\n",
      "Epoch 152/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1064Test Accuracy after Epoch 152: 95.75%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1064 - val_accuracy: 0.9645 - val_loss: 0.1190\n",
      "Epoch 153/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1044Test Accuracy after Epoch 153: 96.07%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9667 - loss: 0.1044 - val_accuracy: 0.9667 - val_loss: 0.1174\n",
      "Epoch 154/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1039Test Accuracy after Epoch 154: 96.03%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9672 - loss: 0.1039 - val_accuracy: 0.9633 - val_loss: 0.1267\n",
      "Epoch 155/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1021Test Accuracy after Epoch 155: 95.94%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1022 - val_accuracy: 0.9680 - val_loss: 0.1084\n",
      "Epoch 156/250\n",
      "\u001b[1m1669/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1038Test Accuracy after Epoch 156: 95.66%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9664 - loss: 0.1038 - val_accuracy: 0.9647 - val_loss: 0.1294\n",
      "Epoch 157/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.1004Test Accuracy after Epoch 157: 96.09%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9679 - loss: 0.1004 - val_accuracy: 0.9628 - val_loss: 0.1208\n",
      "Epoch 158/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.1129Test Accuracy after Epoch 158: 96.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9651 - loss: 0.1129 - val_accuracy: 0.9645 - val_loss: 0.1189\n",
      "Epoch 159/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.1081Test Accuracy after Epoch 159: 95.90%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.1081 - val_accuracy: 0.9650 - val_loss: 0.1189\n",
      "Epoch 160/250\n",
      "\u001b[1m1672/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1118Test Accuracy after Epoch 160: 95.96%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9646 - loss: 0.1118 - val_accuracy: 0.9637 - val_loss: 0.1221\n",
      "Epoch 161/250\n",
      "\u001b[1m1663/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9639 - loss: 0.1129Test Accuracy after Epoch 161: 95.83%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9639 - loss: 0.1129 - val_accuracy: 0.9643 - val_loss: 0.1179\n",
      "Epoch 162/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1066Test Accuracy after Epoch 162: 96.00%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1066 - val_accuracy: 0.9642 - val_loss: 0.1190\n",
      "Epoch 163/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.1061Test Accuracy after Epoch 163: 95.87%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9654 - loss: 0.1061 - val_accuracy: 0.9667 - val_loss: 0.1173\n",
      "Epoch 164/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1042Test Accuracy after Epoch 164: 95.98%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9666 - loss: 0.1042 - val_accuracy: 0.9642 - val_loss: 0.1190\n",
      "Epoch 165/250\n",
      "\u001b[1m1683/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.1059Test Accuracy after Epoch 165: 95.99%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9658 - loss: 0.1058 - val_accuracy: 0.9668 - val_loss: 0.1176\n",
      "Epoch 166/250\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1024Test Accuracy after Epoch 166: 95.97%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1024 - val_accuracy: 0.9643 - val_loss: 0.1255\n",
      "Epoch 167/250\n",
      "\u001b[1m1670/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1095Test Accuracy after Epoch 167: 95.91%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.1095 - val_accuracy: 0.9652 - val_loss: 0.1188\n",
      "Epoch 168/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9670 - loss: 0.1045Test Accuracy after Epoch 168: 96.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9670 - loss: 0.1045 - val_accuracy: 0.9663 - val_loss: 0.1148\n",
      "Epoch 169/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0973Test Accuracy after Epoch 169: 96.08%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9686 - loss: 0.0973 - val_accuracy: 0.9688 - val_loss: 0.1142\n",
      "Epoch 170/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9675 - loss: 0.0998Test Accuracy after Epoch 170: 95.97%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9675 - loss: 0.0999 - val_accuracy: 0.9657 - val_loss: 0.1213\n",
      "Epoch 171/250\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0991Test Accuracy after Epoch 171: 96.02%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0991 - val_accuracy: 0.9685 - val_loss: 0.1153\n",
      "Epoch 172/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1027Test Accuracy after Epoch 172: 95.79%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9668 - loss: 0.1027 - val_accuracy: 0.9625 - val_loss: 0.1220\n",
      "Epoch 173/250\n",
      "\u001b[1m1660/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1041Test Accuracy after Epoch 173: 96.00%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1041 - val_accuracy: 0.9642 - val_loss: 0.1168\n",
      "Epoch 174/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1032Test Accuracy after Epoch 174: 96.01%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1032 - val_accuracy: 0.9635 - val_loss: 0.1165\n",
      "Epoch 175/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1045Test Accuracy after Epoch 175: 96.11%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9676 - loss: 0.1045 - val_accuracy: 0.9675 - val_loss: 0.1156\n",
      "Epoch 176/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1042Test Accuracy after Epoch 176: 96.01%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.1043 - val_accuracy: 0.9637 - val_loss: 0.1219\n",
      "Epoch 177/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1054Test Accuracy after Epoch 177: 96.00%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9663 - loss: 0.1055 - val_accuracy: 0.9633 - val_loss: 0.1198\n",
      "Epoch 178/250\n",
      "\u001b[1m1670/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9671 - loss: 0.1026Test Accuracy after Epoch 178: 95.88%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9671 - loss: 0.1027 - val_accuracy: 0.9672 - val_loss: 0.1110\n",
      "Epoch 179/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1016Test Accuracy after Epoch 179: 96.09%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9673 - loss: 0.1016 - val_accuracy: 0.9672 - val_loss: 0.1180\n",
      "Epoch 180/250\n",
      "\u001b[1m1678/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9671 - loss: 0.0991Test Accuracy after Epoch 180: 95.85%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9671 - loss: 0.0992 - val_accuracy: 0.9632 - val_loss: 0.1243\n",
      "Epoch 181/250\n",
      "\u001b[1m1683/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1011Test Accuracy after Epoch 181: 95.58%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9674 - loss: 0.1011 - val_accuracy: 0.9645 - val_loss: 0.1233\n",
      "Epoch 182/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.0995Test Accuracy after Epoch 182: 95.88%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.0995 - val_accuracy: 0.9642 - val_loss: 0.1214\n",
      "Epoch 183/250\n",
      "\u001b[1m1667/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.0995Test Accuracy after Epoch 183: 96.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.0995 - val_accuracy: 0.9653 - val_loss: 0.1179\n",
      "Epoch 184/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.0964Test Accuracy after Epoch 184: 96.20%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9695 - loss: 0.0965 - val_accuracy: 0.9658 - val_loss: 0.1160\n",
      "Epoch 185/250\n",
      "\u001b[1m1662/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.0995Test Accuracy after Epoch 185: 96.08%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9684 - loss: 0.0994 - val_accuracy: 0.9683 - val_loss: 0.1128\n",
      "Epoch 186/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0970Test Accuracy after Epoch 186: 95.93%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9693 - loss: 0.0971 - val_accuracy: 0.9658 - val_loss: 0.1153\n",
      "Epoch 187/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9711 - loss: 0.0911Test Accuracy after Epoch 187: 96.11%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9710 - loss: 0.0911 - val_accuracy: 0.9668 - val_loss: 0.1159\n",
      "Epoch 188/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0934Test Accuracy after Epoch 188: 96.30%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0934 - val_accuracy: 0.9662 - val_loss: 0.1164\n",
      "Epoch 189/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.1026Test Accuracy after Epoch 189: 96.20%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9682 - loss: 0.1026 - val_accuracy: 0.9672 - val_loss: 0.1103\n",
      "Epoch 190/250\n",
      "\u001b[1m1665/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.0938Test Accuracy after Epoch 190: 96.40%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9705 - loss: 0.0938 - val_accuracy: 0.9658 - val_loss: 0.1083\n",
      "Epoch 191/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9705 - loss: 0.0939Test Accuracy after Epoch 191: 96.37%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9705 - loss: 0.0939 - val_accuracy: 0.9688 - val_loss: 0.1073\n",
      "Epoch 192/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.0986Test Accuracy after Epoch 192: 96.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9684 - loss: 0.0986 - val_accuracy: 0.9678 - val_loss: 0.1115\n",
      "Epoch 193/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9696 - loss: 0.0943Test Accuracy after Epoch 193: 96.39%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9696 - loss: 0.0943 - val_accuracy: 0.9650 - val_loss: 0.1117\n",
      "Epoch 194/250\n",
      "\u001b[1m1666/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9682 - loss: 0.0982Test Accuracy after Epoch 194: 96.29%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.0982 - val_accuracy: 0.9682 - val_loss: 0.1117\n",
      "Epoch 195/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.0907Test Accuracy after Epoch 195: 96.18%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.0907 - val_accuracy: 0.9687 - val_loss: 0.1080\n",
      "Epoch 196/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9711 - loss: 0.0908Test Accuracy after Epoch 196: 96.04%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9711 - loss: 0.0908 - val_accuracy: 0.9687 - val_loss: 0.1037\n",
      "Epoch 197/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9700 - loss: 0.0974Test Accuracy after Epoch 197: 96.39%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9700 - loss: 0.0974 - val_accuracy: 0.9657 - val_loss: 0.1100\n",
      "Epoch 198/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0971Test Accuracy after Epoch 198: 96.34%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.0971 - val_accuracy: 0.9677 - val_loss: 0.1034\n",
      "Epoch 199/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9703 - loss: 0.0926Test Accuracy after Epoch 199: 96.48%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9703 - loss: 0.0926 - val_accuracy: 0.9647 - val_loss: 0.1125\n",
      "Epoch 200/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9714 - loss: 0.0932Test Accuracy after Epoch 200: 95.92%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9714 - loss: 0.0932 - val_accuracy: 0.9657 - val_loss: 0.1121\n",
      "Epoch 201/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.0998Test Accuracy after Epoch 201: 95.78%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.0998 - val_accuracy: 0.9643 - val_loss: 0.1178\n",
      "Epoch 202/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9694 - loss: 0.0980Test Accuracy after Epoch 202: 96.02%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9694 - loss: 0.0980 - val_accuracy: 0.9643 - val_loss: 0.1207\n",
      "Epoch 203/250\n",
      "\u001b[1m1675/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9679 - loss: 0.0981Test Accuracy after Epoch 203: 96.33%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9679 - loss: 0.0981 - val_accuracy: 0.9672 - val_loss: 0.1127\n",
      "Epoch 204/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9701 - loss: 0.0911Test Accuracy after Epoch 204: 96.26%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9701 - loss: 0.0911 - val_accuracy: 0.9645 - val_loss: 0.1178\n",
      "Epoch 205/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9707 - loss: 0.0924Test Accuracy after Epoch 205: 96.30%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.0924 - val_accuracy: 0.9690 - val_loss: 0.1139\n",
      "Epoch 206/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9696 - loss: 0.0947Test Accuracy after Epoch 206: 96.31%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9696 - loss: 0.0947 - val_accuracy: 0.9675 - val_loss: 0.1067\n",
      "Epoch 207/250\n",
      "\u001b[1m1687/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9698 - loss: 0.0946Test Accuracy after Epoch 207: 96.26%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9698 - loss: 0.0946 - val_accuracy: 0.9693 - val_loss: 0.1047\n",
      "Epoch 208/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9704 - loss: 0.0930Test Accuracy after Epoch 208: 96.15%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9704 - loss: 0.0930 - val_accuracy: 0.9693 - val_loss: 0.1039\n",
      "Epoch 209/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.0931Test Accuracy after Epoch 209: 96.39%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.0931 - val_accuracy: 0.9650 - val_loss: 0.1092\n",
      "Epoch 210/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0981Test Accuracy after Epoch 210: 96.07%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0981 - val_accuracy: 0.9668 - val_loss: 0.1040\n",
      "Epoch 211/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9706 - loss: 0.0932Test Accuracy after Epoch 211: 96.20%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.9706 - loss: 0.0933 - val_accuracy: 0.9652 - val_loss: 0.1129\n",
      "Epoch 212/250\n",
      "\u001b[1m1662/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9703 - loss: 0.0950Test Accuracy after Epoch 212: 96.34%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9703 - loss: 0.0950 - val_accuracy: 0.9688 - val_loss: 0.1112\n",
      "Epoch 213/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.0956Test Accuracy after Epoch 213: 95.87%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.0956 - val_accuracy: 0.9630 - val_loss: 0.1302\n",
      "Epoch 214/250\n",
      "\u001b[1m1663/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1017Test Accuracy after Epoch 214: 96.06%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9669 - loss: 0.1017 - val_accuracy: 0.9632 - val_loss: 0.1228\n",
      "Epoch 215/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9664 - loss: 0.1051Test Accuracy after Epoch 215: 95.60%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9664 - loss: 0.1051 - val_accuracy: 0.9643 - val_loss: 0.1268\n",
      "Epoch 216/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9665 - loss: 0.1077Test Accuracy after Epoch 216: 95.92%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9665 - loss: 0.1077 - val_accuracy: 0.9645 - val_loss: 0.1250\n",
      "Epoch 217/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9668 - loss: 0.1043Test Accuracy after Epoch 217: 95.87%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9668 - loss: 0.1043 - val_accuracy: 0.9617 - val_loss: 0.1308\n",
      "Epoch 218/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9692 - loss: 0.1021Test Accuracy after Epoch 218: 95.95%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9692 - loss: 0.1021 - val_accuracy: 0.9642 - val_loss: 0.1217\n",
      "Epoch 219/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9662 - loss: 0.1043Test Accuracy after Epoch 219: 95.67%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9662 - loss: 0.1043 - val_accuracy: 0.9623 - val_loss: 0.1244\n",
      "Epoch 220/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0979Test Accuracy after Epoch 220: 96.30%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0980 - val_accuracy: 0.9665 - val_loss: 0.1187\n",
      "Epoch 221/250\n",
      "\u001b[1m1679/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9682 - loss: 0.1002Test Accuracy after Epoch 221: 96.01%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9682 - loss: 0.1002 - val_accuracy: 0.9663 - val_loss: 0.1168\n",
      "Epoch 222/250\n",
      "\u001b[1m1678/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9669 - loss: 0.1048Test Accuracy after Epoch 222: 95.98%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9669 - loss: 0.1048 - val_accuracy: 0.9662 - val_loss: 0.1135\n",
      "Epoch 223/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0993Test Accuracy after Epoch 223: 96.19%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9686 - loss: 0.0993 - val_accuracy: 0.9662 - val_loss: 0.1107\n",
      "Epoch 224/250\n",
      "\u001b[1m1680/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9693 - loss: 0.0964Test Accuracy after Epoch 224: 96.46%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9693 - loss: 0.0964 - val_accuracy: 0.9657 - val_loss: 0.1144\n",
      "Epoch 225/250\n",
      "\u001b[1m1676/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9709 - loss: 0.0906Test Accuracy after Epoch 225: 96.07%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9709 - loss: 0.0907 - val_accuracy: 0.9647 - val_loss: 0.1191\n",
      "Epoch 226/250\n",
      "\u001b[1m1678/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9717 - loss: 0.0912Test Accuracy after Epoch 226: 96.13%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9717 - loss: 0.0912 - val_accuracy: 0.9692 - val_loss: 0.1076\n",
      "Epoch 227/250\n",
      "\u001b[1m1664/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0941Test Accuracy after Epoch 227: 96.24%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0941 - val_accuracy: 0.9658 - val_loss: 0.1162\n",
      "Epoch 228/250\n",
      "\u001b[1m1669/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0911Test Accuracy after Epoch 228: 96.31%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9714 - loss: 0.0911 - val_accuracy: 0.9670 - val_loss: 0.1113\n",
      "Epoch 229/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.0996Test Accuracy after Epoch 229: 96.15%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9676 - loss: 0.0996 - val_accuracy: 0.9658 - val_loss: 0.1143\n",
      "Epoch 230/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9707 - loss: 0.0913Test Accuracy after Epoch 230: 96.39%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9707 - loss: 0.0913 - val_accuracy: 0.9657 - val_loss: 0.1120\n",
      "Epoch 231/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9699 - loss: 0.0901Test Accuracy after Epoch 231: 96.38%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9699 - loss: 0.0902 - val_accuracy: 0.9685 - val_loss: 0.1086\n",
      "Epoch 232/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9686 - loss: 0.0966Test Accuracy after Epoch 232: 95.88%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9686 - loss: 0.0967 - val_accuracy: 0.9638 - val_loss: 0.1200\n",
      "Epoch 233/250\n",
      "\u001b[1m1681/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.0986Test Accuracy after Epoch 233: 96.08%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9684 - loss: 0.0986 - val_accuracy: 0.9685 - val_loss: 0.1121\n",
      "Epoch 234/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9705 - loss: 0.0938Test Accuracy after Epoch 234: 95.81%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9705 - loss: 0.0938 - val_accuracy: 0.9670 - val_loss: 0.1141\n",
      "Epoch 235/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0994Test Accuracy after Epoch 235: 95.89%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0994 - val_accuracy: 0.9662 - val_loss: 0.1164\n",
      "Epoch 236/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9678 - loss: 0.0966Test Accuracy after Epoch 236: 96.56%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9678 - loss: 0.0966 - val_accuracy: 0.9647 - val_loss: 0.1214\n",
      "Epoch 237/250\n",
      "\u001b[1m1674/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.0972Test Accuracy after Epoch 237: 96.29%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9688 - loss: 0.0972 - val_accuracy: 0.9670 - val_loss: 0.1203\n",
      "Epoch 238/250\n",
      "\u001b[1m1682/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.0951Test Accuracy after Epoch 238: 96.24%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.0951 - val_accuracy: 0.9665 - val_loss: 0.1221\n",
      "Epoch 239/250\n",
      "\u001b[1m1685/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9695 - loss: 0.0951Test Accuracy after Epoch 239: 96.31%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9695 - loss: 0.0951 - val_accuracy: 0.9653 - val_loss: 0.1231\n",
      "Epoch 240/250\n",
      "\u001b[1m1677/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9702 - loss: 0.0937Test Accuracy after Epoch 240: 96.05%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9702 - loss: 0.0937 - val_accuracy: 0.9672 - val_loss: 0.1168\n",
      "Epoch 241/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9716 - loss: 0.0912Test Accuracy after Epoch 241: 95.72%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.9716 - loss: 0.0912 - val_accuracy: 0.9642 - val_loss: 0.1237\n",
      "Epoch 242/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9679 - loss: 0.1020Test Accuracy after Epoch 242: 95.91%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9679 - loss: 0.1020 - val_accuracy: 0.9670 - val_loss: 0.1127\n",
      "Epoch 243/250\n",
      "\u001b[1m1684/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.0995Test Accuracy after Epoch 243: 96.16%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.0995 - val_accuracy: 0.9647 - val_loss: 0.1173\n",
      "Epoch 244/250\n",
      "\u001b[1m1658/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0930Test Accuracy after Epoch 244: 96.00%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9708 - loss: 0.0931 - val_accuracy: 0.9663 - val_loss: 0.1182\n",
      "Epoch 245/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.0983Test Accuracy after Epoch 245: 96.45%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9689 - loss: 0.0983 - val_accuracy: 0.9657 - val_loss: 0.1106\n",
      "Epoch 246/250\n",
      "\u001b[1m1655/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.0989Test Accuracy after Epoch 246: 96.02%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9687 - loss: 0.0989 - val_accuracy: 0.9652 - val_loss: 0.1201\n",
      "Epoch 247/250\n",
      "\u001b[1m1667/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9684 - loss: 0.1003Test Accuracy after Epoch 247: 96.12%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.1003 - val_accuracy: 0.9663 - val_loss: 0.1209\n",
      "Epoch 248/250\n",
      "\u001b[1m1661/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.1006Test Accuracy after Epoch 248: 96.31%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9683 - loss: 0.1006 - val_accuracy: 0.9662 - val_loss: 0.1182\n",
      "Epoch 249/250\n",
      "\u001b[1m1673/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.0975Test Accuracy after Epoch 249: 96.29%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9694 - loss: 0.0975 - val_accuracy: 0.9645 - val_loss: 0.1223\n",
      "Epoch 250/250\n",
      "\u001b[1m1686/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.0958Test Accuracy after Epoch 250: 96.10%\n",
      "\u001b[1m1688/1688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9701 - loss: 0.0958 - val_accuracy: 0.9638 - val_loss: 0.1202\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Assuming x_train_sc has shape (number_of_samples, number_of_features)\n",
    "# Make sure to replace 'number_of_features' with the actual number of features in your data.\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "# Add hidden layer\n",
    "model.add(Dense(units=128, input_dim=x_train.shape[1], activation='tanh'))\n",
    "# Add hidden layer\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "# # Add hidden layer\n",
    "# model.add(Dense(units=32, activation='relu'))\n",
    "# # Add hidden layer\n",
    "model.add(Dense(units=16, activation='relu'))\n",
    "# Add hidden layer\n",
    "model.add(Dense(units=8, activation='relu'))\n",
    "# Output layer\n",
    "model.add(Dense(units=len(y_train.unique()), activation='softmax'))\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "# Print the model summary\n",
    "# model.summary()\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print the model summary\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "# Custom callback for logging test accuracy at each epoch\n",
    "class TestAccuracyLogger(Callback):\n",
    "    def __init__(self, test_data):\n",
    "        super().__init__()\n",
    "        self.test_data = test_data\n",
    "        self.test_accuracies = []\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        _, test_accuracy = self.model.evaluate(self.test_data[0], self.test_data[1], verbose=0)\n",
    "        self.test_accuracies.append(test_accuracy)\n",
    "        print(f'Test Accuracy after Epoch {epoch + 1}: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Assuming x_test_sc and y_test are your test data and labels\n",
    "# Also, replace 'num_classes' with the actual number of classes in your classification task.\n",
    "\n",
    "# Create an instance of the custom callback\n",
    "test_accuracy_logger = TestAccuracyLogger(test_data=(x_test, y_test))\n",
    "\n",
    "# Fit the model with the custom callback\n",
    "history = model.fit(x_train, y_train, epochs=250, batch_size=32, validation_split=0.1, callbacks=[test_accuracy_logger])\n",
    "\n",
    "# Plot the learning curve\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(test_accuracy_logger.test_accuracies, label='Test Accuracy', linestyle='dashed')\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Fit the model\n",
    "# # print(x_train_sc)\n",
    "# model.fit(x_train_sc, y_encoded, epochs=100, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# # # Evaluate the model on the test set\n",
    "# test_loss, test_accuracy = model.evaluate(x_test_sc, y_test_encoded)\n",
    "# print(\"Test Accuracy: %.2f%%\" % (test_accuracy * 100))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
